{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import html\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_alert(message):\n",
    "    display(HTML('<script type=\"text/javascript\">alert(\"' + message + '\");</script>'))\n",
    "    \n",
    "def browser_notify(message):\n",
    "    display(HTML('<script type=\"text/javascript\">var notification=new Notification(\"Jupyter Notification\",{icon:\"http://blog.jupyter.org/content/images/2015/02/jupyter-sq-text.png\",body:\"' + message + '\"});</script>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_path = \"/Users/COMMANDER/Desktop/MScBA_Consultantcy_Project/GloVe/glove.twitter.27B/\"\n",
    "glove_path_6B = \"/Users/COMMANDER/Desktop/MScBA_Consultantcy_Project/GloVe/glove.6B/\"\n",
    "glove_path_42B = \"/Users/COMMANDER/Desktop/MScBA_Consultantcy_Project/GloVe/glove.42B.300d/\"\n",
    "glove_path_840B = \"/Users/COMMANDER/Desktop/MScBA_Consultantcy_Project/GloVe/glove.840B.300d/\"\n",
    "glove_path_twitter = \"/Users/COMMANDER/Desktop/MScBA_Consultantcy_Project/GloVe/glove.twitter.27B/\"\n",
    "wassa_home = \"/Users/COMMANDER/Downloads/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec + GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding=\"utf8\")\n",
    "    model = {}\n",
    "    num = 1\n",
    "    for line in f:\n",
    "        try:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = [float(val) for val in splitLine[1:]]\n",
    "            model[word] = np.array(embedding)\n",
    "            num += 1\n",
    "        except Exception as e:\n",
    "            print(\"Failed at line \" + str(num))\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google news pretrained vectors\n",
    "wv_model_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter pretrained vectors\n",
    "wv_model_path_1 = \"word2vec_twitter_model.bin\"\n",
    "wv_model_1 = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path_1, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "wv_model_path_2 = glove_path_twitter + \"glove.twitter.27B.200d.txt\"\n",
    "wv_model_2 = loadGloveModel(wv_model_path_2)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "wv_model_path_3 = glove_path_6B + \"glove.6B.300d.txt\"\n",
    "wv_model_3 = loadGloveModel(wv_model_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1917494  words loaded!\n"
     ]
    }
   ],
   "source": [
    "wv_model_path_4 = glove_path_42B + \"glove.42B.300d.txt\"\n",
    "wv_model_4 = loadGloveModel(wv_model_path_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Failed at line 52344\n",
      "Failed at line 128261\n",
      "Failed at line 151101\n",
      "Failed at line 200666\n",
      "Failed at line 209830\n",
      "Failed at line 220775\n",
      "Failed at line 253456\n",
      "Failed at line 365739\n",
      "Failed at line 532041\n",
      "Failed at line 717294\n",
      "Failed at line 994809\n",
      "Failed at line 1123321\n",
      "Failed at line 1148398\n",
      "Failed at line 1352098\n",
      "Failed at line 1499714\n",
      "Failed at line 1533795\n",
      "Failed at line 1899826\n",
      "Failed at line 1921136\n",
      "Failed at line 2058949\n",
      "Failed at line 2165228\n",
      "Done. 2195885  words loaded!\n"
     ]
    }
   ],
   "source": [
    "wv_model_path_5 = glove_path_840B + \"glove.840B.300d.txt\"\n",
    "wv_model_5 = loadGloveModel(wv_model_path_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 400 200 300 300 300\n"
     ]
    }
   ],
   "source": [
    "w2v_dimensions = len(wv_model['word'])\n",
    "w2v_dimensions_1 = len(wv_model_1['word'])\n",
    "w2v_dimensions_2 = len(wv_model_2['word'])\n",
    "w2v_dimensions_3 = len(wv_model_3['word'])\n",
    "w2v_dimensions_4 = len(wv_model_4['word'])\n",
    "w2v_dimensions_5 = len(wv_model_5['word'])\n",
    "\n",
    "print(w2v_dimensions, w2v_dimensions_1, w2v_dimensions_2, w2v_dimensions_3, w2v_dimensions_4, w2v_dimensions_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(word, model, dimensions):\n",
    "\n",
    "    vec_rep = np.zeros(dimensions)\n",
    "    if word in model:\n",
    "        vec_rep = model[word]\n",
    "    \n",
    "    return vec_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from negate import NEGATE as neg\n",
    "import nltk\n",
    "nltk_sw = nltk.corpus.stopwords.words('english') #Number of nltk stop words: 179\n",
    "nltk_sw_neg = [x for x in nltk_sw if x not in neg] #Number of nltk stop words without negating words: 158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string):\n",
    "    split_string = \\\n",
    "        [word for word in string.split()\n",
    "         if word not in nltk_sw_neg]\n",
    "    \n",
    "    \n",
    "    return \" \".join(split_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):  \n",
    "    string = html.unescape(string)\n",
    "    string = string.replace(\"\\\\n\", \" \")\n",
    "    string = string.replace(\"_NEG\", \"\")\n",
    "    string = string.replace(\"_NEGFIRST\", \"\")\n",
    "    string = re.sub(r\"@[A-Za-z0-9_(),!?\\'\\`]+\", \" \", string) # removing any twitter handle mentions\n",
    "    string = re.sub(r\"\\d+\", \" \", string) # removing any words with numbers\n",
    "    string = re.sub(r\"_\", \" \", string)\n",
    "    string = re.sub(r\":\", \" \", string)\n",
    "    string = re.sub(r\"/\", \" \", string)\n",
    "    string = re.sub(r\"#\", \" \", string)\n",
    "    string = re.sub(r\"\\.\", \" \", string)\n",
    "    string = re.sub(r\"\\*\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \", string)\n",
    "    string = re.sub(r\"\\'m\", \" am\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" have\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" not\", string)\n",
    "    string = re.sub(r\"n\\â€™t\", \" not\", string)\n",
    "    string = re.sub(r\"\\'re\", \" are\", string)\n",
    "    string = re.sub(r\"\\â€™re\", \" are\", string)\n",
    "    string = re.sub(r\"\\'d\", \" would\", string)\n",
    "    string = re.sub(r\"\\â€™d\", \" would\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" will\", string)\n",
    "    string = re.sub(r\"\\â€™ll\", \" will\", string)\n",
    "    string = re.sub(r\"'\", \" \", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" !\", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" ?\", string)\n",
    "    string = re.sub(r\"-\", \" \", string)\n",
    "    string = re.sub(r\"<\", \" \", string)\n",
    "    string = re.sub(r\">\", \" \", string)\n",
    "    string = re.sub(r\";\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return remove_stopwords(string.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"ApplÃª isn't looking at buying U.K. startup for $1 billion. They still 600,20 4.5 + 423 look around.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'applÃª not looking buying u k startup $ billion still + look around'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "\n",
    "    def __init__(self, id, text, emotion, intensity):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.emotion = emotion\n",
    "        self.intensity = intensity\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \\\n",
    "            \"id: \" + self.id + \\\n",
    "            \", text: \" + self.text + \\\n",
    "            \", emotion: \" + self.emotion + \\\n",
    "            \", intensity: \" + self.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data(training_data_file_path):\n",
    "\n",
    "    train_list = list()\n",
    "    with open(training_data_file_path, encoding=\"utf8\") as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            train_list.append(Tweet(array[0], list(tknzr.tokenize(clean_str(array[1]))), \n",
    "                                    array[2], str(array[3])))\n",
    "    return train_list\n",
    "            \n",
    "def read_training_data_verbatim(training_data_file_path):\n",
    "\n",
    "    train_list = list()\n",
    "    with open(training_data_file_path, encoding=\"utf8\") as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            train_list.append(Tweet(array[0], array[1], array[2], str(array[3])))\n",
    "    return train_list\n",
    "    \n",
    "def read_test_data(training_data_file_path):\n",
    "\n",
    "    test_list = list()\n",
    "    with open(training_data_file_path, encoding=\"utf8\") as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            test_list.append(Tweet(array[0], clean_str(array[1]), array[2], None))\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linear_factor = PolynomialFeatures(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = \"anger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file_path = \\\n",
    "    wassa_home + \"corpora/multigenre/\" + \\\n",
    "    emotion + \"-ratings-0to1.train.txt\"\n",
    "word_embeddings_path = wassa_home + \"embeddings/\" + emotion + \"-word-embeddings.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction Snippets\n",
    "\n",
    "Emoji Intensity (Thanks to Roger, it works!)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/COMMANDER/Downloads/lexicons/index_emoji.json', encoding=\"utf8\") as emoji_file:\n",
    "    emoji_list = json.load(emoji_file)\n",
    "    \n",
    "emoji_dict = dict()\n",
    "\n",
    "for emoji in emoji_list:\n",
    "    emoji_dict[emoji[\"emoji\"]] = (emoji[\"name\"], emoji[\"polarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoji_intensity(word):\n",
    "    \n",
    "    score = 0.0\n",
    "    if word in emoji_dict.keys():\n",
    "        score = float(emoji_dict[word][1])\n",
    "    \n",
    "    vec_rep = np.array([score])\n",
    "    \n",
    "    \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_word(word):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # first assuem it is a noun\n",
    "    lem_token = lemmatizer.lemmatize(word, pos=wordnet.NOUN)\n",
    "    # otherwise check for verb\n",
    "    if lem_token == word:\n",
    "        lem_token = lemmatizer.lemmatize(word, pos=wordnet.VERB)\n",
    "    # otherwise check for adjektive\n",
    "    if lem_token == word:\n",
    "        lem_token = lemmatizer.lemmatize(word, pos=wordnet.ADJ)\n",
    "    # otherwiese check for adverb\n",
    "    if lem_token == word:\n",
    "        lem_token = lemmatizer.lemmatize(word, pos=wordnet.ADV)\n",
    "    return lem_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_intensity_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-AffectIntensity-Lexicon.txt\"\n",
    "    \n",
    "def get_word_affect_intensity_dict(emotion):\n",
    "    word_intensities = dict()\n",
    "\n",
    "    with open(affect_intensity_file_path) as affect_intensity_file:\n",
    "        for line in affect_intensity_file:\n",
    "            word_int_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_int_array[2] == emotion):\n",
    "                word_intensities[word_int_array[0]] = float(word_int_array[1])\n",
    "\n",
    "    return word_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_intensities = get_word_affect_intensity_dict(emotion)\n",
    "word_intensities_anger = get_word_affect_intensity_dict('anger')\n",
    "word_intensities_fear = get_word_affect_intensity_dict('fear')\n",
    "word_intensities_joy = get_word_affect_intensity_dict('joy')\n",
    "word_intensities_sadness = get_word_affect_intensity_dict('sadness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emo_int_vector(word):\n",
    "    \n",
    "    a_score = 0.0\n",
    "    f_score = 0.0\n",
    "    j_score = 0.0\n",
    "    s_score = 0.0\n",
    "\n",
    "    if word in word_intensities_anger.keys():\n",
    "        a_score = float(word_intensities_anger[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in word_intensities_anger.keys():\n",
    "            a_score = float(word_intensities_anger[word_lemm])\n",
    "\n",
    "    if word in word_intensities_fear.keys():\n",
    "        f_score = float(word_intensities_fear[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in word_intensities_fear.keys():\n",
    "            f_score = float(word_intensities_fear[word_lemm])\n",
    "\n",
    "    if word in word_intensities_joy.keys():\n",
    "        j_score = float(word_intensities_joy[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in word_intensities_joy.keys():\n",
    "            j_score = float(word_intensities_joy[word_lemm])\n",
    "\n",
    "    if word in word_intensities_sadness.keys():\n",
    "        s_score = float(word_intensities_sadness[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in word_intensities_sadness.keys():\n",
    "            s_score = float(word_intensities_sadness[word_lemm])\n",
    "\n",
    "    vec_rep = np.array([a_score, f_score, j_score, s_score])\n",
    "    \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiwordnetscore(word):\n",
    "    \n",
    "    vec_rep = np.zeros(2)\n",
    "    \n",
    "    synsetlist = list(swn.senti_synsets(word))\n",
    "\n",
    "    if synsetlist:\n",
    "        vec_rep[0] = synsetlist[0].pos_score()\n",
    "        vec_rep[1] = synsetlist[0].neg_score()\n",
    "\n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Emotion Presence Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_emotion_lex_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"Lexikon/NRC-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/\" + \\\n",
    "    \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "def get_affect_presence_list(emotion):\n",
    "    word_list = list()\n",
    "    \n",
    "    with open(sentiment_emotion_lex_file_path) as sentiment_emotion_lex_file:\n",
    "        for line in sentiment_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "\n",
    "            if (word_array[1] == emotion and word_array[2] == '1'):\n",
    "                word_list.append(word_array[0])\n",
    "    return word_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_emotion_lex_word_list = get_affect_presence_list(emotion)\n",
    "sentiment_emotion_lex_word_list_anger = get_affect_presence_list(\"anger\")\n",
    "sentiment_emotion_lex_word_list_fear = get_affect_presence_list(\"fear\")\n",
    "sentiment_emotion_lex_word_list_joy = get_affect_presence_list(\"joy\")\n",
    "sentiment_emotion_lex_word_list_sadness = get_affect_presence_list(\"sadness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_emotion_feature(word):\n",
    "    \n",
    "    a_score = 0.0\n",
    "    f_score = 0.0\n",
    "    j_score = 0.0\n",
    "    s_score = 0.0\n",
    "\n",
    "    if word in sentiment_emotion_lex_word_list_anger:\n",
    "        a_score = 1.0\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in sentiment_emotion_lex_word_list_anger:\n",
    "            a_score = 1.0\n",
    "\n",
    "    if word in sentiment_emotion_lex_word_list_fear:\n",
    "        f_score = 1.0\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in sentiment_emotion_lex_word_list_fear:\n",
    "            f_score = 1.0\n",
    "\n",
    "    if word in sentiment_emotion_lex_word_list_joy:\n",
    "        j_score = 1.0\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in sentiment_emotion_lex_word_list_joy:\n",
    "            j_score = 1.0\n",
    "\n",
    "    if word in sentiment_emotion_lex_word_list_sadness:\n",
    "        s_score = 1.0\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in sentiment_emotion_lex_word_list_sadness:\n",
    "            s_score = 1.0\n",
    "\n",
    "    vec_rep = np.array([a_score, f_score, j_score, s_score])\n",
    "    \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Emotion Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_emotion_lex_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"Lexikon/NRC-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/\" + \\\n",
    "    \"NRC-Hashtag-Emotion-Lexicon-v0.2.txt\"\n",
    "    \n",
    "def get_hashtag_emotion_intensity(emotion):\n",
    "    hastag_intensities = dict()\n",
    "    \n",
    "    with open(hashtag_emotion_lex_file_path) as hashtag_emotion_lex_file:\n",
    "        for line in hashtag_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if (word_array[0] == emotion):\n",
    "                hastag_intensities[word_array[1]] = float(word_array[2])\n",
    "                \n",
    "    return hastag_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_emotion_intensities = get_hashtag_emotion_intensity(emotion)\n",
    "hashtag_emotion_intensities_anger = get_hashtag_emotion_intensity('anger')\n",
    "hashtag_emotion_intensities_fear = get_hashtag_emotion_intensity('fear')\n",
    "hashtag_emotion_intensities_joy = get_hashtag_emotion_intensity('joy')\n",
    "hashtag_emotion_intensities_sadness = get_hashtag_emotion_intensity('sadness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_emotion_vector(word):\n",
    "    \n",
    "    a_score = 0.0\n",
    "    f_score = 0.0\n",
    "    j_score = 0.0\n",
    "    s_score = 0.0\n",
    "    \n",
    "    if word in hashtag_emotion_intensities_anger.keys():\n",
    "        a_score = float(hashtag_emotion_intensities_anger[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in hashtag_emotion_intensities_anger.keys():\n",
    "            a_score = float(hashtag_emotion_intensities_anger[word_lemm])\n",
    "\n",
    "    if word in hashtag_emotion_intensities_fear.keys():\n",
    "        f_score = float(hashtag_emotion_intensities_fear[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in hashtag_emotion_intensities_fear.keys():\n",
    "            f_score = float(hashtag_emotion_intensities_fear[word_lemm])\n",
    "\n",
    "    if word in hashtag_emotion_intensities_joy.keys():\n",
    "        j_score = float(hashtag_emotion_intensities_joy[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in hashtag_emotion_intensities_joy.keys():\n",
    "            j_score = float(hashtag_emotion_intensities_joy[word_lemm])\n",
    "\n",
    "    if word in hashtag_emotion_intensities_sadness.keys():\n",
    "        s_score = float(hashtag_emotion_intensities_sadness[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in hashtag_emotion_intensities_sadness.keys():\n",
    "            s_score = float(hashtag_emotion_intensities_sadness[word_lemm])\n",
    "        \n",
    "    vec_rep = np.array([a_score, f_score, j_score, s_score])\n",
    "            \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticon Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/Emoticon-unigrams.txt\"\n",
    "emoticon_lexicon_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/Emoticon-bigrams.txt\"\n",
    "    \n",
    "emoticon_lexicon_unigrams = dict()\n",
    "emoticon_lexicon_bigrams = dict()\n",
    "\n",
    "def get_emoticon_lexicon_unigram_dict():\n",
    "    with open(emoticon_lexicon_unigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_lexicon_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_lexicon_unigrams\n",
    "\n",
    "def get_emoticon_lexicon_bigram_dict():\n",
    "    with open(emoticon_lexicon_bigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_lexicon_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_lexicon_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_unigram_dict = get_emoticon_lexicon_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_bigram_dict = get_emoticon_lexicon_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_sentiment_emoticon_lexicon_vector(word):\n",
    "    \n",
    "    vec_rep = np.zeros(3)\n",
    "    if word in emoticon_lexicon_unigram_dict.keys():\n",
    "        vec_rep = emoticon_lexicon_unigram_dict[word]\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in emoticon_lexicon_unigram_dict.keys():\n",
    "            vec_rep = emoticon_lexicon_unigram_dict[word_lemm]\n",
    "        \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]\n",
    "\n",
    "def get_bigram_sentiment_emoticon_lexicon_vector(word):\n",
    "    \n",
    "    vec_rep = np.zeros(3)\n",
    "    if word in emoticon_lexicon_bigram_dict.keys():\n",
    "        vec_rep = emoticon_lexicon_bigram_dict[word]\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in emoticon_lexicon_bigram_dict.keys():\n",
    "            vec_rep = emoticon_lexicon_bigram_dict[word_lemm]\n",
    "        \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticon Sentiment Aff-Neg Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_afflex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/Emoticon-AFFLEX-NEGLEX-unigrams.txt\"\n",
    "\n",
    "emoticon_afflex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/Emoticon-AFFLEX-NEGLEX-bigrams.txt\"\n",
    "    \n",
    "emoticon_afflex_unigrams = dict()\n",
    "emoticon_afflex_bigrams = dict()\n",
    "\n",
    "def get_emoticon_afflex_unigram_dict():\n",
    "    with open(emoticon_afflex_unigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_afflex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_afflex_unigrams\n",
    "\n",
    "def get_emoticon_afflex_bigram_dict():\n",
    "    with open(emoticon_afflex_bigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_afflex_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_afflex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_afflex_unigram_dict = get_emoticon_afflex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_afflex_bigram_dict = get_emoticon_afflex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_sentiment_emoticon_afflex_vector(word):\n",
    "    \n",
    "    vec_rep = np.zeros(3)\n",
    "    if word in emoticon_afflex_unigram_dict.keys():\n",
    "        vec_rep = emoticon_afflex_unigram_dict[word]\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in emoticon_afflex_unigram_dict.keys():\n",
    "            vec_rep = emoticon_afflex_unigram_dict[word_lemm]\n",
    "        \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]\n",
    "\n",
    "def get_bigram_sentiment_emoticon_afflex_vector(word):\n",
    "    \n",
    "    vec_rep = np.zeros(3)\n",
    "    if word in emoticon_afflex_bigram_dict.keys():\n",
    "        vec_rep = emoticon_afflex_bigram_dict[word]\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in emoticon_afflex_bigram_dict.keys():\n",
    "            vec_rep = emoticon_afflex_bigram_dict[word_lemm]\n",
    "    \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Sentiment Aff-Neg Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_affneglex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/HS-AFFLEX-NEGLEX-unigrams.txt\"\n",
    "hashtag_affneglex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/HS-AFFLEX-NEGLEX-bigrams.txt\"\n",
    "    \n",
    "hashtag_affneglex_unigrams = dict()\n",
    "hashtag_affneglex_bigrams = dict()\n",
    "\n",
    "def get_hashtag_affneglex_unigram_dict():\n",
    "    with open(hashtag_affneglex_unigrams_file_path) as hashtag_sent_lex_file:\n",
    "        for line in hashtag_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            hashtag_affneglex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hashtag_affneglex_unigrams\n",
    "\n",
    "def get_hashtag_affneglex_bigram_dict():\n",
    "    with open(hashtag_affneglex_bigrams_file_path) as hashtag_sent_lex_file:\n",
    "        for line in hashtag_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            hashtag_affneglex_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "\n",
    "    return hashtag_affneglex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_affneglex_unigram_dict = get_hashtag_affneglex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_affneglex_bigram_dict = get_hashtag_affneglex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_sentiment_hashtag_affneglex_vector(word):\n",
    "    \n",
    "    vec_rep = np.zeros(3)\n",
    "    if word in hashtag_affneglex_unigram_dict.keys():\n",
    "        vec_rep = hashtag_affneglex_unigram_dict[word]\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in hashtag_affneglex_unigram_dict.keys():\n",
    "            vec_rep = hashtag_affneglex_unigram_dict[word_lemm]\n",
    "        \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]\n",
    "\n",
    "def get_bigram_sentiment_hashtag_affneglex_vector(word):\n",
    "    \n",
    "    vec_rep = np.zeros(3)\n",
    "    if word in hashtag_affneglex_bigram_dict.keys():\n",
    "        vec_rep = hashtag_affneglex_bigram_dict[word]\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in hashtag_affneglex_bigram_dict.keys():\n",
    "            vec_rep = hashtag_affneglex_bigram_dict[word_lemm]\n",
    "        \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sent_lex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/HS-unigrams.txt\"\n",
    "hash_sent_lex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/HS-bigrams.txt\"\n",
    "\n",
    "def get_hash_sent_lex_unigram_dict():\n",
    "    \n",
    "    hash_sent_lex_unigrams = dict()\n",
    "    with open(hash_sent_lex_unigrams_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if clean_str(word_array[0]):\n",
    "                hash_sent_lex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hash_sent_lex_unigrams\n",
    "\n",
    "def get_hash_sent_lex_bigram_dict():\n",
    "\n",
    "    hash_sent_lex_bigrams = dict()\n",
    "    with open(hash_sent_lex_bigrams_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if clean_str(word_array[0]):\n",
    "                hash_sent_lex_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hash_sent_lex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sent_lex_unigram_dict = get_hash_sent_lex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sent_lex_bigram_dict = get_hash_sent_lex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_sentiment_hash_sent_lex_vector(word):\n",
    "    \n",
    "    vec_rep = np.zeros(3)\n",
    "    if word in hash_sent_lex_unigram_dict.keys():\n",
    "        vec_rep = hash_sent_lex_unigram_dict[word]\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in hash_sent_lex_unigram_dict.keys():\n",
    "            vec_rep = hash_sent_lex_unigram_dict[word_lemm]\n",
    "        \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]\n",
    "\n",
    "\n",
    "def get_bigram_sentiment_hash_sent_lex_vector(word):\n",
    "\n",
    "    vec_rep = np.zeros(3)\n",
    "    if word in hash_sent_lex_bigram_dict.keys():\n",
    "        vec_rep = hash_sent_lex_bigram_dict[word]\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in hash_sent_lex_bigram_dict.keys():\n",
    "            vec_rep = hash_sent_lex_bigram_dict[word_lemm]\n",
    "            \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depeche Mood (There is DepecheMood V2, try it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "depeche_mood_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/DepecheMood_V1.0/DepecheMood_normfreq2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depeche_vector_dict():\n",
    "    depeche_vector_dict = dict()\n",
    "    with open(depeche_mood_file_path) as depeche_mood_file:\n",
    "        for line in depeche_mood_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            depeche_vector_dict[word_array[0].split(\"#\")[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return depeche_vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "depeche_vector_dict = get_depeche_vector_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depeche_mood_vector(word):\n",
    "    \n",
    "    vec_rep = np.zeros(8)\n",
    "    if word in depeche_vector_dict.keys():\n",
    "        vec_rep = np.array(depeche_vector_dict[word])\n",
    "    else:\n",
    "        word_lemm = lemm_word(word)\n",
    "        if word_lemm in depeche_vector_dict.keys():\n",
    "            vec_rep = np.array(depeche_vector_dict[word_lemm])\n",
    "\n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & Vectorizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "\n",
    "def learn_unigram_word_embedding(word):\n",
    "    \n",
    "    word_feature_embedding_dict = dict()\n",
    "    \n",
    "    '''Pre-trained Word embeddings'''\n",
    "    index = 0\n",
    "    word_feature_embedding_dict[index] = get_word2vec_embedding(word, wv_model, w2v_dimensions)\n",
    "\n",
    "    index = 1\n",
    "    word_feature_embedding_dict[index] = get_word2vec_embedding(word, wv_model_1, w2v_dimensions_1)\n",
    "\n",
    "    index = 2\n",
    "    word_feature_embedding_dict[index] = get_word2vec_embedding(word, wv_model_2, w2v_dimensions_2)\n",
    "\n",
    "    index = 3\n",
    "    word_feature_embedding_dict[index] = get_word2vec_embedding(word, wv_model_3, w2v_dimensions_3)\n",
    "\n",
    "    index = 4\n",
    "    word_feature_embedding_dict[index] = get_word2vec_embedding(word, wv_model_4, w2v_dimensions_4)\n",
    "\n",
    "    index = 5\n",
    "    word_feature_embedding_dict[index] = get_word2vec_embedding(word, wv_model_5, w2v_dimensions_5)\n",
    "\n",
    "    '''NRC Emotion Intensity Lexicon'''\n",
    "    index = 6\n",
    "    word_feature_embedding_dict[index] = get_emo_int_vector(word)\n",
    "\n",
    "    '''WordNet'''\n",
    "    index = 7\n",
    "    word_feature_embedding_dict[index] = get_sentiwordnetscore(word)\n",
    "\n",
    "    '''NRC Sentiment Lexica'''\n",
    "    index = 8\n",
    "    word_feature_embedding_dict[index] = get_sentiment_emotion_feature(word)\n",
    "\n",
    "    index = 9\n",
    "    word_feature_embedding_dict[index] = get_unigram_sentiment_emoticon_lexicon_vector(word)\n",
    "\n",
    "    index = 10\n",
    "    word_feature_embedding_dict[index] = get_unigram_sentiment_emoticon_afflex_vector(word)\n",
    "\n",
    "    '''NRC Hashtag Lexica'''\n",
    "    index = 11\n",
    "    word_feature_embedding_dict[index] = get_hashtag_emotion_vector(word)\n",
    "\n",
    "    index = 12\n",
    "    word_feature_embedding_dict[index] = get_unigram_sentiment_hash_sent_lex_vector(word)\n",
    "\n",
    "    index = 13\n",
    "    word_feature_embedding_dict[index] = get_unigram_sentiment_hashtag_affneglex_vector(word)\n",
    "\n",
    "    '''Emoji Polarities'''\n",
    "    index = 14\n",
    "    word_feature_embedding_dict[index] = get_emoji_intensity(word)\n",
    "    \n",
    "    '''Depeche Mood'''\n",
    "    index = 15\n",
    "    word_feature_embedding_dict[index] = get_depeche_mood_vector(word)\n",
    "\n",
    "    return word_feature_embedding_dict\n",
    "\n",
    "\n",
    "def learn_bigram_word_embedding(word):\n",
    "    \n",
    "    word_feature_embedding_dict = dict()\n",
    "    \n",
    "    '''NRC Sentiment Lexica'''\n",
    "\n",
    "    index = 0\n",
    "    word_feature_embedding_dict[index] = get_bigram_sentiment_emoticon_lexicon_vector(word)\n",
    "\n",
    "    index = 1\n",
    "    word_feature_embedding_dict[index] = get_bigram_sentiment_emoticon_afflex_vector(word)\n",
    "\n",
    "    '''NRC Hashtag Lexica'''\n",
    "    index = 2\n",
    "    word_feature_embedding_dict[index] = get_bigram_sentiment_hash_sent_lex_vector(word)\n",
    "\n",
    "    index = 3\n",
    "    word_feature_embedding_dict[index] = get_bigram_sentiment_hashtag_affneglex_vector(word)\n",
    "\n",
    "    return word_feature_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_embedding(word, word_embedding_dict, bin_string):\n",
    "    \n",
    "    word_feature_embedding_dict = word_embedding_dict[word]\n",
    "    final_embedding = np.array([])\n",
    "    \n",
    "    for i in range(16):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding\n",
    "\n",
    "def get_bigram_embedding(bigram, word_embedding_dict, bin_string):\n",
    "    \n",
    "    word_feature_embedding_dict = word_embedding_dict[word]\n",
    "    final_embedding = np.array([])\n",
    "    \n",
    "    for i in range(4):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feature_string = \"1111111111111111\"\n",
    "bigram_feature_string = \"1111\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining_tweets = read_training_data(training_data_file_path)\\ndev_tweets = read_training_data(dev_set_path)\\n\\nscore_train = list()\\ntweet_train = list()\\nfor tweet in training_tweets:\\n    tweet_train.append(tweet.text)\\n    #score_train.append(float(tweet.intensity))\\n\\nfor tweet in dev_tweets:\\n    tweet_train.append(tweet.text)\\n    #score_train.append(float(tweet.intensity))    \\nprint(len(score_train))\\nscore_train = np.asarray(score_train)\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "training_tweets = read_training_data(training_data_file_path)\n",
    "dev_tweets = read_training_data(dev_set_path)\n",
    "\n",
    "score_train = list()\n",
    "tweet_train = list()\n",
    "for tweet in training_tweets:\n",
    "    tweet_train.append(tweet.text)\n",
    "    #score_train.append(float(tweet.intensity))\n",
    "\n",
    "for tweet in dev_tweets:\n",
    "    tweet_train.append(tweet.text)\n",
    "    #score_train.append(float(tweet.intensity))    \n",
    "print(len(score_train))\n",
    "score_train = np.asarray(score_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nraw_test_tweets = read_training_data_verbatim(test_data_file_path)\\ntest_tweets = read_training_data(test_data_file_path)\\n\\ntweet_test_raw = list()\\ntweet_test = list()\\ny_gold = list()\\n\\nfor tweet in raw_test_tweets:\\n    tweet_test_raw.append(tweet.text)\\n\\nfor tweet in test_tweets:\\n    tweet_test.append(tweet.text)\\n    y_gold.append(float(tweet.intensity))\\n    \\nprint(len(y_gold))\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "raw_test_tweets = read_training_data_verbatim(test_data_file_path)\n",
    "test_tweets = read_training_data(test_data_file_path)\n",
    "\n",
    "tweet_test_raw = list()\n",
    "tweet_test = list()\n",
    "y_gold = list()\n",
    "\n",
    "for tweet in raw_test_tweets:\n",
    "    tweet_test_raw.append(tweet.text)\n",
    "\n",
    "for tweet in test_tweets:\n",
    "    tweet_test.append(tweet.text)\n",
    "    y_gold.append(float(tweet.intensity))\n",
    "    \n",
    "print(len(y_gold))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_embeddings(tweets):\n",
    "    \n",
    "    max_tweet_length = -1\n",
    "    word_embedding_dict = dict()\n",
    "\n",
    "    for tweet in tweets:\n",
    "        if len(tweet) > max_tweet_length:\n",
    "            max_tweet_length = len(tweet)\n",
    "\n",
    "        for token in tweet:\n",
    "            if token not in word_embedding_dict.keys():\n",
    "                word_embedding_dict[token] = learn_unigram_word_embedding(token)\n",
    "                \n",
    "    return word_embedding_dict, max_tweet_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpora_utils import CorporaHelper, CorporaProperties\n",
    "corpora_helper = CorporaHelper(\"multigenre.csv\")\n",
    "\n",
    "corpora_helper.translate_urls()\n",
    "corpora_helper.translate_emoticons()\n",
    "corpora_helper.translate_emojis()\n",
    "corpora_helper.translate_email()\n",
    "corpora_helper.translate_mention()\n",
    "corpora_helper.translate_html_tags()\n",
    "corpora_helper.translate_camel_case()\n",
    "corpora_helper.translate_underscore()\n",
    "\n",
    "corpora_helper.translate_string('-LRB-','(')\n",
    "corpora_helper.translate_string('-RRB-',')')\n",
    "corpora_helper.translate_string('`',\"'\") # ` to '\n",
    "corpora_helper.translate_string(\"''\",'\"') # double '' to \"\n",
    "corpora_helper.translate_contractions()\n",
    "corpora_helper.translate_string(\"'\",\"\") # remove '\n",
    "corpora_helper.translate_string(\"\\\\n\",\" \") # replace new lines with space\n",
    "\n",
    "#corpora_helper.spell_correction()\n",
    "corpora_helper.add_space_at_word_puntuation()\n",
    "corpora_helper.translate_to_lower()\n",
    "\n",
    "corpora = list()\n",
    "for corpus_id, corpus in corpora_helper.get_data().iterrows():\n",
    "# tokenize the cleaned corpora\n",
    "    corpora.append(list(tknzr.tokenize(corpus[CorporaProperties.CLEANED_CORPUS.value])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = corpora\n",
    "embedding_info = build_word_embeddings(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(embedding_info, open(\"multigenre_embedding.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectors\n",
    "with open(word_embeddings_path, 'wb') as word_embeddings_file:\n",
    "     pickle.dump(embedding_info, word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_notify(\"Persisted to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore vectors\n",
    "with open(word_embeddings_path, 'rb') as word_embeddings_file:\n",
    "    embedding_info = pickle.load(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = embedding_info[0]\n",
    "MAX_SEQUENCE_LENGTH = embedding_info[1]\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = len(get_unigram_embedding(\"glad\", embedding_info[0], unigram_feature_string))\n",
    "print(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = dict()\n",
    "current_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_tweets(tweets):\n",
    "    global current_index\n",
    "    vectors = list()\n",
    "    for tweet in tweets:        \n",
    "        vector = list()\n",
    "        for word in tweet:\n",
    "            word_index = None\n",
    "            \n",
    "            if word in word_indices:\n",
    "                word_index = word_indices[word]\n",
    "            else:\n",
    "                word_index = current_index\n",
    "                current_index += 1\n",
    "                word_indices[word] = word_index\n",
    "            \n",
    "            vector.append(word_index)\n",
    "        \n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tweet_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = list()\n",
    "word_embedding_matrix.append(np.zeros(EMBEDDING_DIM))\n",
    "\n",
    "for word in sorted(word_indices, key=word_indices.get):\n",
    "    embedding_features = get_unigram_embedding(word, embedding_info[0], unigram_feature_string)    \n",
    "    word_embedding_matrix.append(embedding_features)\n",
    "\n",
    "word_embedding_matrix = np.asarray(word_embedding_matrix, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = scale(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_notify(\"Vectorization Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wassa_home + 'word_indices_new.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_indices, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
