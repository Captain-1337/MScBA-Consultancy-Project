{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import html\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_alert(message):\n",
    "    display(HTML('<script type=\"text/javascript\">alert(\"' + message + '\");</script>'))\n",
    "    \n",
    "def browser_notify(message):\n",
    "    display(HTML('<script type=\"text/javascript\">var notification=new Notification(\"Jupyter Notification\",{icon:\"http://blog.jupyter.org/content/images/2015/02/jupyter-sq-text.png\",body:\"' + message + '\"});</script>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_path = \"/Users/COMMANDER/Desktop/MScBA Consultantcy Project/GloVe/glove.twitter.27B/\"\n",
    "glove_path_6B = \"/Users/COMMANDER/Desktop/MScBA Consultantcy Project/GloVe/glove.6B/\"\n",
    "glove_path_42B = \"/Users/COMMANDER/Desktop/MScBA Consultantcy Project/GloVe/glove.42B.300d/\"\n",
    "glove_path_840B = \"/Users/COMMANDER/Desktop/MScBA Consultantcy Project/GloVe/glove.840B.300d/\"\n",
    "glove_path_twitter = \"/Users/COMMANDER/Desktop/MScBA Consultantcy Project/GloVe/glove.twitter.27B/\"\n",
    "wassa_home = \"/Users/COMMANDER/Downloads/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec + GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',encoding=\"utf8\")\n",
    "    model = {}\n",
    "    num = 1\n",
    "    for line in f:\n",
    "        try:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = [float(val) for val in splitLine[1:]]\n",
    "            model[word] = np.array(embedding)\n",
    "            num += 1\n",
    "        except Exception as e:\n",
    "            print(\"Failed at line \" + str(num))\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google news pretrained vectors\n",
    "wv_model_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter pretrained vectors\n",
    "wv_model_path_1 = \"word2vec_twitter_model.bin\"\n",
    "wv_model_1 = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path_1, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_path_2 = glove_path_twitter + \"glove.twitter.27B.200d.txt\"\n",
    "wv_model_2 = loadGloveModel(wv_model_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_path_3 = glove_path_6B + \"glove.6B.300d.txt\"\n",
    "wv_model_3 = loadGloveModel(wv_model_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_path_4 = glove_path_42B + \"glove.42B.300d.txt\"\n",
    "wv_model_4 = loadGloveModel(wv_model_path_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_path_5 = glove_path_840B + \"glove.840B.300d.txt\"\n",
    "wv_model_5 = loadGloveModel(wv_model_path_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dimensions = len(wv_model['word'])\n",
    "w2v_dimensions_1 = len(wv_model_1['word'])\n",
    "w2v_dimensions_2 = len(wv_model_2['word'])\n",
    "w2v_dimensions_3 = len(wv_model_3['word'])\n",
    "w2v_dimensions_4 = len(wv_model_4['word'])\n",
    "w2v_dimensions_5 = len(wv_model_5['word'])\n",
    "\n",
    "print(w2v_dimensions, w2v_dimensions_1, w2v_dimensions_2, w2v_dimensions_3, w2v_dimensions_4, w2v_dimensions_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(word, model, dimensions):\n",
    "\n",
    "    vec_rep = np.zeros(dimensions)\n",
    "    if word in model:\n",
    "        vec_rep = model[word]\n",
    "    \n",
    "    return vec_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word2vec_embedding(\"charm\", wv_model_1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string):\n",
    "    split_string = \\\n",
    "        [word for word in string.split()\n",
    "         if word not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(split_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):  \n",
    "    string = html.unescape(string)\n",
    "    string = string.replace(\"\\\\n\", \" \")\n",
    "    string = string.replace(\"_NEG\", \"\")\n",
    "    string = string.replace(\"_NEGFIRST\", \"\")\n",
    "    string = re.sub(r\"@[A-Za-z0-9_(),!?\\'\\`]+\", \" \", string) # removing any twitter handle mentions\n",
    "    string = re.sub(r\"\\d+\", \" \", string) # removing any words with numbers\n",
    "    string = re.sub(r\"_\", \" \", string)\n",
    "    string = re.sub(r\":\", \" \", string)\n",
    "    string = re.sub(r\"/\", \" \", string)\n",
    "    string = re.sub(r\"#\", \" \", string)\n",
    "    string = re.sub(r\"\\.\", \" \", string)\n",
    "    string = re.sub(r\"\\*\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \", string)\n",
    "    string = re.sub(r\"\\'m\", \" am\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" have\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" not\", string)\n",
    "    string = re.sub(r\"n\\’t\", \" not\", string)\n",
    "    string = re.sub(r\"\\'re\", \" are\", string)\n",
    "    string = re.sub(r\"\\’re\", \" are\", string)\n",
    "    string = re.sub(r\"\\'d\", \" would\", string)\n",
    "    string = re.sub(r\"\\’d\", \" would\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" will\", string)\n",
    "    string = re.sub(r\"\\’ll\", \" will\", string)\n",
    "    string = re.sub(r\"'\", \" \", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" !\", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" ?\", string)\n",
    "    string = re.sub(r\"-\", \" \", string)\n",
    "    string = re.sub(r\"<\", \" \", string)\n",
    "    string = re.sub(r\">\", \" \", string)\n",
    "    string = re.sub(r\";\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return remove_stopwords(string.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "\n",
    "    def __init__(self, id, text, emotion, intensity):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.emotion = emotion\n",
    "        self.intensity = intensity\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \\\n",
    "            \"id: \" + self.id + \\\n",
    "            \", text: \" + self.text + \\\n",
    "            \", emotion: \" + self.emotion + \\\n",
    "            \", intensity: \" + self.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data(training_data_file_path):\n",
    "\n",
    "    train_list = list()\n",
    "    with open(training_data_file_path, encoding=\"utf8\") as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            train_list.append(Tweet(array[0], list(tknzr.tokenize(clean_str(array[1]))), \n",
    "                                    array[2], str(array[3])))\n",
    "    return train_list\n",
    "            \n",
    "def read_training_data_verbatim(training_data_file_path):\n",
    "\n",
    "    train_list = list()\n",
    "    with open(training_data_file_path, encoding=\"utf8\") as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            train_list.append(Tweet(array[0], array[1], array[2], str(array[3])))\n",
    "    return train_list\n",
    "    \n",
    "def read_test_data(training_data_file_path):\n",
    "\n",
    "    test_list = list()\n",
    "    with open(training_data_file_path, encoding=\"utf8\") as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            test_list.append(Tweet(array[0], clean_str(array[1]), array[2], None))\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_linear_factor = PolynomialFeatures(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = \"anger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file_path = \\\n",
    "    wassa_home + \"corpora/multigenre/\" + \\\n",
    "    emotion + \"-ratings-0to1.train.txt\"\n",
    "word_embeddings_path = wassa_home + \"embeddings/\" + emotion + \"-word-embeddings.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction Snippets\n",
    "\n",
    "Emoji Intensity (Thanks to Roger, it works!)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/COMMANDER/Downloads/lexicons/index_emoji.json', encoding=\"utf8\") as emoji_file:\n",
    "    emoji_list = json.load(emoji_file)\n",
    "    \n",
    "emoji_dict = dict()\n",
    "\n",
    "for emoji in emoji_list:\n",
    "    emoji_dict[emoji[\"emoji\"]] = (emoji[\"name\"], emoji[\"polarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoji_intensity(word):\n",
    "    \n",
    "    score = 0.0\n",
    "    if word in emoji_dict.keys():\n",
    "        score = float(emoji_dict[word][1])\n",
    "    \n",
    "    vec_rep = np.array([score])\n",
    "    \n",
    "    \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_intensity_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-AffectIntensity-Lexicon.txt\"\n",
    "    \n",
    "def get_word_affect_intensity_dict(emotion):\n",
    "    word_intensities = dict()\n",
    "\n",
    "    with open(affect_intensity_file_path) as affect_intensity_file:\n",
    "        for line in affect_intensity_file:\n",
    "            word_int_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_int_array[2] == emotion):\n",
    "                word_intensities[word_int_array[0]] = float(word_int_array[1])\n",
    "\n",
    "    return word_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_intensities = get_word_affect_intensity_dict(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emo_int_vector(word):\n",
    "    \n",
    "    score = 0.0\n",
    "    if word in word_intensities.keys():\n",
    "        score = float(word_intensities[word])\n",
    "        \n",
    "    vec_rep = np.array([score])\n",
    "    \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_sentiwordnetscore(word):\n",
    "    \n",
    "    vec_rep = np.zeros(2)\n",
    "    \n",
    "    synsetlist = list(swn.senti_synsets(word))\n",
    "\n",
    "    if synsetlist:\n",
    "        vec_rep[0] = synsetlist[0].pos_score()\n",
    "        vec_rep[1] = synsetlist[0].neg_score()\n",
    "\n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Emotion Presence Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wassa_home' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9291797db117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentiment_emotion_lex_file_path\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mwassa_home\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;34m\"Lexikon/NRC-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/\"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wassa_home' is not defined"
     ]
    }
   ],
   "source": [
    "sentiment_emotion_lex_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"Lexikon/NRC-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/\" + \\\n",
    "    \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "def get_affect_presence_list(emotion):\n",
    "    word_list = list()\n",
    "    \n",
    "    with open(sentiment_emotion_lex_file_path) as sentiment_emotion_lex_file:\n",
    "        for line in sentiment_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "\n",
    "            if (word_array[1] == emotion and word_array[2] == '1'):\n",
    "                word_list.append(word_array[0])\n",
    "    return word_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_emotion_lex_word_list = get_affect_presence_list(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_emotion_feature(word):\n",
    "    \n",
    "    score = 0.0\n",
    "    if word in sentiment_emotion_lex_word_list:\n",
    "        score = 1.0\n",
    "    vec_rep = np.array([score])\n",
    "    \n",
    "    return non_linear_factor.fit_transform([vec_rep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Emotion Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_emotion_lex_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"Lexikon/NRC-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/\" + \\\n",
    "    \"NRC-Hashtag-Emotion-Lexicon-v0.2.txt\"\n",
    "    \n",
    "def get_hashtag_emotion_intensity(emotion):\n",
    "    hastag_intensities = dict()\n",
    "    \n",
    "    with open(hashtag_emotion_lex_file_path) as hashtag_emotion_lex_file:\n",
    "        for line in hashtag_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if (word_array[0] == emotion):\n",
    "                hastag_intensities[word_array[1]] = float(word_array[2])\n",
    "                \n",
    "    return hastag_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_emotion_intensities = get_hashtag_emotion_intensity(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python37164bitbasecondaa7b2c48f2e394be3b27942eeeab651d1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
