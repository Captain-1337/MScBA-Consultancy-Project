{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, LSTM, GRU, AveragePooling1D, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from corpora_utils import CorporaHelper, CorporaDomains, CorporaProperties\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Activate GPU\n",
    "#WARNING GPU TAKES 5 TIMES LONGER THAN CPU! With Consul Project 1\n",
    "#Check for GPU\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n",
    "# GPU CONFIG\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\"\"\"\n",
    "\n",
    "#Deactivate GPU\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIGENRE = 'muligenre'\n",
    "TWITTER = 'twitter'\n",
    "MG_AND_TWITTER = 'mg_and_twitter'\n",
    "\n",
    "# set wich corpora to use Multigenre or twitter\n",
    "use_mg_train_corpora = TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_labels = []\n",
    "train_texts = []\n",
    "test_labels = []\n",
    "test_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpora(filepath, sep=';'):\n",
    "    print('Load: ', filepath)\n",
    "    corpora_helper = CorporaHelper(filepath, separator=sep)\n",
    "    count_joy = 0\n",
    "    count_sadness = 0\n",
    "    count_anger = 0\n",
    "    count_fear = 0\n",
    "    labels = []\n",
    "    texts = []\n",
    "    # preprocessing corpora\n",
    "    corpora_helper.translate_urls()\n",
    "    corpora_helper.translate_emoticons()\n",
    "    corpora_helper.translate_emojis()\n",
    "    corpora_helper.translate_email()\n",
    "    #corpora_helper.translate_mention()\n",
    "    corpora_helper.translate_html_tags()\n",
    "    #corpora_helper.translate_camel_case()\n",
    "    corpora_helper.translate_underscore()\n",
    "\n",
    "    corpora_helper.translate_string('-LRB-','(')\n",
    "    corpora_helper.translate_string('-RRB-',')')\n",
    "    corpora_helper.translate_string('`',\"'\") # ` to '\n",
    "    corpora_helper.translate_string(\"''\",'\"') # double '' to \"\n",
    "    corpora_helper.translate_contractions()\n",
    "    corpora_helper.translate_string(\"'\",\"\") # remove '\n",
    "    corpora_helper.translate_string(\"\\\\n\",\" \") # replace new lines with space\n",
    "\n",
    "    #corpora_helper.spell_correction()\n",
    "    corpora_helper.add_space_at_special_chars()\n",
    "    corpora_helper.add_space_at_special_chars(regexlist = r\"([#])\")\n",
    "    #corpora_helper.translate_to_lower()\n",
    "\n",
    "    # 0 anger\n",
    "    # 1 fear\n",
    "    # 2 joy\n",
    "    # 3 sadness\n",
    "    for index, corpus in corpora_helper.get_data().iterrows():\n",
    "        if corpus[CorporaProperties.EMOTION.value] == 'anger':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(0)\n",
    "            count_anger += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'fear':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(1)\n",
    "            count_fear += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'joy':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(2)\n",
    "            count_joy += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'sadness':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(3)\n",
    "            count_sadness += 1\n",
    "    print('number of anger labels: ',count_anger)\n",
    "    print('number of fear labels: ', count_fear)\n",
    "    print('number of joy labels: ', count_joy)\n",
    "    print('number of sadness labels: ', count_sadness)\n",
    "    print('----------------------------------------------------------------------')\n",
    "    return texts, labels\n",
    "    #max_data = count_anger + count_fear + count_joy + count_sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use TWITTER train corpora\n",
      "Load:  corpora/twitter_2000_train.csv\n",
      "number of anger labels:  1800\n",
      "number of fear labels:  1800\n",
      "number of joy labels:  1800\n",
      "number of sadness labels:  1800\n",
      "----------------------------------------------------------------------\n",
      "Load:  corpora/twitter_2000_test.csv\n",
      "number of anger labels:  200\n",
      "number of fear labels:  200\n",
      "number of joy labels:  200\n",
      "number of sadness labels:  200\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_file = \"\"\n",
    "test_file = \"\"\n",
    "sep = ';'\n",
    "word_embeddings_path = ''\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    train_file = \"corpora/multigenre_450_train.csv\"\n",
    "    test_file = \"corpora/multigenre_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = ';'\n",
    "    print(\"Use MULTIGENRE train corpora\")\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    train_file = \"corpora/twitter_2000_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER train corpora\")\n",
    "else:\n",
    "    train_file = \"corpora/twitter_2000_mg_450_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_mg_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER and MULTIGENRE train corpora\")\n",
    "    \n",
    "train_texts, train_labels = load_corpora(train_file, sep=sep)\n",
    "test_texts, test_labels = load_corpora(test_file, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared custom ensemble embedding\n",
    "with open(word_embeddings_path, 'rb') as word_embeddings_file:\n",
    "    embedding_info = pickle.load(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding helper functions\n",
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "    \n",
    "def get_unigram_embedding(word, word_embedding_dict, bin_string):\n",
    "    \n",
    "    if word in word_embedding_dict:\n",
    "        word_feature_embedding_dict = word_embedding_dict[word]\n",
    "        final_embedding = np.array([])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for i in range(16):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen:  100\n"
     ]
    }
   ],
   "source": [
    "pre_padding = 0\n",
    "embeddings_index = embedding_info[0]\n",
    "MAX_SEQUENCE_LENGTH = embedding_info[1]\n",
    "maxlen = MAX_SEQUENCE_LENGTH\n",
    "print(\"maxlen: \",maxlen)\n",
    "#MAX_NB_WORDS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Unigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting relevant embeddings for multigenre\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    # Multigenre\n",
    "    unigram_feature_string = \"1001111111111101\"\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    # Twitter\n",
    "    unigram_feature_string = \"0110001111111101\"\n",
    "    unigram_feature_string = \"1111111111111111\"\n",
    "else:\n",
    "    # Twitter and Multigenre\n",
    "    unigram_feature_string = \"1110010000000000\"\n",
    "# 1 Google news pretrained vectors : GoogleNews-vectors-negative300.bin.gz  \n",
    "# 2 Twitter pretrained vectors: word2vec_twitter_model.bin\n",
    "# 3 glove.twitter.27B.200d.txt\n",
    "# 4 glove.6B.300d.txt\n",
    "# 5 glove.42B.300d.txt\n",
    "# 6 glove.840B.300d.txt\n",
    "# 7 NRC Emotion Intensity Lexicon\n",
    "# 8 senti word net\n",
    "#9  NRC Sentiment lexicon: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#10 lexicons/Emoticon-unigrams.txt\n",
    "#11 lexicons/Emoticon-AFFLEX-NEGLEX-unigrams.txt\n",
    "#12 NRC Hashtag Lexica: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#13 HS-unigrams.txtNRC-Hashtag-Emotion-Lexicon-v0.2.txt\n",
    "#14 HS-AFFLEX-NEGLEX-unigrams.txt\n",
    "#15 Emoji Polarities\n",
    "#16 Depeche mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep learning with multigenre and twitter corpus and 4 emotions\n",
    "\"\"\"\n",
    "# K-Fold variables\n",
    "num_folds = 3 # 10\n",
    "fold_runs = 2 # 3\n",
    "fold_no = 1\n",
    "# train\n",
    "epochs = 4\n",
    "max_words = 20000\n",
    "# max. different words:\n",
    "# Multigerne: 5140  => 10000 or 3000 or 1000 ?\n",
    "# Twitter: 17580 => 20000 or 10000 ?\n",
    "# MG and Twitter: 20073 => evtl. 20000?\n",
    "#optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer = Adam(learning_rate=0.001) # default 0.001\n",
    "skfold = StratifiedKFold(n_splits = num_folds, random_state = 7, shuffle = True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_per_fold = []\n",
    "avg_acc_per_run = []\n",
    "avg_loss_per_run = []\n",
    "avg_precision_per_run = []\n",
    "avg_recall_per_run = []\n",
    "avg_f1_per_run = []\n",
    "create_final_model = True\n",
    "# run only final model without kfold\n",
    "run_final_train_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 2164\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = len(get_unigram_embedding(\"glad\", embedding_info[0], unigram_feature_string))\n",
    "print(\"Embedding dimension:\",EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, filters = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train an test data set\n",
    "def create_data(texts, labels, maxlen):\n",
    "    ## Create one hot encoding\n",
    "    #max_words = 10000\n",
    "    #maxlen = 100 # max. number of words in sequences\n",
    "    #tokenizer = Tokenizer(num_words=max_words, filters = '')\n",
    "    #tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    #word_i = tokenizer.word_index\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    labels_arr = np.asarray(labels)\n",
    "    print('Shape of data:', data.shape)\n",
    "    print('Shape of labels:', labels_arr.shape)\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "    # mix the data\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels_arr = labels_arr[indices]\n",
    "\n",
    "    # split in train and validate\n",
    "    x_data = data\n",
    "    y_data = labels_arr\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer\n",
    "all_texts = train_texts.copy()\n",
    "all_texts.append(test_texts.copy())\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (7200, 100)\n",
      "Shape of labels: (7200,)\n",
      "-------------------------------------------\n",
      "Shape of data: (800, 100)\n",
      "Shape of labels: (800,)\n",
      "-------------------------------------------\n",
      "16580 unique Tokens found.\n"
     ]
    }
   ],
   "source": [
    "# Train an word index for embedding enrichment\n",
    "x_train, y_train = create_data(train_texts, train_labels, maxlen)\n",
    "x_test, y_test = create_data(test_texts, test_labels, maxlen)\n",
    "word_index = tokenizer.word_index\n",
    "x_train_copy = x_train.copy()\n",
    "y_train_copy = y_train.copy()\n",
    "print ('%s unique Tokens found.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Matrix\n",
    "word_embedding_matrix = list()\n",
    "word_embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "#word_embedding_matrix.append(np.zeros(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items(): # sorted(word_indices, key=word_indices.get):\n",
    "    embedding_features = get_unigram_embedding(word, embedding_info[0], unigram_feature_string)\n",
    "    if i < max_words:\n",
    "        if embedding_features is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            word_embedding_matrix[i] = embedding_features\n",
    "\n",
    "word_embedding_matrix = np.asarray(word_embedding_matrix, dtype='f')\n",
    "word_embedding_matrix = scale(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM 2164\n",
      "input_length 100\n"
     ]
    }
   ],
   "source": [
    "#print('word_indices_len',word_indices_len)\n",
    "print('EMBEDDING_DIM',EMBEDDING_DIM)\n",
    "print('input_length', MAX_SEQUENCE_LENGTH + pre_padding)\n",
    "embedding = Embedding(max_words, EMBEDDING_DIM, input_length=maxlen, trainable=False)\n",
    "#embedding = Embedding(word_indices_len + 1, EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH + pre_padding, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running \"optimal\" BiLSTM Model with 3 Runs and 10 k-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Bidirectional(LSTM(32, dropout=0.4, recurrent_dropout=0.4, return_sequences=True)))   \n",
    "    model.add(Dense(16, activation='relu'))   \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 1 ...\n",
      "Train on 4800 samples, validate on 2400 samples\n",
      "Epoch 1/4\n",
      "4800/4800 [==============================] - 27s 6ms/step - loss: 1.0713 - acc: 0.5487 - val_loss: 0.6106 - val_acc: 0.7904\n",
      "Epoch 2/4\n",
      "4800/4800 [==============================] - 26s 5ms/step - loss: 0.4824 - acc: 0.8415 - val_loss: 0.3959 - val_acc: 0.8583\n",
      "Epoch 3/4\n",
      "4800/4800 [==============================] - 26s 5ms/step - loss: 0.3145 - acc: 0.8940 - val_loss: 0.3581 - val_acc: 0.8717\n",
      "Epoch 4/4\n",
      "4800/4800 [==============================] - 26s 5ms/step - loss: 0.2376 - acc: 0.9215 - val_loss: 0.3342 - val_acc: 0.8763\n",
      "2400/2400 [==============================] - 5s 2ms/step\n",
      "Score for fold 1: loss of 0.3342018474141757; accuracy of 87.62500286102295%\n",
      "2400/2400 [==============================] - 5s 2ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 1 ...\n",
      "Train on 4800 samples, validate on 2400 samples\n",
      "Epoch 1/4\n",
      "4800/4800 [==============================] - 27s 6ms/step - loss: 0.8419 - acc: 0.6771 - val_loss: 0.4200 - val_acc: 0.8512\n",
      "Epoch 2/4\n",
      "4800/4800 [==============================] - 25s 5ms/step - loss: 0.3650 - acc: 0.8700 - val_loss: 0.3025 - val_acc: 0.8846\n",
      "Epoch 3/4\n",
      "4800/4800 [==============================] - 25s 5ms/step - loss: 0.2525 - acc: 0.9121 - val_loss: 0.2779 - val_acc: 0.8896\n",
      "Epoch 4/4\n",
      "4800/4800 [==============================] - 25s 5ms/step - loss: 0.2050 - acc: 0.9256 - val_loss: 0.2650 - val_acc: 0.8963\n",
      "2400/2400 [==============================] - 4s 2ms/step\n",
      "Score for fold 2: loss of 0.2650146943082412; accuracy of 89.62500095367432%\n",
      "2400/2400 [==============================] - 4s 2ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 1 ...\n",
      "Train on 4800 samples, validate on 2400 samples\n",
      "Epoch 1/4\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.6996 - acc: 0.7183 - val_loss: 0.4053 - val_acc: 0.8554\n",
      "Epoch 2/4\n",
      "4800/4800 [==============================] - 27s 6ms/step - loss: 0.3039 - acc: 0.8923 - val_loss: 0.3431 - val_acc: 0.8742\n",
      "Epoch 3/4\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.2134 - acc: 0.9262 - val_loss: 0.3340 - val_acc: 0.8746\n",
      "Epoch 4/4\n",
      "4800/4800 [==============================] - 28s 6ms/step - loss: 0.1682 - acc: 0.9425 - val_loss: 0.3343 - val_acc: 0.8767\n",
      "2400/2400 [==============================] - 4s 2ms/step\n",
      "Score for fold 3: loss of 0.3343098497390747; accuracy of 87.66666650772095%\n",
      "2400/2400 [==============================] - 6s 3ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3342018474141757 - Accuracy: 87.62500286102295%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2650146943082412 - Accuracy: 89.62500095367432%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3343098497390747 - Accuracy: 87.66666650772095%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 88.3055567741394 (+- 0.9331429586958481)\n",
      "> Loss: 0.31117546382049716\n",
      "> Precision: 0.8837931941650105\n",
      "> Recall: 0.8830555555555556\n",
      "> F1: 0.8832291177424576\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 2 ...\n",
      "Train on 4800 samples, validate on 2400 samples\n",
      "Epoch 1/4\n",
      "4800/4800 [==============================] - 32s 7ms/step - loss: 0.8284 - acc: 0.6694 - val_loss: 0.4684 - val_acc: 0.8404\n",
      "Epoch 2/4\n",
      "4800/4800 [==============================] - 30s 6ms/step - loss: 0.3537 - acc: 0.8788 - val_loss: 0.4015 - val_acc: 0.8571\n",
      "Epoch 3/4\n",
      "4800/4800 [==============================] - 30s 6ms/step - loss: 0.2591 - acc: 0.9069 - val_loss: 0.3705 - val_acc: 0.8721\n",
      "Epoch 4/4\n",
      "4800/4800 [==============================] - 29s 6ms/step - loss: 0.2008 - acc: 0.9310 - val_loss: 0.3592 - val_acc: 0.8712\n",
      "2400/2400 [==============================] - 5s 2ms/step\n",
      "Score for fold 1: loss of 0.3591779402891795; accuracy of 87.12499737739563%\n",
      "2400/2400 [==============================] - 8s 3ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 2 ...\n",
      "Train on 4800 samples, validate on 2400 samples\n",
      "Epoch 1/4\n",
      "4800/4800 [==============================] - 37s 8ms/step - loss: 0.7449 - acc: 0.7104 - val_loss: 0.3984 - val_acc: 0.8567\n",
      "Epoch 2/4\n",
      "4800/4800 [==============================] - 44s 9ms/step - loss: 0.3398 - acc: 0.8712 - val_loss: 0.3070 - val_acc: 0.8813\n",
      "Epoch 3/4\n",
      "4800/4800 [==============================] - 43s 9ms/step - loss: 0.2493 - acc: 0.9104 - val_loss: 0.2901 - val_acc: 0.8871\n",
      "Epoch 4/4\n",
      "4800/4800 [==============================] - 43s 9ms/step - loss: 0.2018 - acc: 0.9273 - val_loss: 0.2846 - val_acc: 0.8854\n",
      "2400/2400 [==============================] - 8s 3ms/step\n",
      "Score for fold 2: loss of 0.2846303924669822; accuracy of 88.54166865348816%\n",
      "2400/2400 [==============================] - 13s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 2 ...\n",
      "Train on 4800 samples, validate on 2400 samples\n",
      "Epoch 1/4\n",
      "4800/4800 [==============================] - 46s 10ms/step - loss: 0.7267 - acc: 0.7100 - val_loss: 0.4074 - val_acc: 0.8529\n",
      "Epoch 2/4\n",
      "4800/4800 [==============================] - 71s 15ms/step - loss: 0.3036 - acc: 0.8917 - val_loss: 0.3246 - val_acc: 0.8754\n",
      "Epoch 3/4\n",
      "4800/4800 [==============================] - 71s 15ms/step - loss: 0.2239 - acc: 0.9215 - val_loss: 0.3335 - val_acc: 0.8775\n",
      "Epoch 4/4\n",
      "4800/4800 [==============================] - 70s 15ms/step - loss: 0.1749 - acc: 0.9408 - val_loss: 0.3141 - val_acc: 0.8804\n",
      "2400/2400 [==============================] - 8s 3ms/step\n",
      "Score for fold 3: loss of 0.31406805286804834; accuracy of 88.04166913032532%\n",
      "2400/2400 [==============================] - 13s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3591779402891795 - Accuracy: 87.12499737739563%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2846303924669822 - Accuracy: 88.54166865348816%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.31406805286804834 - Accuracy: 88.04166913032532%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 87.9027783870697 (+- 0.5866329659153622)\n",
      "> Loss: 0.3192921285414034\n",
      "> Precision: 0.8798281409099166\n",
      "> Recall: 0.8790277777777779\n",
      "> F1: 0.8791973157436658\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Score for k-fold runs\n",
      "------------------------------------------------------------------------\n",
      "> Run 1 Fold averages - Loss: 0.31117546382049716 - Accuracy: 88.3055567741394% \n",
      "> Run 1 Fold averages - Precision: 0.8837931941650105 - Recall: 0.8830555555555556 F1: 0.8832291177424576\n",
      "------------------------------------------------------------------------\n",
      "> Run 2 Fold averages - Loss: 0.3192921285414034 - Accuracy: 87.9027783870697% \n",
      "> Run 2 Fold averages - Precision: 0.8798281409099166 - Recall: 0.8790277777777779 F1: 0.8791973157436658\n",
      "------------------------------------------------------------------------\n",
      "Overall average scores for all 2 runs:\n",
      "> Accuracy: 88.10416758060455 (+- 0.20138919353485107)\n",
      "> Loss: 0.31523379618095027\n",
      "> Precision: 0.8818106675374635\n",
      "> Recall: 0.8810416666666667\n",
      "> F1: 0.8812132167430617\n",
      "------------------------------------------------------------------------\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 2164)         43280000  \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 100, 64)           562432    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100, 16)           1040      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 43,843,540\n",
      "Trainable params: 563,540\n",
      "Non-trainable params: 43,280,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for final model ...\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/4\n",
      "7200/7200 [==============================] - 54s 8ms/step - loss: 0.9151 - acc: 0.6244 - val_loss: 0.4180 - val_acc: 0.8700\n",
      "Epoch 2/4\n",
      "7200/7200 [==============================] - 55s 8ms/step - loss: 0.3716 - acc: 0.8686 - val_loss: 0.3165 - val_acc: 0.8913\n",
      "Epoch 3/4\n",
      "7200/7200 [==============================] - 54s 8ms/step - loss: 0.2647 - acc: 0.9031 - val_loss: 0.2915 - val_acc: 0.8913\n",
      "Epoch 4/4\n",
      "7200/7200 [==============================] - 54s 7ms/step - loss: 0.2144 - acc: 0.9221 - val_loss: 0.2888 - val_acc: 0.9038\n",
      "Evaluate final model on test data\n",
      "800/800 [==============================] - 2s 3ms/step\n",
      "test loss, test acc: [0.2888066363334656, 0.9037500023841858]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZwV5Z3v8c+XBkGWQFhUpMFm0aBOAmIHDRDFiQtGo8ExLyGMcbnzIm43E3ONMTGLieGONzqj49XE6UxIosFgvC6jGZdEk+hMMpPQKAiuabHRDi4ICggiNPzuH1XdnD6c7i6aXsvv+/U6r1PLU3We5xR8+zlP1amjiMDMzPKrV1dXwMzMOpaD3sws5xz0ZmY556A3M8s5B72ZWc456M3Mcs5B/z4k6UFJ57R32a4kqVbS8R2w35A0IZ2+RdI3spRtw+vMk/SrttbTrCXydfQ9g6R3Cmb7A+8BO9L5z0fEos6vVfchqRb4u4h4pJ33G8DBEVHTXmUlVQAvAX0ior496mnWkt5dXQHLJiIGNky3FGqSejs8rLvwv8fuwUM3PZykmZLqJH1F0mvAjyV9UNIvJa2V9FY6XV6wze8k/V06fa6k/5R0XVr2JUknt7HsWEmPS9ok6RFJN0v6WTP1zlLHqyX9Pt3fryQNL1h/tqTVktZJurKF9+doSa9JKitYNlvSU+n0VEn/JeltSa9KuknSPs3s6yeSvlsw/+V0mzWSzi8qe4qkJyVtlPSKpKsKVj+ePr8t6R1JH2t4bwu2nyZpiaQN6fO0rO/NHr7PQyX9OG3DW5LuLVh3uqRlaRtelDQrXd5kmEzSVQ3HWVJFOoT1PyS9DPwmXX5nehw2pP9GDi/Yfl9J/5gezw3pv7F9Jf27pP9Z1J6nJH26VFuteQ76fDgAGAocBMwnOa4/TufHAO8CN7Ww/VHA88Bw4HvAjySpDWVvB/4EDAOuAs5u4TWz1PGzwHnAfsA+wGUAkg4DfpDu/8D09copISL+G9gM/HXRfm9Pp3cAl6bt+RjwCeCiFupNWodZaX1OAA4Gis8PbAY+BwwBTgEuLAioY9LnIRExMCL+q2jfQ4F/B25M2/ZPwL9LGlbUht3emxJae59vIxkKPDzd1/VpHaYCtwJfTttwDFDb3PtRwrHAocBJ6fyDJO/TfsATQOFQ43XAkcA0kn/HlwM7gZ8Cf9tQSNIkYBTwwB7UwwAiwo8e9iD5D3d8Oj0T2Ab0a6H8ZOCtgvnfkQz9AJwL1BSs6w8EcMCelCUJkXqgf8H6nwE/y9imUnX8esH8RcBD6fQ3gcUF6wak78Hxzez7u8DCdHoQSQgf1EzZLwL3FMwHMCGd/gnw3XR6IXBNQblDCsuW2O8NwPXpdEVatnfB+nOB/0ynzwb+VLT9fwHntvbe7Mn7DIwkCdQPlij3Lw31benfXzp/VcNxLmjbuBbqMCQtM5jkD9G7wKQS5foC60nOe0DyB+H7nf3/LQ8P9+jzYW1EbG2YkdRf0r+kH4U3kgwVDCkcvijyWsNERGxJJwfuYdkDgfUFywBeaa7CGev4WsH0loI6HVi474jYDKxr7rVIeu9nSOoLnAE8ERGr03ockg5nvJbW43+T9O5b06QOwOqi9h0l6bfpkMkG4IKM+23Y9+qiZatJerMNmntvmmjlfR5NcszeKrHpaODFjPUtpfG9kVQm6Zp0+Gcjuz4ZDE8f/Uq9VkS8B/wC+FtJvYC5JJ9AbA856POh+NKp/wV8CDgqIj7ArqGC5oZj2sOrwFBJ/QuWjW6h/N7U8dXCfaevOay5whHxDElQnkzTYRtIhoCeI+k1fgD4WlvqQPKJptDtwH3A6IgYDNxSsN/WLnVbQzLUUmgM8JcM9SrW0vv8CskxG1Jiu1eA8c3sczPJp7kGB5QoU9jGzwKnkwxvDSbp9TfU4U1gawuv9VNgHsmQ2pYoGuaybBz0+TSI5OPw2+l477c6+gXTHnI1cJWkfSR9DPhUB9Xx/wGnSpqRnjj9Dq3/W74d+AJJ0N1ZVI+NwDuSJgIXZqzDL4BzJR2W/qEprv8gkt7y1nS8+7MF69aSDJmMa2bfDwCHSPqspN6SzgIOA36ZsW7F9Sj5PkfEqyRj599PT9r2kdTwh+BHwHmSPiGpl6RR6fsDsAyYk5avBM7MUIf3SD519Sf51NRQh50kw2D/JOnAtPf/sfTTF2mw7wT+Effm28xBn083APuS9Jb+G3iok153HskJzXUk4+J3kPwHL6XNdYyIp4GLScL7VeAtoK6VzX5Ocj7jNxHxZsHyy0hCeBPww7TOWerwYNqG3wA16XOhi4DvSNpEck7hFwXbbgEWAL9XcrXP0UX7XgecStIbX0dycvLUonpn1dr7fDawneRTzRsk5yiIiD+RnOy9HtgAPMauTxnfIOmBvwV8m6afkEq5leQT1V+AZ9J6FLoMWAEsIRmT/z80zaZbgQ+TnPOxNvAXpqzDSLoDeC4iOvwTheWXpM8B8yNiRlfXpadyj97ajaSPShqfftSfRTIue29r25k1Jx0Wuwio6uq69GQOemtPB5Bc+vcOyTXgF0bEk11aI+uxJJ1Ecj7jdVofHrIWeOjGzCzn3KM3M8u5bnlTs+HDh0dFRUVXV8PMrMdYunTpmxExotS6bhn0FRUVVFdXd3U1zMx6DEnF36Zu5KEbM7Occ9CbmeWcg97MLOcc9GZmOeegNzPLOQe9mVkXW7QIKiqgV6/kedGi1rbYM93y8kozs/eLRYtg/nzYkv5kz+rVyTzAvHnt8xru0ZuZdaErr9wV8g22bEmWtxcHvZlZF3r55T1b3hYOejOzLjSm+EcoW1neFg56M7MutGAB9O/fdFn//sny9uKgNzPrQvPmQVUVHHQQSMlzVVX7nYgFX3VjZtbl5s1r32Av5h69mVnOOejNzHLOQW9mlnMOejOznHPQm5nlnIPe7H2mo2+gZd2PL680ex/pjBtoWXZbtsDatbse27bB6ae3/+soIlovJM0C/hkoA/41Iq4pWv9BYCEwHtgKnB8RK7NsW0plZWX4x8HN2l9FRRLuxQ46CGprO7s2+RIBGzcmgf3mm00DvLlH8c3MRoyAN95o2+tLWhoRlaXWtdqjl1QG3AycANQBSyTdFxHPFBT7GrAsImZLmpiW/0TGbc2sk3TGDbTyYudOeOutbIHdEO7btpXe1777JiHe8Dj00OR5+PCmy/fbr2PakmXoZipQExGrACQtBk4HCsP6MOAfACLiOUkVkvYHxmXY1sw6yZgxpXv07XkDre6qvr5pT7u1Xve6dbBjR+l9DRq0K5xHj4YpU5oGdvFjwIDObWuxLEE/CnilYL4OOKqozHLgDOA/JU0FDgLKM24LgKT5wHyAMe+Hf3VmXWDBgqZj9ND+N9DqLO+9l723vXZt0jtvztChu0L5kENg+vTdw7qw9923b+e1sz1kCXqVWFY8sH8N8M+SlgErgCeB+ozbJgsjqoAqSMboM9TLzPZQwwnXK69MhmvGjElCvqtPxEbA5s3ND4mUWr5pU+l9lZUlodwQzJMmtdzbHjYMeuf8spQszasDRhfMlwNrCgtExEbgPABJAl5KH/1b29bMOldH30ALkuDesGHPetxbt5be1z77NA3m8eNbDu4hQ5JLR22XLEG/BDhY0ljgL8Ac4LOFBSQNAbZExDbg74DHI2KjpFa3NbPub8cOWL++9ZORhdPbt5fe14ABu0L5gAPgr/6q5eAeNCi5fa+1XatBHxH1ki4BHia5RHJhRDwt6YJ0/S3AocCtknaQnGj9Hy1t2zFNse4oovkrEazr7NwJb7+dvbe9fn2yTSmDB+8K5YoK+OhHWw7uffft1KYaGa+j72y+jr5niYBXX4WaGnjxxeS5cHrDhq6uoWUlJWPWzYV08eWAw4cnQyvW9fbqOnozSC5Ne+WV0kH+4ovw7ru7ypaVJT278ePhqKPgwAM9ZtodDRmye5APHZocP8sXB701eu89eOmlpgHeEOq1tU3HXPv2hXHjYMIEOOGEJNQnTEieDzoI+vTpsmaYWREH/fvM5s2lg/zFF5PL7QpH8gYOTMJ70iQ444xkuiHMR41yL92sp3DQ59BbbzU/Xv7aa03LDhuWhPeMGU175RMmJB/lfbWDWc/noO+BIuD113cfJ28I9eJvAB54YBLcJ5/ctFc+fnwyTmtm+eag76Z27IC6uuaHWTZv3lW2V69kXHz8eDjrrKa98nHjkq+4m9n7l4O+C23bltxgqtRVLKtWNb3+fJ99ktAePx6OO25XkE+YkIS8L3Ezs+Y46DvYli1JaJcaZlm9uumXUAYMSAL8sMPgtNOajpmXl/uyNzNrGwd9O9iwofnx8jVFd/YZOjQJ7qOPTu43Ujhmvv/+PvlpZu3PQZ9BRPI18Oa+LPTmm03LH3BAEt4nnti0Vz5+fBL0ZmadyUGf2rkz6X2XCvKamqa3RJWS27tOmLD79eXjxiXXn5uZdRfvq6Cvr9918rO4d75qVdPbpPbpA2PHJuE9Y0bTK1kqKnreDw+Y2ftX7oJ+69YktEsNs6xenYR9g333TYL7kEPgk59seiXL6NE++bm3Fi3qfj9wYfZ+lJug37EjCerir/EPHpwEd2UlzJnTdMx85Eif/OwoixY1/cm61auTeXDYm3W23AR9WRnMng0f/GDTMfOhQx3mXeHKK5v+Likk81de6aA362y5CXqA66/v6hpYg5df3rPlZtZxfP9B6xBjxuzZcjPrOA566xALFux+j53+/ZPlZta5HPTWIebNg6qq5D48UvJcVeXxebOukKsxeute5s1zsJt1B+7Rm5nlnIPezCznHPRmZjnnoDczy7lMQS9plqTnJdVIuqLE+sGS7pe0XNLTks4rWFcraYWkZZKq27PyZmbWulavupFUBtwMnADUAUsk3RcRzxQUuxh4JiI+JWkE8LykRRHR8GN4x0VE0V3bzcysM2Tp0U8FaiJiVRrci4HTi8oEMEiSgIHAeqAeMzPrclmCfhTwSsF8Xbqs0E3AocAaYAXw9xHR8GuoAfxK0lJJ85t7EUnzJVVLql67dm3mBpiZWcuyBH2pez9G0fxJwDLgQGAycJOkD6TrpkfEFOBk4GJJx5R6kYioiojKiKgcMWJEttqbmVmrsgR9HTC6YL6cpOde6Dzg7kjUAC8BEwEiYk36/AZwD8lQkJmZdZIsQb8EOFjSWEn7AHOA+4rKvAx8AkDS/sCHgFWSBkgalC4fAJwIrGyvypuZWetaveomIuolXQI8DJQBCyPiaUkXpOtvAa4GfiJpBclQz1ci4k1J44B7knO09AZuj4iHOqgtZmZWgiKKh9u7XmVlZVRX+5J7M7OsJC2NiMpS6/zNWDOznHPQm5nlnIPezCznHPRmZjnnoDczyzkHvZlZzjnozcxyzkFvZpZzDnozs5xz0JuZ5ZyD3sws5xz0ZmY556A3M8s5B72ZWc456M3Mcs5Bb2aWcw56M7Occ9CbmeWcg97MLOcc9GZmOeegNzPLOQe9mVnOOejNzHLOQW9mlnOZgl7SLEnPS6qRdEWJ9YMl3S9puaSnJZ2XdVszM+tYrQa9pDLgZuBk4DBgrqTDiopdDDwTEZOAmcA/Ston47ZmZtaBsvTopwI1EbEqIrYBi4HTi8oEMEiSgIHAeqA+47ZmZtaBsgT9KOCVgvm6dFmhm4BDgTXACuDvI2Jnxm0BkDRfUrWk6rVr12asvpmZtSZL0KvEsiiaPwlYBhwITAZukvSBjNsmCyOqIqIyIipHjBiRoVpmZpZFlqCvA0YXzJeT9NwLnQfcHYka4CVgYsZtzcysA2UJ+iXAwZLGStoHmAPcV1TmZeATAJL2Bz4ErMq4rZmZdaDerRWIiHpJlwAPA2XAwoh4WtIF6fpbgKuBn0haQTJc85WIeBOg1LYd0xQzMytFESWHzLtUZWVlVFdXd3U1zMx6DElLI6Ky1Dp/M9bMLOcc9GZmOeegNzPLOQe9mVnOOejNzHLOQW9mlnMOejOznHPQm5nlnIPezCznHPRmZjnnoDczyzkHvZlZzjnozcxyzkFvZpZzDnozs5xz0JuZ5ZyD3sws5xz0ZmY556A3M8s5B72ZWc456M3Mcs5Bb2aWcw56M7Occ9CbmeVcpqCXNEvS85JqJF1RYv2XJS1LHysl7ZA0NF1XK2lFuq66vRtgZmYt691aAUllwM3ACUAdsETSfRHxTEOZiLgWuDYt/yng0ohYX7Cb4yLizXatuZmZZZKlRz8VqImIVRGxDVgMnN5C+bnAz9ujcmZmtveyBP0o4JWC+bp02W4k9QdmAXcVLA7gV5KWSprf1oqamVnbtDp0A6jEsmim7KeA3xcN20yPiDWS9gN+Lem5iHh8txdJ/gjMBxgzZkyGapmZWRZZevR1wOiC+XJgTTNl51A0bBMRa9LnN4B7SIaCdhMRVRFRGRGVI0aMyFAtMzPLIkvQLwEOljRW0j4kYX5fcSFJg4FjgX8rWDZA0qCGaeBEYGV7VNzMzLJpdegmIuolXQI8DJQBCyPiaUkXpOtvSYvOBn4VEZsLNt8fuEdSw2vdHhEPtWcDzMysZYpobri961RWVkZ1tS+5NzPLStLSiKgstc7fjDUzyzkHvZlZzjnozcxyzkFvZpZzDnozs5xz0JuZ5ZyD3sws5xz0ZmY556A3M8s5B72ZWc456M3Mcs5Bb2aWcw56M7Occ9CbmeWcg97MLOcc9GZmOeegNzPLOQe9mVnOOejNzHLOQW9mlnMOejOznHPQm5nlnIPezCznHPRmZjnnoDczy7lMQS9plqTnJdVIuqLE+i9LWpY+VkraIWlolm3NzKxjtRr0ksqAm4GTgcOAuZIOKywTEddGxOSImAx8FXgsItZn2dbMzDpWlh79VKAmIlZFxDZgMXB6C+XnAj9v47ZmZtbOsgT9KOCVgvm6dNluJPUHZgF3tWHb+ZKqJVWvXbs2Q7XMzCyLLEGvEsuimbKfAn4fEev3dNuIqIqIyoioHDFiRIZqmZlZFlmCvg4YXTBfDqxppuwcdg3b7Om2ZmbWAbIE/RLgYEljJe1DEub3FReSNBg4Fvi3Pd3WzMw6Tu/WCkREvaRLgIeBMmBhRDwt6YJ0/S1p0dnAryJic2vbtncjzMyseYpobri961RWVkZ1dXVXV8PMrMeQtDQiKkut8zdjzcxyzkFvZpZzDnozs5xz0JuZ5ZyD3sws5xz0ZmY556A3M8s5B72ZWc456M3Mcs5Bb2aWcw56M7Occ9CbmeWcg97MLOcc9GZmOeegNzPLOQe9mVnOtfoLU2b2/rJ9+3bq6urYunVrV1fFSujXrx/l5eX06dMn8zYOejNroq6ujkGDBlFRUYGkrq6OFYgI1q1bR11dHWPHjs28nYduzKyJrVu3MmzYMId8NySJYcOG7fGnLQe9me3GId99teXYOOjNzHLOQW9me2XRIqiogF69kudFi9q+r3Xr1jF58mQmT57MAQccwKhRoxrnt23b1uK21dXVfOELX2j1NaZNm9b2CvZQPhlrZm22aBHMnw9btiTzq1cn8wDz5u35/oYNG8ayZcsAuOqqqxg4cCCXXXZZ4/r6+np69y4dW5WVlVRWVrb6Gn/4wx/2vGI9nHv0ZtZmV165K+QbbNmSLG8v5557Ll/60pc47rjj+MpXvsKf/vQnpk2bxhFHHMG0adN4/vnnAfjd737HqaeeCiR/JM4//3xmzpzJuHHjuPHGGxv3N3DgwMbyM2fO5Mwzz2TixInMmzePiADggQceYOLEicyYMYMvfOELjfstVFtby8c//nGmTJnClClTmvwB+d73vseHP/xhJk2axBVXXAFATU0Nxx9/PJMmTWLKlCm8+OKL7fcmtSJTj17SLOCfgTLgXyPimhJlZgI3AH2ANyPi2HR5LbAJ2AHUR0Trf3LNrEd4+eU9W95WL7zwAo888ghlZWVs3LiRxx9/nN69e/PII4/wta99jbvuumu3bZ577jl++9vfsmnTJj70oQ9x4YUX7nbt+ZNPPsnTTz/NgQceyPTp0/n9739PZWUln//853n88ccZO3Ysc+fOLVmn/fbbj1//+tf069ePP//5z8ydO5fq6moefPBB7r33Xv74xz/Sv39/1q9fD8C8efO44oormD17Nlu3bmXnzp3t+ya1oNWgl1QG3AycANQBSyTdFxHPFJQZAnwfmBURL0var2g3x0XEm+1YbzPrBsaMSYZrSi1vT5/5zGcoKysDYMOGDZxzzjn8+c9/RhLbt28vuc0pp5xC37596du3L/vttx+vv/465eXlTcpMnTq1cdnkyZOpra1l4MCBjBs3rvE69blz51JVVbXb/rdv384ll1zCsmXLKCsr44UXXgDgkUce4bzzzqN///4ADB06lE2bNvGXv/yF2bNnA8mXnjpTlqGbqUBNRKyKiG3AYuD0ojKfBe6OiJcBIuKN9q2mmXVHCxZAmmeN+vdPlrenAQMGNE5/4xvf4LjjjmPlypXcf//9zV5T3rdv38bpsrIy6uvrM5VpGL5pzfXXX8/+++/P8uXLqa6ubjxZHBG7XQKZdZ8dJUvQjwJeKZivS5cVOgT4oKTfSVoq6XMF6wL4Vbp8fnMvImm+pGpJ1WvXrs1afzPrQvPmQVUVHHQQSMlzVVXbTsRmtWHDBkaNSiLoJz/5Sbvvf+LEiaxatYra2loA7rjjjmbrMXLkSHr16sVtt93Gjh07ADjxxBNZuHAhW9KTF+vXr+cDH/gA5eXl3HvvvQC89957jes7Q5agL3V1fvGfp97AkcApwEnANyQdkq6bHhFTgJOBiyUdU+pFIqIqIiojonLEiBHZam9mXW7ePKithZ07k+eODHmAyy+/nK9+9atMnz69MVzb07777sv3v/99Zs2axYwZM9h///0ZPHjwbuUuuugifvrTn3L00UfzwgsvNH7qmDVrFqeddhqVlZVMnjyZ6667DoDbbruNG2+8kY985CNMmzaN1157rd3r3hy19pFC0seAqyLipHT+qwAR8Q8FZa4A+kXEVen8j4CHIuLOon1dBbwTEde19JqVlZVRXV29x40xs7337LPPcuihh3Z1NbrUO++8w8CBA4kILr74Yg4++GAuvfTSrq5Wo1LHSNLS5i52ydKjXwIcLGmspH2AOcB9RWX+Dfi4pN6S+gNHAc9KGiBpUFqJAcCJwMo9apGZWSf74Q9/yOTJkzn88MPZsGEDn//857u6Snul1atuIqJe0iXAwySXVy6MiKclXZCuvyUinpX0EPAUsJPkEsyVksYB96QnJnoDt0fEQx3VGDOz9nDppZd2qx783sp0HX1EPAA8ULTslqL5a4Fri5atAibtZR3NzGwv+JuxZmY556A3M8s5B72ZWc456M2s25g5cyYPP/xwk2U33HADF110UYvbNFyO/clPfpK33357tzJXXXVV4/Xszbn33nt55pnGO7vwzW9+k0ceeWRPqt9tOejNrNuYO3cuixcvbrJs8eLFzd5YrNgDDzzAkCFD2vTaxUH/ne98h+OPP75N++pufD96M2vWF78I6e3h283kyXDDDaXXnXnmmXz961/nvffeo2/fvtTW1rJmzRpmzJjBhRdeyJIlS3j33Xc588wz+fa3v73b9hUVFVRXVzN8+HAWLFjArbfeyujRoxkxYgRHHnkkkFwjX1VVxbZt25gwYQK33XYby5Yt47777uOxxx7ju9/9LnfddRdXX301p556KmeeeSaPPvool112GfX19Xz0ox/lBz/4AX379qWiooJzzjmH+++/n+3bt3PnnXcyceLEJnWqra3l7LPPZvPmzQDcdNNNjT9+8r3vfY/bbruNXr16cfLJJ3PNNddQU1PDBRdcwNq1aykrK+POO+9k/Pjxe/Weu0dvZt3GsGHDmDp1Kg89lHzdZvHixZx11llIYsGCBVRXV/PUU0/x2GOP8dRTTzW7n6VLl7J48WKefPJJ7r77bpYsWdK47owzzmDJkiUsX76cQw89lB/96EdMmzaN0047jWuvvZZly5Y1CdatW7dy7rnncscdd7BixQrq6+v5wQ9+0Lh++PDhPPHEE1x44YUlh4cabmf8xBNPcMcddzT+Clbh7YyXL1/O5ZdfDiS3M7744otZvnw5f/jDHxg5cuTevam4R29mLWiu592RGoZvTj/9dBYvXszChQsB+MUvfkFVVRX19fW8+uqrPPPMM3zkIx8puY//+I//YPbs2Y23Cj7ttNMa161cuZKvf/3rvP3227zzzjucdNJJLdbn+eefZ+zYsRxySHL7rnPOOYebb76ZL37xi0DyhwPgyCOP5O67795t++5wO+Pc9Ojb83crzazrfPrTn+bRRx/liSee4N1332XKlCm89NJLXHfddTz66KM89dRTnHLKKc3enrhB8a2CG5x77rncdNNNrFixgm9961ut7qe1+4E13Oq4uVshd4fbGeci6Bt+t3L1aojY9buVDnuznmfgwIHMnDmT888/v/Ek7MaNGxkwYACDBw/m9ddf58EHH2xxH8cccwz33HMP7777Lps2beL+++9vXLdp0yZGjhzJ9u3bWVQQEoMGDWLTpk277WvixInU1tZSU1MDJHehPPbYYzO3pzvczjgXQd8Zv1tpZp1n7ty5LF++nDlz5gAwadIkjjjiCA4//HDOP/98pk+f3uL2U6ZM4ayzzmLy5Mn8zd/8DR//+Mcb11199dUcddRRnHDCCU1OnM6ZM4drr72WI444osnvufbr148f//jHfOYzn+HDH/4wvXr14oILLsjclu5wO+NWb1PcFfb0NsW9eiU9+WJSco9sM8vOtynu/jriNsXdXnO/T9nev1tpZtYT5SLoO+t3K83MeqJcBH1X/G6lWZ51xyFdS7Tl2OTmOvp58xzsZu2hX79+rFu3jmHDhjV7iaJ1jYhg3bp1e3x9fW6C3szaR3l5OXV1daxdu7arq2Il9OvXj/Ly8j3axkFvZk306dOHsWPHdnU1rB3lYozezMya56A3M8s5B72ZWc51y2/GSloLrG7j5sOBN9uxOl0pL23JSzvAbemO8tIO2Lu2HBQRI0qt6JZBvzckVTf3NeCeJi9tyUs7wG3pjvLSDui4tnjoxsws5xz0ZmY5l8egr+rqCrSjvLQlL+0At6U7yks7oMcGsbsAAAMYSURBVIPakrsxejMzayqPPXozMyvgoDczy7keGfSSFkp6Q9LKZtZL0o2SaiQ9JWlKZ9cxqwxtmSlpg6Rl6eObnV3HLCSNlvRbSc9KelrS35co0yOOS8a2dPvjIqmfpD9JWp6249slyvSUY5KlLd3+mBSSVCbpSUm/LLGufY9LRPS4B3AMMAVY2cz6TwIPAgKOBv7Y1XXei7bMBH7Z1fXM0I6RwJR0ehDwAnBYTzwuGdvS7Y9L+j4PTKf7AH8Eju6hxyRLW7r9MSmq75eA20vVub2PS4/s0UfE48D6FoqcDtwaif8Ghkga2Tm12zMZ2tIjRMSrEfFEOr0JeBYYVVSsRxyXjG3p9tL3+Z10tk/6KL76oqcckyxt6TEklQOnAP/aTJF2PS49MugzGAW8UjBfRw/8j1rgY+lH1gclHd7VlWmNpArgCJJeV6Eed1xaaAv0gOOSDg8sA94Afh0RPfaYZGgL9IBjkroBuBzY2cz6dj0ueQ36Uj+L01P/+j9Bcg+LScD/Be7t4vq0SNJA4C7gixGxsXh1iU267XFppS094rhExI6ImAyUA1Ml/VVRkR5zTDK0pUccE0mnAm9ExNKWipVY1ubjktegrwNGF8yXA2u6qC57JSI2NnxkjYgHgD6ShndxtUqS1IckGBdFxN0livSY49JaW3rScQGIiLeB3wGzilb1mGPSoLm29KBjMh04TVItsBj4a0k/KyrTrsclr0F/H/C59Mz10cCGiHi1qyvVFpIOUPrDnZKmkhyzdV1bq92ldfwR8GxE/FMzxXrEccnSlp5wXCSNkDQknd4XOB54rqhYTzkmrbalJxwTgIj4akSUR0QFMAf4TUT8bVGxdj0uPfKnBCX9nOQM+3BJdcC3SE7OEBG3AA+QnLWuAbYA53VNTVuXoS1nAhdKqgfeBeZEelq+m5kOnA2sSMdRAb4GjIEed1yytKUnHJeRwE8llZGE3i8i4peSLoAed0yytKUnHJNmdeRx8S0QzMxyLq9DN2ZmlnLQm5nlnIPezCznHPRmZjnnoDczyzkHvZlZzjnozcxy7v8DOA4foBNJrzUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run x Times the folds\n",
    "if not run_final_train_only:\n",
    "    for run_num in range(1,fold_runs+1):\n",
    "        # k-fold\n",
    "        for train_ind, val_ind in skfold.split(x_train,y_train):\n",
    "\n",
    "            # Create model\n",
    "            model = create_model()\n",
    "\n",
    "            # Load GloVe embedding\n",
    "            model.layers[0].set_weights([word_embedding_matrix])\n",
    "            model.layers[0].trainable = False\n",
    "\n",
    "            # Train and Evaluate\n",
    "            model.compile(optimizer=optimizer,\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['acc'])\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} ind run {run_num} ...')\n",
    "\n",
    "            history = model.fit(x_train[train_ind], y_train[train_ind],\n",
    "                                epochs=epochs,\n",
    "                                batch_size=64,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_train[val_ind], y_train[val_ind]))\n",
    "\n",
    "            # metrics\n",
    "            scores = model.evaluate(x_train[val_ind], y_train[val_ind], batch_size=32)\n",
    "            #print(f'Score for fold {fold_no}: {model.metrics_name[0]} of {scores[0]}; {model.metrics_name[1]} of {scores[1]*100}%')\n",
    "            print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
    "            acc_per_fold.append(scores[1]*100)\n",
    "            loss_per_fold.append(scores[0])\n",
    "\n",
    "            # Evaluation metrics precison recall f1\n",
    "            y_pred = model.predict(x_train[val_ind], batch_size=64, verbose=1)\n",
    "            y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(y_train[val_ind], y_pred_bool)\n",
    "            mean_precision = np.mean(precision)\n",
    "            mean_recall = np.mean(recall)\n",
    "            mean_f1 = np.mean(f1)\n",
    "            precision_per_fold.append(mean_precision)\n",
    "            recall_per_fold.append(mean_recall)\n",
    "            f1_per_fold.append(mean_f1)\n",
    "\n",
    "            fold_no += 1\n",
    "\n",
    "        # == Provide average scores ==\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Score per fold')\n",
    "        for i in range(0, len(acc_per_fold)):\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Average scores for all folds:')\n",
    "        avg_acc_per_run.append(np.mean(acc_per_fold))\n",
    "        avg_loss_per_run.append(np.mean(loss_per_fold))\n",
    "        avg_precision_per_run.append(np.mean(precision_per_fold))\n",
    "        avg_recall_per_run.append(np.mean(recall_per_fold))\n",
    "        avg_f1_per_run.append(np.mean(f1_per_fold))\n",
    "\n",
    "        print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "        print(f'> Precision: {np.mean(precision_per_fold)}')\n",
    "        print(f'> Recall: {np.mean(recall_per_fold)}')\n",
    "        print(f'> F1: {np.mean(f1_per_fold)}')\n",
    "        print('------------------------------------------------------------------------')\n",
    "\n",
    "        # reset fold vars\n",
    "        acc_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        precision_per_fold = []\n",
    "        recall_per_fold = []\n",
    "        f1_per_fold = []\n",
    "        fold_no = 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score for k-fold runs')\n",
    "    for i in range(0, len(avg_acc_per_run)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Run {i+1} Fold averages - Loss: {avg_loss_per_run[i]} - Accuracy: {avg_acc_per_run[i]}% ')\n",
    "        print(f'> Run {i+1} Fold averages - Precision: {avg_precision_per_run[i]} - Recall: {avg_recall_per_run[i]} F1: {avg_f1_per_run[i]}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Overall average scores for all {fold_runs} runs:')\n",
    "    print(f'> Accuracy: {np.mean(avg_acc_per_run)} (+- {np.std(avg_acc_per_run)})')\n",
    "    print(f'> Loss: {np.mean(avg_loss_per_run)}')\n",
    "    print(f'> Precision: {np.mean(avg_precision_per_run)}')\n",
    "    print(f'> Recall: {np.mean(avg_recall_per_run)}')\n",
    "    print(f'> F1: {np.mean(avg_f1_per_run)}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "# create final model #Todo sync with fold rund\n",
    "if create_final_model:\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "\n",
    "    # Load GloVe embedding\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "\n",
    "    # Train and Evaluate\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Training for final model ...')\n",
    "\n",
    "    history = model.fit(x_train_copy, y_train_copy,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    # Save Model\n",
    "    if use_mg_train_corpora == MULTIGENRE:\n",
    "        model.save('models/model_emotion_detection_multigenre.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre.pkl\", \"wb\"))\n",
    "    elif use_mg_train_corpora == TWITTER:\n",
    "        model.save('models/model_emotion_detection_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_twitter.pkl\", \"wb\"))\n",
    "    else:\n",
    "        model.save('models/model_emotion_detection_multigenre_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre_twitter.pkl\", \"wb\"))\n",
    "\n",
    "    # Test final model\n",
    "    print(\"Evaluate final model on test data\")\n",
    "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    # For Model evaluation metrics run evalModel\n",
    "\n",
    "    # Plot performance\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
