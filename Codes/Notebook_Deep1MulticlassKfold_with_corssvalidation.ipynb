{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, LSTM, GRU, AveragePooling1D, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "#import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from corpora_utils import CorporaHelper,CorporaDomains, CorporaProperties\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Activate GPU\n",
    "#WARNING GPU TAKES 5 TIMES LONGER THAN CPU! With Consul Project 1\n",
    "#Check for GPU\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n",
    "# GPU CONFIG\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\"\"\"\n",
    "\n",
    "#Deactivate GPU\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIGENRE = 'muligenre'\n",
    "TWITTER = 'twitter'\n",
    "MG_AND_TWITTER = 'mg_and_twitter'\n",
    "\n",
    "# set wich corpora to use Multigenre or twitter\n",
    "use_mg_train_corpora = MG_AND_TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_labels = []\n",
    "train_texts = []\n",
    "test_labels = []\n",
    "test_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpora(filepath, sep=';'):\n",
    "    print('Load: ', filepath)\n",
    "    corpora_helper = CorporaHelper(filepath, separator=sep)\n",
    "    count_joy = 0\n",
    "    count_sadness = 0\n",
    "    count_anger = 0\n",
    "    count_fear = 0\n",
    "    labels = []\n",
    "    texts = []\n",
    "    # preprocessing corpora\n",
    "    corpora_helper.translate_urls()\n",
    "    corpora_helper.translate_emoticons()\n",
    "    corpora_helper.translate_emojis()\n",
    "    corpora_helper.translate_email()\n",
    "    #corpora_helper.translate_mention()\n",
    "    corpora_helper.translate_html_tags()\n",
    "    #corpora_helper.translate_camel_case()\n",
    "    corpora_helper.translate_underscore()\n",
    "\n",
    "    corpora_helper.translate_string('-LRB-','(')\n",
    "    corpora_helper.translate_string('-RRB-',')')\n",
    "    corpora_helper.translate_string('`',\"'\") # ` to '\n",
    "    corpora_helper.translate_string(\"''\",'\"') # double '' to \"\n",
    "    corpora_helper.translate_contractions()\n",
    "    corpora_helper.translate_string(\"'\",\"\") # remove '\n",
    "    corpora_helper.translate_string(\"\\\\n\",\" \") # replace new lines with space\n",
    "\n",
    "    #corpora_helper.spell_correction()\n",
    "    corpora_helper.add_space_at_special_chars()\n",
    "    corpora_helper.add_space_at_special_chars(regexlist = r\"([#])\")\n",
    "    #corpora_helper.translate_to_lower()\n",
    "\n",
    "    # 0 anger\n",
    "    # 1 fear\n",
    "    # 2 joy\n",
    "    # 3 sadness\n",
    "    for index, corpus in corpora_helper.get_data().iterrows():\n",
    "        if corpus[CorporaProperties.EMOTION.value] == 'anger':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(0)\n",
    "            count_anger += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'fear':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(1)\n",
    "            count_fear += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'joy':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(2)\n",
    "            count_joy += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'sadness':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(3)\n",
    "            count_sadness += 1\n",
    "    print('number of anger labels: ',count_anger)\n",
    "    print('number of fear labels: ', count_fear)\n",
    "    print('number of joy labels: ', count_joy)\n",
    "    print('number of sadness labels: ', count_sadness)\n",
    "    print('----------------------------------------------------------------------')\n",
    "    return texts, labels\n",
    "    #max_data = count_anger + count_fear + count_joy + count_sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use TWITTER and MULTIGENRE train corpora\n",
      "Load:  corpora/twitter_2000_mg_450_train.csv\n",
      "number of anger labels:  2205\n",
      "number of fear labels:  2205\n",
      "number of joy labels:  2205\n",
      "number of sadness labels:  2205\n",
      "----------------------------------------------------------------------\n",
      "Load:  corpora/twitter_2000_mg_450_test.csv\n",
      "number of anger labels:  245\n",
      "number of fear labels:  245\n",
      "number of joy labels:  245\n",
      "number of sadness labels:  245\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_file = \"\"\n",
    "test_file = \"\"\n",
    "sep = ';'\n",
    "word_embeddings_path = ''\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    train_file = \"corpora/multigenre_450_train.csv\"\n",
    "    test_file = \"corpora/multigenre_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = ';'\n",
    "    print(\"Use MULTIGENRE train corpora\")\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    train_file = \"corpora/twitter_2000_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER train corpora\")\n",
    "else:\n",
    "    train_file = \"corpora/twitter_2000_mg_450_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_mg_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER and MULTIGENRE train corpora\")\n",
    "    \n",
    "train_texts, train_labels = load_corpora(train_file, sep=sep)\n",
    "test_texts, test_labels = load_corpora(test_file, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared custom ensemble embedding\n",
    "with open(word_embeddings_path, 'rb') as word_embeddings_file:\n",
    "    embedding_info = pickle.load(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding helper functions\n",
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "    \n",
    "def get_unigram_embedding(word, word_embedding_dict, bin_string):\n",
    "    \n",
    "    if word in word_embedding_dict:\n",
    "        word_feature_embedding_dict = word_embedding_dict[word]\n",
    "        final_embedding = np.array([])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for i in range(16):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen:  100\n"
     ]
    }
   ],
   "source": [
    "pre_padding = 0\n",
    "embeddings_index = embedding_info[0]\n",
    "MAX_SEQUENCE_LENGTH = embedding_info[1]\n",
    "maxlen = MAX_SEQUENCE_LENGTH\n",
    "print(\"maxlen: \",maxlen)\n",
    "#MAX_NB_WORDS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Unigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting relevant embeddings for multigenre\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    # Multigenre\n",
    "    unigram_feature_string = \"1001111111111101\"\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    # Twitter\n",
    "    unigram_feature_string = \"0110001111111101\"\n",
    "    unigram_feature_string = \"1111111111111111\"\n",
    "else:\n",
    "    # Twitter and Multigenre\n",
    "    unigram_feature_string = \"1110010000000000\"\n",
    "# 1 Google news pretrained vectors : GoogleNews-vectors-negative300.bin.gz  \n",
    "# 2 Twitter pretrained vectors: word2vec_twitter_model.bin\n",
    "# 3 glove.twitter.27B.200d.txt\n",
    "# 4 glove.6B.300d.txt\n",
    "# 5 glove.42B.300d.txt\n",
    "# 6 glove.840B.300d.txt\n",
    "# 7 NRC Emotion Intensity Lexicon\n",
    "# 8 senti word net\n",
    "#9  NRC Sentiment lexicon: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#10 lexicons/Emoticon-unigrams.txt\n",
    "#11 lexicons/Emoticon-AFFLEX-NEGLEX-unigrams.txt\n",
    "#12 NRC Hashtag Lexica: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#13 HS-unigrams.txtNRC-Hashtag-Emotion-Lexicon-v0.2.txt\n",
    "#14 HS-AFFLEX-NEGLEX-unigrams.txt\n",
    "#15 Emoji Polarities\n",
    "#16 Depeche mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep learning with multigenre and twitter corpus and 4 emotions\n",
    "\"\"\"\n",
    "# K-Fold variables\n",
    "num_folds = 3 # 10\n",
    "fold_runs = 1 # 3\n",
    "fold_no = 1\n",
    "# train\n",
    "epochs = 5\n",
    "max_words = 10000\n",
    "# max. different words:\n",
    "# Multigerne: 5140  => 10000 or 3000 or 1000 ?\n",
    "# Twitter: 17580 => 20000 or 10000 ?\n",
    "# MG and Twitter: 20073 => evtl. 20000?\n",
    "#optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer = Adam(learning_rate=0.001) # default 0.001\n",
    "skfold = StratifiedKFold(n_splits = num_folds, random_state = 7, shuffle = True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_per_fold = []\n",
    "avg_acc_per_run = []\n",
    "avg_loss_per_run = []\n",
    "avg_precision_per_run = []\n",
    "avg_recall_per_run = []\n",
    "avg_f1_per_run = []\n",
    "create_final_model = True\n",
    "# run only final model without kfold\n",
    "run_final_train_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1200\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = len(get_unigram_embedding(\"glad\", embedding_info[0], unigram_feature_string))\n",
    "print(\"Embedding dimension:\",EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, filters = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train an test data set\n",
    "def create_data(texts, labels, maxlen):\n",
    "    ## Create one hot encoding\n",
    "    #max_words = 10000\n",
    "    #maxlen = 100 # max. number of words in sequences\n",
    "    #tokenizer = Tokenizer(num_words=max_words, filters = '')\n",
    "    #tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    #word_i = tokenizer.word_index\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    labels_arr = np.asarray(labels)\n",
    "    print('Shape of data:', data.shape)\n",
    "    print('Shape of labels:', labels_arr.shape)\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "    # mix the data\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels_arr = labels_arr[indices]\n",
    "\n",
    "    # split in train and validate\n",
    "    x_data = data\n",
    "    y_data = labels_arr\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer\n",
    "all_texts = train_texts.copy()\n",
    "all_texts.append(test_texts.copy())\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (8820, 100)\n",
      "Shape of labels: (8820,)\n",
      "-------------------------------------------\n",
      "Shape of data: (980, 100)\n",
      "Shape of labels: (980,)\n",
      "-------------------------------------------\n",
      "19020 unique Tokens found.\n"
     ]
    }
   ],
   "source": [
    "# Train an word index for embedding enrichment\n",
    "x_train, y_train = create_data(train_texts, train_labels, maxlen)\n",
    "x_test, y_test = create_data(test_texts, test_labels, maxlen)\n",
    "word_index = tokenizer.word_index\n",
    "x_train_copy = x_train.copy()\n",
    "y_train_copy = y_train.copy()\n",
    "print ('%s unique Tokens found.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Matrix\n",
    "word_embedding_matrix = list()\n",
    "word_embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "#word_embedding_matrix.append(np.zeros(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items(): # sorted(word_indices, key=word_indices.get):\n",
    "    embedding_features = get_unigram_embedding(word, embedding_info[0], unigram_feature_string)\n",
    "    if i < max_words:\n",
    "        if embedding_features is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            word_embedding_matrix[i] = embedding_features\n",
    "\n",
    "word_embedding_matrix = np.asarray(word_embedding_matrix, dtype='f')\n",
    "word_embedding_matrix = scale(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM 1200\n",
      "input_length 100\n"
     ]
    }
   ],
   "source": [
    "#print('word_indices_len',word_indices_len)\n",
    "print('EMBEDDING_DIM',EMBEDDING_DIM)\n",
    "print('input_length', MAX_SEQUENCE_LENGTH + pre_padding)\n",
    "embedding = Embedding(max_words, EMBEDDING_DIM, input_length=maxlen, trainable=False)\n",
    "#embedding = Embedding(word_indices_len + 1, EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH + pre_padding, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef create_model():\\n\\n    # Create model\\n\\n    model = Sequential()\\n    model.add(embedding)\\n    model.add(Conv1D(32,5, activation='relu'))\\n    #model.add(layers.GRU(32,dropout=0.4, recurrent_dropout=0.4)) #acc = 25 \\n    #model.add(layers.LSTM(32,dropout=0.6, recurrent_dropout=0.6)) #acc = 83.7\\n    #model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.6, recurrent_dropout=0.6))) #acc = 83.2\\n    #model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.5, recurrent_dropout=0.5))) #acc = 83.4\\n    #model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.4, recurrent_dropout=0.4))) #acc = 84.5\\n    #model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.3, recurrent_dropout=0.3))) #acc = 85.2\\n    #model.add(Dense(8, activation='relu'))\\n    #model.add(Dense(32, activation='relu'))\\n    model.add(Dense(4, activation='softmax'))\\n\\n    return model\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def create_model():\n",
    "\n",
    "    # Create model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(32,5, activation='relu'))\n",
    "    #model.add(layers.GRU(32,dropout=0.4, recurrent_dropout=0.4)) #acc = 25 \n",
    "    #model.add(layers.LSTM(32,dropout=0.6, recurrent_dropout=0.6)) #acc = 83.7\n",
    "    #model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.6, recurrent_dropout=0.6))) #acc = 83.2\n",
    "    #model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.5, recurrent_dropout=0.5))) #acc = 83.4\n",
    "    #model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.4, recurrent_dropout=0.4))) #acc = 84.5\n",
    "    #model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.3, recurrent_dropout=0.3))) #acc = 85.2\n",
    "    #model.add(Dense(8, activation='relu'))\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"Adam\", learningrate = 0.001)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-Model (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "        \n",
    "    model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.4, return_sequences=True))\n",
    "    \n",
    "    model.add(Dense(16, activation='relu'))\n",
    "\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"Adam\", learningrate = 0.001)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM-Model (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "        \n",
    "    model.add(Bidirectional(LSTM(32, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))\n",
    "    \n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"Adam\", learningrate = 0.001)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    \n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(GRU(64, dropout=0.25, recurrent_dropout=0.25, return_sequences=True))\n",
    "    model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"Adam\", learningrate = 0.001)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 1 ...\n",
      "Train on 5880 samples, validate on 2940 samples\n",
      "Epoch 1/5\n",
      "5880/5880 [==============================] - 18s 3ms/step - loss: 1.1588 - acc: 0.5009 - val_loss: 0.7093 - val_acc: 0.7289\n",
      "Epoch 2/5\n",
      "5880/5880 [==============================] - 17s 3ms/step - loss: 0.6288 - acc: 0.7665 - val_loss: 0.4913 - val_acc: 0.8126\n",
      "Epoch 3/5\n",
      "5880/5880 [==============================] - 17s 3ms/step - loss: 0.4695 - acc: 0.8299 - val_loss: 0.4520 - val_acc: 0.8241\n",
      "Epoch 4/5\n",
      "5880/5880 [==============================] - 17s 3ms/step - loss: 0.3870 - acc: 0.8583 - val_loss: 0.4098 - val_acc: 0.8412\n",
      "Epoch 5/5\n",
      "5880/5880 [==============================] - 17s 3ms/step - loss: 0.3394 - acc: 0.8760 - val_loss: 0.3994 - val_acc: 0.8442\n",
      "2940/2940 [==============================] - 4s 1ms/step\n",
      "Score for fold 1: loss of 0.39943946352621323; accuracy of 84.42176580429077%\n",
      "2940/2940 [==============================] - 4s 1ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 1 ...\n",
      "Train on 5880 samples, validate on 2940 samples\n",
      "Epoch 1/5\n",
      "5880/5880 [==============================] - 19s 3ms/step - loss: 0.8334 - acc: 0.6614 - val_loss: 0.4909 - val_acc: 0.8238\n",
      "Epoch 2/5\n",
      "5880/5880 [==============================] - 18s 3ms/step - loss: 0.4525 - acc: 0.8333 - val_loss: 0.4273 - val_acc: 0.8391\n",
      "Epoch 3/5\n",
      "5880/5880 [==============================] - 19s 3ms/step - loss: 0.3761 - acc: 0.8636 - val_loss: 0.4139 - val_acc: 0.8490\n",
      "Epoch 4/5\n",
      "5880/5880 [==============================] - 19s 3ms/step - loss: 0.3208 - acc: 0.8813 - val_loss: 0.4115 - val_acc: 0.8544\n",
      "Epoch 5/5\n",
      "5880/5880 [==============================] - 18s 3ms/step - loss: 0.2854 - acc: 0.8968 - val_loss: 0.4161 - val_acc: 0.8571\n",
      "2940/2940 [==============================] - 4s 1ms/step\n",
      "Score for fold 2: loss of 0.4160531624847529; accuracy of 85.71428656578064%\n",
      "2940/2940 [==============================] - 5s 2ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 1 ...\n",
      "Train on 5880 samples, validate on 2940 samples\n",
      "Epoch 1/5\n",
      "5880/5880 [==============================] - 21s 4ms/step - loss: 0.8259 - acc: 0.6592 - val_loss: 0.4864 - val_acc: 0.8061\n",
      "Epoch 2/5\n",
      "5880/5880 [==============================] - 19s 3ms/step - loss: 0.4425 - acc: 0.8337 - val_loss: 0.4326 - val_acc: 0.8361\n",
      "Epoch 3/5\n",
      "5880/5880 [==============================] - 19s 3ms/step - loss: 0.3544 - acc: 0.8680 - val_loss: 0.4265 - val_acc: 0.8388\n",
      "Epoch 4/5\n",
      "5880/5880 [==============================] - 20s 3ms/step - loss: 0.3066 - acc: 0.8854 - val_loss: 0.4187 - val_acc: 0.8412\n",
      "Epoch 5/5\n",
      "5880/5880 [==============================] - 20s 3ms/step - loss: 0.2756 - acc: 0.9009 - val_loss: 0.4259 - val_acc: 0.8422\n",
      "2940/2940 [==============================] - 4s 2ms/step\n",
      "Score for fold 3: loss of 0.4258510032478644; accuracy of 84.21768546104431%\n",
      "2940/2940 [==============================] - 5s 2ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.39943946352621323 - Accuracy: 84.42176580429077%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4160531624847529 - Accuracy: 85.71428656578064%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4258510032478644 - Accuracy: 84.21768546104431%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 84.78457927703857 (+- 0.6626607622205044)\n",
      "> Loss: 0.41378120975294347\n",
      "> Precision: 0.8480383048897594\n",
      "> Recall: 0.8478458049886622\n",
      "> F1: 0.8475698699767701\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Score for k-fold runs\n",
      "------------------------------------------------------------------------\n",
      "> Run 1 Fold averages - Loss: 0.41378120975294347 - Accuracy: 84.78457927703857% \n",
      "> Run 1 Fold averages - Precision: 0.8480383048897594 - Recall: 0.8478458049886622 F1: 0.8475698699767701\n",
      "------------------------------------------------------------------------\n",
      "Overall average scores for all 1 runs:\n",
      "> Accuracy: 84.78457927703857 (+- 0.0)\n",
      "> Loss: 0.41378120975294347\n",
      "> Precision: 0.8480383048897594\n",
      "> Recall: 0.8478458049886622\n",
      "> F1: 0.8475698699767701\n",
      "------------------------------------------------------------------------\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 1200)         12000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 100, 64)           315648    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100, 16)           1040      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 12,316,756\n",
      "Trainable params: 316,756\n",
      "Non-trainable params: 12,000,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for final model ...\n",
      "Train on 8820 samples, validate on 980 samples\n",
      "Epoch 1/5\n",
      "8820/8820 [==============================] - 25s 3ms/step - loss: 1.0062 - acc: 0.5720 - val_loss: 0.5490 - val_acc: 0.8010\n",
      "Epoch 2/5\n",
      "8820/8820 [==============================] - 23s 3ms/step - loss: 0.4949 - acc: 0.8177 - val_loss: 0.4422 - val_acc: 0.8398\n",
      "Epoch 3/5\n",
      "8820/8820 [==============================] - 22s 3ms/step - loss: 0.4079 - acc: 0.8476 - val_loss: 0.4110 - val_acc: 0.8520\n",
      "Epoch 4/5\n",
      "8820/8820 [==============================] - 22s 3ms/step - loss: 0.3453 - acc: 0.8704 - val_loss: 0.3946 - val_acc: 0.8612\n",
      "Epoch 5/5\n",
      "8820/8820 [==============================] - 23s 3ms/step - loss: 0.3116 - acc: 0.8799 - val_loss: 0.4036 - val_acc: 0.8633\n",
      "Evaluate final model on test data\n",
      "980/980 [==============================] - 1s 1ms/step\n",
      "test loss, test acc: [0.40356521849729576, 0.863265335559845]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run x Times the folds\n",
    "if not run_final_train_only:\n",
    "    for run_num in range(1,fold_runs+1):\n",
    "        # k-fold\n",
    "        for train_ind, val_ind in skfold.split(x_train,y_train):\n",
    "\n",
    "            # Create model\n",
    "            model = create_model()\n",
    "\n",
    "            # Load GloVe embedding\n",
    "            model.layers[0].set_weights([word_embedding_matrix])\n",
    "            model.layers[0].trainable = False\n",
    "\n",
    "            # Train and Evaluate\n",
    "            model.compile(optimizer=optimizer,\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['acc'])\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} ind run {run_num} ...')\n",
    "\n",
    "            history = model.fit(x_train[train_ind], y_train[train_ind],\n",
    "                                epochs=epochs,\n",
    "                                batch_size=64,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_train[val_ind], y_train[val_ind]))\n",
    "\n",
    "            # metrics\n",
    "            scores = model.evaluate(x_train[val_ind], y_train[val_ind], batch_size=32)\n",
    "            #print(f'Score for fold {fold_no}: {model.metrics_name[0]} of {scores[0]}; {model.metrics_name[1]} of {scores[1]*100}%')\n",
    "            print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
    "            acc_per_fold.append(scores[1]*100)\n",
    "            loss_per_fold.append(scores[0])\n",
    "\n",
    "            # Evaluation metrics precison recall f1\n",
    "            y_pred = model.predict(x_train[val_ind], batch_size=64, verbose=1)\n",
    "            y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(y_train[val_ind], y_pred_bool)\n",
    "            mean_precision = np.mean(precision)\n",
    "            mean_recall = np.mean(recall)\n",
    "            mean_f1 = np.mean(f1)\n",
    "            precision_per_fold.append(mean_precision)\n",
    "            recall_per_fold.append(mean_recall)\n",
    "            f1_per_fold.append(mean_f1)\n",
    "\n",
    "            fold_no += 1\n",
    "\n",
    "        # == Provide average scores ==\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Score per fold')\n",
    "        for i in range(0, len(acc_per_fold)):\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Average scores for all folds:')\n",
    "        avg_acc_per_run.append(np.mean(acc_per_fold))\n",
    "        avg_loss_per_run.append(np.mean(loss_per_fold))\n",
    "        avg_precision_per_run.append(np.mean(precision_per_fold))\n",
    "        avg_recall_per_run.append(np.mean(recall_per_fold))\n",
    "        avg_f1_per_run.append(np.mean(f1_per_fold))\n",
    "\n",
    "        print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "        print(f'> Precision: {np.mean(precision_per_fold)}')\n",
    "        print(f'> Recall: {np.mean(recall_per_fold)}')\n",
    "        print(f'> F1: {np.mean(f1_per_fold)}')\n",
    "        print('------------------------------------------------------------------------')\n",
    "\n",
    "        # reset fold vars\n",
    "        acc_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        precision_per_fold = []\n",
    "        recall_per_fold = []\n",
    "        f1_per_fold = []\n",
    "        fold_no = 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score for k-fold runs')\n",
    "    for i in range(0, len(avg_acc_per_run)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Run {i+1} Fold averages - Loss: {avg_loss_per_run[i]} - Accuracy: {avg_acc_per_run[i]}% ')\n",
    "        print(f'> Run {i+1} Fold averages - Precision: {avg_precision_per_run[i]} - Recall: {avg_recall_per_run[i]} F1: {avg_f1_per_run[i]}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Overall average scores for all {fold_runs} runs:')\n",
    "    print(f'> Accuracy: {np.mean(avg_acc_per_run)} (+- {np.std(avg_acc_per_run)})')\n",
    "    print(f'> Loss: {np.mean(avg_loss_per_run)}')\n",
    "    print(f'> Precision: {np.mean(avg_precision_per_run)}')\n",
    "    print(f'> Recall: {np.mean(avg_recall_per_run)}')\n",
    "    print(f'> F1: {np.mean(avg_f1_per_run)}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "# create final model #Todo sync with fold rund\n",
    "if create_final_model:\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "\n",
    "    # Load GloVe embedding\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "\n",
    "    # Train and Evaluate\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Training for final model ...')\n",
    "\n",
    "    history = model.fit(x_train_copy, y_train_copy,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    # Save Model\n",
    "    if use_mg_train_corpora == MULTIGENRE:\n",
    "        model.save('models/model_emotion_detection_multigenre.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre.pkl\", \"wb\"))\n",
    "    elif use_mg_train_corpora == TWITTER:\n",
    "        model.save('models/model_emotion_detection_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_twitter.pkl\", \"wb\"))\n",
    "    else:\n",
    "        model.save('models/model_emotion_detection_multigenre_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre_twitter.pkl\", \"wb\"))\n",
    "\n",
    "    # Test final model\n",
    "    print(\"Evaluate final model on test data\")\n",
    "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    # For Model evaluation metrics run evalModel\n",
    "\n",
    "    # Plot performance\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nResults of Final model generated from 3 k-Folds and 2 runs with various unigram features:\\n\\nDescription:                                          F1:          Unigram feature:     test loss, test acc on final model:\\nwithout NRC and DepecheMood                           84.7 | 83.9  \"1110010000000000\"   0.412, 0.849 | 0.388, 0.861\\nwith depechemood                                      84.6 |       \"1110010000000001\"   0.405, 0.861 |\\nwith NRC                                              86.0         \"1110010010000000\"\\nwith NRC Emotion Intensity                            84.2 | 84.5  \"1110011000000000\"   0.380, 0.864. | 0.405, 0.851\\nwith NRC Emotion Intensity and NRC and Depechemood    85.8   \"1110011010000001\" \\nall unigram features acitvated                        85.0   \"1111111111111111\"\\n'"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analysis of Lexicon features\n",
    "Results of Final model generated with BiLSTM-NN from 3 k-Folds and 2 runs with various unigram features:\n",
    "\n",
    "                                                                   Run1 | Run2            Run1 | Run2\n",
    "Unigram feature:  Description:                                     F1-overall:    loss,  acc (on final model):   Epochs:\n",
    "1110010000000000  without NRC and DepecheMood                      84.6 | 83.7    0.398, 0.865 | 0.402, 0.856     5 \n",
    "1110010000000001  with DepecheMood                                 84.0 | 84.0    0.405, 0.861 | 0.398, 0.852     5\n",
    "1110010010000000  with NRC                                         84.7 | 84.2    0.389, 0.860 | 0.375, 0.860     5\n",
    "1110011000000000  with NRC Emotion Intensity                       84.6 | 84.7    0.404, 0.852 | 0.409, 0.854     5   \n",
    "1110011010000001  with NRC Emotion Intensity, NRC and DepecheMood  84.3 | 84.3    0.404, 0.859 | 0.408, 0.862     5 \n",
    "1111111111111111  all unigram features acitvated                   84.6 | 84.8    0.432, 0.852 | 0.367, 0.868     5 \n",
    "\n",
    "\n",
    "Analysis of Embedding features\n",
    "Results of Final model generated with BiLSTM-NN from 3 k-Folds and 2 runs with various unigram features:\n",
    "\n",
    "                                                                   Run1 | Run2            Run1 | Run2\n",
    "Unigram feature:  Description:                                     F1-overall:    loss,  acc (on final model):   Epochs:\n",
    "1000000000000000  only Word2Vec Google News Embedding              84.0 | 83.3    0.446, 0.843 | 0.459, 0.837     5\n",
    "0100000000000000  only Word2Vec Twitter News Embedding             84.0 | 83.6    0.460, 0.850 | 0.440, 0.847     5\n",
    "0010000000000000  only GloVe Twitter Embedding                     81.9 | 82.6    0.439, 0.847 | 0.465, 0.823    10\n",
    "0000010000000000  only Glove Common Crawl Embedding                84.5 | 84.8    0.430, 0.849 | 0.417, 0.848    10\n",
    "1100010000000000  all Embeddings without GloVe Twitter             84.3 | 83.9    0.408, 0.853 | 0.412, 0.856     5\n",
    "1110010000000000  all Embeddings                                   84.7 | 84.7    0.384, 0.867 | 0.404, 0.863     5                5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
