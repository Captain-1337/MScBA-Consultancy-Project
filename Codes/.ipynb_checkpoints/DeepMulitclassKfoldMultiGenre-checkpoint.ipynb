{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from corpora_utils import CorporaHelper,CorporaDomains, CorporaProperties\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.layers.convolutional import Conv1D\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import html\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from corpora_utils import CorporaHelper,CorporaDomains, CorporaProperties\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = 'All this not sleeping has a terrible way of playing with your memory.' # fear => test\n",
    "#corpus = \"The Rock is destined to be the 21st Century s new Conan and that he s going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\"\n",
    "corpus = 'If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .' # joy \n",
    "# load data\n",
    "labels = []\n",
    "texts = []\n",
    "corpora_helper = CorporaHelper(\"multigenre.csv\")\n",
    "count_joy = 0\n",
    "count_sadness = 0\n",
    "count_anger = 0\n",
    "count_fear = 0\n",
    "number_of_classes: 4\n",
    "max_per_emotion = 240\n",
    "max_data = 4*240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing corpora\n",
    "corpora_helper.translate_contractions() # problem space before '\n",
    "corpora_helper.translate_urls() # http;/sdasd  => URL\n",
    "#corpora_helper.translate_emoticons()\n",
    "#corpora_helper.translate_emojis()\n",
    "#corpora_helper.translate_html_tags()\n",
    "corpora_helper.translate_camel_case()\n",
    "corpora_helper.translate_underscore()\n",
    "# todo remove @blabla\n",
    "corpora_helper.add_space_at_special_chars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of anger labels:  240\n",
      "number of fear labels:  240\n",
      "number of joy labels:  240\n",
      "number of sadness labels:  240\n"
     ]
    }
   ],
   "source": [
    "for index, corpus in corpora_helper.get_data().iterrows():\n",
    "    # only moviereviews\n",
    "    if True or corpus[CorporaProperties.DOMAIN.value] == CorporaDomains.MOVIEREVIEW.value:\n",
    "        # only joy\n",
    "        # only disgust\n",
    "        if corpus[CorporaProperties.EMOTION.value] == 'anger':\n",
    "            if max_per_emotion > count_anger:\n",
    "                texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "                labels.append(0)\n",
    "                count_anger += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'fear':\n",
    "            if max_per_emotion > count_fear:\n",
    "                texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "                labels.append(1)\n",
    "                count_fear += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'joy':\n",
    "            if max_per_emotion > count_joy:\n",
    "                texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "                labels.append(2)\n",
    "                count_joy += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'sadness':\n",
    "            if max_per_emotion > count_sadness:\n",
    "                texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "                labels.append(3)\n",
    "                count_sadness += 1\n",
    "print('number of anger labels: ',count_anger)\n",
    "print('number of fear labels: ', count_fear)\n",
    "print('number of joy labels: ',count_joy)\n",
    "print('number of sadness labels: ', count_sadness)\n",
    "max_data = count_anger + count_fear + count_joy + count_sadness\n",
    "# 0 anger\n",
    "# 1 fear\n",
    "# 2 joy\n",
    "# 3 sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold variables\n",
    "num_folds = 10\n",
    "fold_no = 1\n",
    "skfold = StratifiedKFold(n_splits = num_folds, random_state = 7, shuffle = True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4115 eindeutige Tokens gefunden.\n"
     ]
    }
   ],
   "source": [
    "## Create one hot encoding\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print ('%s eindeutige Tokens gefunden.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared Multigenre embedding\n",
    "\n",
    "word_embeddings_path = 'multigenre_embedding_final_new.pkl'\n",
    "with open(word_embeddings_path, 'rb') as word_embeddings_file:\n",
    "    embedding_info = pickle.load(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "    \n",
    "def get_unigram_embedding(word, word_embedding_dict, bin_string):\n",
    "    \n",
    "    if word in word_embedding_dict:\n",
    "        word_feature_embedding_dict = word_embedding_dict[word]\n",
    "        final_embedding = np.array([])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for i in range(16):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 2071\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "unigram_feature_string = \"1111111111111111\"\n",
    "#unigram_feature_string = \"0000000000000000\"\n",
    "#word_indices_len = len(word_indices)\n",
    "pre_padding = 0\n",
    "embeddings_index = embedding_info[0]\n",
    "MAX_SEQUENCE_LENGTH = embedding_info[1]\n",
    "maxlen = MAX_SEQUENCE_LENGTH\n",
    "#MAX_NB_WORDS = 10000\n",
    "\n",
    "EMBEDDING_DIM = len(get_unigram_embedding(\"glad\", embedding_info[0], unigram_feature_string))\n",
    "print(\"Embedding dimension:\",EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "word_embedding_matrix = list()\n",
    "word_embedding_matrix = np.zeros((max_words, EMBEDDING_DIM)) # evtl. change to word_indices_len\n",
    "#word_embedding_matrix.append(np.zeros(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items(): # sorted(word_indices, key=word_indices.get):\n",
    "    embedding_features = get_unigram_embedding(word, embedding_info[0], unigram_feature_string)\n",
    "    if i < max_words:\n",
    "        if embedding_features is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            word_embedding_matrix[i] = embedding_features\n",
    "\n",
    "word_embedding_matrix = np.asarray(word_embedding_matrix, dtype='f')\n",
    "word_embedding_matrix = scale(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM 2071\n",
      "input_length 130\n"
     ]
    }
   ],
   "source": [
    "#print('word_indices_len',word_indices_len)\n",
    "print('EMBEDDING_DIM',EMBEDDING_DIM)\n",
    "print('input_length', MAX_SEQUENCE_LENGTH + pre_padding)\n",
    "embedding = Embedding(max_words, EMBEDDING_DIM, input_length=maxlen, trainable=False)\n",
    "#embedding = Embedding(word_indices_len + 1, EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH + pre_padding, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (960, 130)\n",
      "Shape of labels: (960,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "#maxlen = 100 # max. number of words in sequences\n",
    "\n",
    "training_samples = int(max_data * 0.9) #672  70% of 960\n",
    "validation_samples = int(max_data * 0) # 192 20% of 960\n",
    "test_samples = int(max_data * 0.1) #96  10% of 960\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix the data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and validate\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "x_test = data[training_samples + validation_samples: training_samples + validation_samples + test_samples]\n",
    "y_test = labels[training_samples + validation_samples: training_samples + validation_samples + test_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Train on 777 samples, validate on 87 samples\n",
      "Epoch 1/10\n",
      "777/777 [==============================] - 5s 6ms/step - loss: 1.3634 - acc: 0.3333 - val_loss: 1.2186 - val_acc: 0.4483\n",
      "Epoch 2/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.2491 - acc: 0.4453 - val_loss: 1.1091 - val_acc: 0.5402\n",
      "Epoch 3/10\n",
      "777/777 [==============================] - 4s 6ms/step - loss: 1.1299 - acc: 0.4968 - val_loss: 1.0670 - val_acc: 0.4368\n",
      "Epoch 4/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.0038 - acc: 0.5869 - val_loss: 0.9687 - val_acc: 0.5517\n",
      "Epoch 5/10\n",
      "777/777 [==============================] - 4s 6ms/step - loss: 0.8848 - acc: 0.6486 - val_loss: 1.0021 - val_acc: 0.4943\n",
      "Epoch 6/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.8197 - acc: 0.6744 - val_loss: 1.0154 - val_acc: 0.5172\n",
      "Epoch 7/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.7523 - acc: 0.6950 - val_loss: 0.9787 - val_acc: 0.5287\n",
      "Epoch 8/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.6366 - acc: 0.7735 - val_loss: 0.9852 - val_acc: 0.5517\n",
      "Epoch 9/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.5627 - acc: 0.8005 - val_loss: 0.9700 - val_acc: 0.5977\n",
      "Epoch 10/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.5307 - acc: 0.8121 - val_loss: 1.0289 - val_acc: 0.5287\n",
      "87/87 [==============================] - 0s 1ms/step\n",
      "Score for fold 1: ... of 1.0288782119750977; ... of 52.87356376647949%\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Train on 777 samples, validate on 87 samples\n",
      "Epoch 1/10\n",
      "777/777 [==============================] - 5s 7ms/step - loss: 1.3890 - acc: 0.3102 - val_loss: 1.3644 - val_acc: 0.3103\n",
      "Epoch 2/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.3249 - acc: 0.3604 - val_loss: 1.2701 - val_acc: 0.4483\n",
      "Epoch 3/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.1970 - acc: 0.4633 - val_loss: 1.1980 - val_acc: 0.4713\n",
      "Epoch 4/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.0875 - acc: 0.5212 - val_loss: 1.1608 - val_acc: 0.4943\n",
      "Epoch 5/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.9662 - acc: 0.6062 - val_loss: 1.0893 - val_acc: 0.4483\n",
      "Epoch 6/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.8830 - acc: 0.6499 - val_loss: 1.0056 - val_acc: 0.5057\n",
      "Epoch 7/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.7898 - acc: 0.7104 - val_loss: 0.9881 - val_acc: 0.5862\n",
      "Epoch 8/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.6880 - acc: 0.7593 - val_loss: 0.9076 - val_acc: 0.5977\n",
      "Epoch 9/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.5900 - acc: 0.7979 - val_loss: 0.9049 - val_acc: 0.5517\n",
      "Epoch 10/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.5667 - acc: 0.7876 - val_loss: 0.9909 - val_acc: 0.5402\n",
      "87/87 [==============================] - 0s 953us/step\n",
      "Score for fold 2: ... of 0.9909038543701172; ... of 54.0229856967926%\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Train on 777 samples, validate on 87 samples\n",
      "Epoch 1/10\n",
      "777/777 [==============================] - 5s 6ms/step - loss: 1.3362 - acc: 0.3398 - val_loss: 1.1765 - val_acc: 0.4598\n",
      "Epoch 2/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.2219 - acc: 0.4324 - val_loss: 1.0649 - val_acc: 0.5057\n",
      "Epoch 3/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.0797 - acc: 0.5225 - val_loss: 0.9353 - val_acc: 0.6092\n",
      "Epoch 4/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.9774 - acc: 0.5830 - val_loss: 0.8946 - val_acc: 0.6207\n",
      "Epoch 5/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.8931 - acc: 0.6306 - val_loss: 0.9693 - val_acc: 0.5517\n",
      "Epoch 6/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.8027 - acc: 0.6834 - val_loss: 0.8802 - val_acc: 0.5632\n",
      "Epoch 7/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.7035 - acc: 0.7297 - val_loss: 0.8246 - val_acc: 0.6092\n",
      "Epoch 8/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.6536 - acc: 0.7619 - val_loss: 0.8631 - val_acc: 0.6207\n",
      "Epoch 9/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.5577 - acc: 0.8044 - val_loss: 0.8709 - val_acc: 0.6207\n",
      "Epoch 10/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.4605 - acc: 0.8430 - val_loss: 0.9432 - val_acc: 0.6437\n",
      "87/87 [==============================] - 0s 953us/step\n",
      "Score for fold 3: ... of 0.9431810975074768; ... of 64.36781883239746%\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 777 samples, validate on 87 samples\n",
      "Epoch 1/10\n",
      "777/777 [==============================] - 5s 6ms/step - loss: 1.3734 - acc: 0.3359 - val_loss: 1.2500 - val_acc: 0.4368\n",
      "Epoch 2/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.1531 - acc: 0.4981 - val_loss: 1.1241 - val_acc: 0.4483\n",
      "Epoch 3/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 1.0492 - acc: 0.5380 - val_loss: 1.1157 - val_acc: 0.4943\n",
      "Epoch 4/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.9743 - acc: 0.5804 - val_loss: 1.1158 - val_acc: 0.4828\n",
      "Epoch 5/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.8748 - acc: 0.6474 - val_loss: 1.1060 - val_acc: 0.5747\n",
      "Epoch 6/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.7719 - acc: 0.7040 - val_loss: 1.1421 - val_acc: 0.4828\n",
      "Epoch 7/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.6739 - acc: 0.7542 - val_loss: 1.1727 - val_acc: 0.5172\n",
      "Epoch 8/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.5904 - acc: 0.7889 - val_loss: 1.2006 - val_acc: 0.5172\n",
      "Epoch 9/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.4851 - acc: 0.8378 - val_loss: 1.2271 - val_acc: 0.5747\n",
      "Epoch 10/10\n",
      "777/777 [==============================] - 4s 5ms/step - loss: 0.4309 - acc: 0.8481 - val_loss: 1.2644 - val_acc: 0.5747\n",
      "87/87 [==============================] - 0s 942us/step\n",
      "Score for fold 4: ... of 1.2644299268722534; ... of 57.47126340866089%\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Train on 778 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "778/778 [==============================] - 5s 6ms/step - loss: 1.4048 - acc: 0.2866 - val_loss: 1.2159 - val_acc: 0.4884\n",
      "Epoch 2/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.2287 - acc: 0.4460 - val_loss: 1.1796 - val_acc: 0.4070\n",
      "Epoch 3/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.1636 - acc: 0.5090 - val_loss: 1.0205 - val_acc: 0.5698\n",
      "Epoch 4/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.0632 - acc: 0.5103 - val_loss: 0.9643 - val_acc: 0.5581\n",
      "Epoch 5/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.9681 - acc: 0.5540 - val_loss: 0.8801 - val_acc: 0.5814\n",
      "Epoch 6/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.8600 - acc: 0.6478 - val_loss: 0.8666 - val_acc: 0.6047\n",
      "Epoch 7/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.7400 - acc: 0.7224 - val_loss: 0.8949 - val_acc: 0.5930\n",
      "Epoch 8/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.6689 - acc: 0.7494 - val_loss: 0.8602 - val_acc: 0.6047\n",
      "Epoch 9/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.5706 - acc: 0.7969 - val_loss: 0.8392 - val_acc: 0.6395\n",
      "Epoch 10/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.4936 - acc: 0.8201 - val_loss: 0.8645 - val_acc: 0.6744\n",
      "86/86 [==============================] - 0s 976us/step\n",
      "Score for fold 5: ... of 0.8645070195198059; ... of 67.44186282157898%\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Train on 778 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "778/778 [==============================] - 5s 6ms/step - loss: 1.3679 - acc: 0.3226 - val_loss: 1.2647 - val_acc: 0.4302\n",
      "Epoch 2/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.2003 - acc: 0.4370 - val_loss: 1.1904 - val_acc: 0.4767\n",
      "Epoch 3/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.1101 - acc: 0.5000 - val_loss: 1.1561 - val_acc: 0.4535\n",
      "Epoch 4/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.9768 - acc: 0.6041 - val_loss: 1.1240 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.9026 - acc: 0.6144 - val_loss: 1.1364 - val_acc: 0.4419\n",
      "Epoch 6/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.7947 - acc: 0.6799 - val_loss: 1.1476 - val_acc: 0.4767\n",
      "Epoch 7/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.7218 - acc: 0.7404 - val_loss: 1.1675 - val_acc: 0.4884\n",
      "Epoch 8/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.6357 - acc: 0.7545 - val_loss: 1.1575 - val_acc: 0.5581\n",
      "Epoch 9/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.5697 - acc: 0.8111 - val_loss: 1.1934 - val_acc: 0.4651\n",
      "Epoch 10/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.4926 - acc: 0.8175 - val_loss: 1.1984 - val_acc: 0.5233\n",
      "86/86 [==============================] - 0s 895us/step\n",
      "Score for fold 6: ... of 1.1984199285507202; ... of 52.32558250427246%\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Train on 778 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "778/778 [==============================] - 5s 6ms/step - loss: 1.3537 - acc: 0.3496 - val_loss: 1.1174 - val_acc: 0.5233\n",
      "Epoch 2/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.1671 - acc: 0.4987 - val_loss: 1.0717 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.0485 - acc: 0.5296 - val_loss: 0.9499 - val_acc: 0.5698\n",
      "Epoch 4/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.8911 - acc: 0.6581 - val_loss: 0.9523 - val_acc: 0.5581\n",
      "Epoch 5/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.7874 - acc: 0.6902 - val_loss: 1.0084 - val_acc: 0.5349\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778/778 [==============================] - 4s 5ms/step - loss: 0.6700 - acc: 0.7378 - val_loss: 0.9311 - val_acc: 0.6279\n",
      "Epoch 7/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.5414 - acc: 0.8085 - val_loss: 1.0070 - val_acc: 0.5698\n",
      "Epoch 8/10\n",
      "778/778 [==============================] - 4s 6ms/step - loss: 0.5210 - acc: 0.8111 - val_loss: 0.9846 - val_acc: 0.6047\n",
      "Epoch 9/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.4262 - acc: 0.8586 - val_loss: 0.9866 - val_acc: 0.6163\n",
      "Epoch 10/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.3786 - acc: 0.8779 - val_loss: 1.1612 - val_acc: 0.5349\n",
      "86/86 [==============================] - 0s 906us/step\n",
      "Score for fold 7: ... of 1.1611888408660889; ... of 53.48837375640869%\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Train on 778 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "778/778 [==============================] - 5s 6ms/step - loss: 1.3601 - acc: 0.3380 - val_loss: 1.2222 - val_acc: 0.3721\n",
      "Epoch 2/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.2106 - acc: 0.4743 - val_loss: 1.1413 - val_acc: 0.4419\n",
      "Epoch 3/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.1294 - acc: 0.4884 - val_loss: 1.0664 - val_acc: 0.5116\n",
      "Epoch 4/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.9949 - acc: 0.6093 - val_loss: 1.0542 - val_acc: 0.4767\n",
      "Epoch 5/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.9266 - acc: 0.6234 - val_loss: 1.0585 - val_acc: 0.4535\n",
      "Epoch 6/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.8304 - acc: 0.6799 - val_loss: 0.9638 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.7466 - acc: 0.7301 - val_loss: 0.9102 - val_acc: 0.6047\n",
      "Epoch 8/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.6447 - acc: 0.7519 - val_loss: 0.9548 - val_acc: 0.5581\n",
      "Epoch 9/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.5605 - acc: 0.7956 - val_loss: 0.8931 - val_acc: 0.6395\n",
      "Epoch 10/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.4446 - acc: 0.8663 - val_loss: 0.8622 - val_acc: 0.6744\n",
      "86/86 [==============================] - 0s 994us/step\n",
      "Score for fold 8: ... of 0.8621518611907959; ... of 67.44186282157898%\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Train on 778 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "778/778 [==============================] - 5s 6ms/step - loss: 1.3530 - acc: 0.3303 - val_loss: 1.2002 - val_acc: 0.4651\n",
      "Epoch 2/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.2230 - acc: 0.4254 - val_loss: 1.0511 - val_acc: 0.5698\n",
      "Epoch 3/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.0909 - acc: 0.5373 - val_loss: 0.9278 - val_acc: 0.6279\n",
      "Epoch 4/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.9738 - acc: 0.5848 - val_loss: 0.9099 - val_acc: 0.5581\n",
      "Epoch 5/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.8475 - acc: 0.6427 - val_loss: 0.8820 - val_acc: 0.5581\n",
      "Epoch 6/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.7827 - acc: 0.7005 - val_loss: 0.8447 - val_acc: 0.5814\n",
      "Epoch 7/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.6766 - acc: 0.7391 - val_loss: 0.7957 - val_acc: 0.5930\n",
      "Epoch 8/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.6043 - acc: 0.7905 - val_loss: 0.8180 - val_acc: 0.6395\n",
      "Epoch 9/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.5391 - acc: 0.7956 - val_loss: 0.7817 - val_acc: 0.6628\n",
      "Epoch 10/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.4471 - acc: 0.8380 - val_loss: 0.8084 - val_acc: 0.6047\n",
      "86/86 [==============================] - 0s 936us/step\n",
      "Score for fold 9: ... of 0.808435320854187; ... of 60.4651153087616%\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 2071)         20710000  \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 126, 32)           331392    \n",
      "_________________________________________________________________\n",
      "bilstm3 (Bidirectional)      (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 21,091,572\n",
      "Trainable params: 381,572\n",
      "Non-trainable params: 20,710,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Train on 778 samples, validate on 86 samples\n",
      "Epoch 1/10\n",
      "778/778 [==============================] - 4s 6ms/step - loss: 1.3647 - acc: 0.3329 - val_loss: 1.2089 - val_acc: 0.3837\n",
      "Epoch 2/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.2081 - acc: 0.4177 - val_loss: 1.3025 - val_acc: 0.4302\n",
      "Epoch 3/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 1.0675 - acc: 0.5180 - val_loss: 1.1993 - val_acc: 0.4535\n",
      "Epoch 4/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.9660 - acc: 0.5746 - val_loss: 1.2097 - val_acc: 0.4186\n",
      "Epoch 5/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.8762 - acc: 0.6375 - val_loss: 1.0379 - val_acc: 0.5349\n",
      "Epoch 6/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.7822 - acc: 0.6954 - val_loss: 1.1207 - val_acc: 0.4535\n",
      "Epoch 7/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.7112 - acc: 0.7185 - val_loss: 1.1264 - val_acc: 0.4535\n",
      "Epoch 8/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.6014 - acc: 0.7725 - val_loss: 1.0634 - val_acc: 0.5698\n",
      "Epoch 9/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.5089 - acc: 0.8252 - val_loss: 1.1658 - val_acc: 0.5349\n",
      "Epoch 10/10\n",
      "778/778 [==============================] - 4s 5ms/step - loss: 0.4415 - acc: 0.8483 - val_loss: 1.1657 - val_acc: 0.5000\n",
      "86/86 [==============================] - 0s 999us/step\n",
      "Score for fold 10: ... of 1.1656949520111084; ... of 50.0%\n"
     ]
    }
   ],
   "source": [
    "conv_1 = Conv1D(32, 5, activation='relu', name='conv1')\n",
    "conv_2 = Conv1D(32, 5, activation='relu', name='conv2')\n",
    "conv_3 = Conv1D(256, 3, activation='relu', name='conv3')\n",
    "conv_4 = Conv1D(256, 3, activation='relu', name='conv4')\n",
    "conv_5 = Conv1D(256, 3, activation='relu', name='conv5')\n",
    "conv_6 = Conv1D(1024, 3, activation='relu', name='conv6')\n",
    "conv_7 = Conv1D(1024, 3, activation='relu', name='conv7')\n",
    "conv_8 = Conv1D(1024, 3, activation='relu', name='conv8')\n",
    "\n",
    "pool_1 = AveragePooling1D(pool_size=3, strides=2, name='pool1')\n",
    "pool_2 = AveragePooling1D(pool_size=3, strides=2, name='pool2')\n",
    "pool_3 = MaxPooling1D(pool_size=3, strides=2, name='pool3')\n",
    "pool_4 = MaxPooling1D(pool_size=3, strides=2, name='pool4')\n",
    "\n",
    "lstm_1 = LSTM(32, dropout=0.2, recurrent_dropout=0.2, name='lstm1', return_sequences=True)\n",
    "lstm_2 = LSTM(128, dropout=0.2, recurrent_dropout=0.2, name='lstm2', return_sequences=True)\n",
    "lstm_3 = LSTM(64, dropout=0.2, recurrent_dropout=0.2, name='lstm3')\n",
    "\n",
    "gru_1 = GRU(256, dropout=0.25, recurrent_dropout=0.25, name='gru1', return_sequences=True)\n",
    "gru_2 = GRU(256, dropout=0.25, recurrent_dropout=0.25, name='gru2', return_sequences=True)\n",
    "gru_3 = GRU(256, dropout=0.25, recurrent_dropout=0.25, name='gru3')\n",
    "\n",
    "bi_lstm_1 = Bidirectional(lstm_1, name='bilstm1')\n",
    "bi_lstm_2 = Bidirectional(lstm_2, name='bilstm2')\n",
    "bi_lstm_3 = Bidirectional(lstm_3, name='bilstm3')\n",
    "\n",
    "dense_1 = Dense(32, activation='relu', name='dense1')\n",
    "dense_2 = Dense(4, activation='sigmoid', name='dense2')\n",
    "\n",
    "drop_1 = Dropout(0.5, name='drop1')\n",
    "drop_2 = Dropout(0.5, name='drop2')\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "fold_no = 1\n",
    "\n",
    "for train_ind, val_ind in skfold.split(x_train,y_train):\n",
    "\n",
    "    #Create model\n",
    "    #model = Sequential()\n",
    "    #model.add(embedding)\n",
    "    #model.add(Conv1D(32,5, activation='relu'))\n",
    "    #model.add(Flatten()) #3D to 2D\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dense(4, activation='softmax'))\n",
    "    #model.summary()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(32, 5, activation='relu', name='conv1'))\n",
    "    #model.add(Conv1D(32, 5, activation='relu', name='conv2'))\n",
    "    ##model.add(pool_1)\n",
    "    ##model.add(conv_3)\n",
    "    ##model.add(conv_4)\n",
    "    ##model.add(pool_2)\n",
    "    ##model.add(conv_5)\n",
    "    ##model.add(lstm_1)\n",
    "    ##model.add(lstm_2)\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.4, recurrent_dropout=0.4, name='lstm3'), name='bilstm3'))\n",
    "    #model.add(Dense(32, activation='relu', name='dense1'))\n",
    "    ##model.add(drop_1)\n",
    "    model.add(Dense(4, activation='softmax', name='dense2'))\n",
    "    model.summary()\n",
    "\n",
    "    # Load GloVe embedding\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "\n",
    "    # Train and Evaluate\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    history = model.fit(x_train[train_ind], y_train[train_ind],\n",
    "                        epochs=10,\n",
    "                        batch_size=32,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_train[val_ind], y_train[val_ind]))\n",
    "\n",
    "    # metrics\n",
    "    scores = model.evaluate(x_train[val_ind], y_train[val_ind], batch_size=128)\n",
    "    #print(f'Score for fold {fold_no}: {model.metrics_name[0]} of {scores[0]}; {model.metrics_name[1]} of {scores[1]*100}%')\n",
    "    print(f'Score for fold {fold_no}: ... of {scores[0]}; ... of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1]*100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.0288782119750977 - Accuracy: 52.87356376647949%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.9909038543701172 - Accuracy: 54.0229856967926%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.9431810975074768 - Accuracy: 64.36781883239746%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.2644299268722534 - Accuracy: 57.47126340866089%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.8645070195198059 - Accuracy: 67.44186282157898%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.1984199285507202 - Accuracy: 52.32558250427246%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 1.1611888408660889 - Accuracy: 53.48837375640869%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.8621518611907959 - Accuracy: 67.44186282157898%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.808435320854187 - Accuracy: 60.4651153087616%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 1.1656949520111084 - Accuracy: 50.0%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 57.989842891693115 (+- 6.195585238435796)\n",
      "> Loss: 1.028779101371765\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.1370117664337158 - Accuracy: 60.919541120529175%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.3123679161071777 - Accuracy: 49.425286054611206%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.9504621028900146 - Accuracy: 63.21839094161987%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.4416422843933105 - Accuracy: 54.0229856967926%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.9941840767860413 - Accuracy: 63.95348906517029%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.2764909267425537 - Accuracy: 56.976741552352905%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.9973974823951721 - Accuracy: 58.139532804489136%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.9462456703186035 - Accuracy: 68.60465407371521%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 1.1384328603744507 - Accuracy: 56.976741552352905%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 1.3700578212738037 - Accuracy: 53.48837375640869%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 58.5725736618042 (+- 5.405463504125277)\n",
      "> Loss: 1.1564292907714844\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate you bastard ! Go away !\n",
      "prediction: [[1.33738965e-02 2.15144813e-01 1.09855209e-04 2.78741196e-02]]\n",
      "This is a lovely film , but it makes me sad .\n",
      "prediction: [[1.9559909e-03 9.5694375e-01 7.5814046e-06 5.2494201e-05]]\n",
      "That is because - damn it !\n",
      "prediction: [[0.0039189  0.11857425 0.00050364 0.01575176]]\n",
      "Die bitch ! You make me angry .\n",
      "prediction: [[1.33738965e-02 2.15144813e-01 1.09855209e-04 2.78741196e-02]]\n",
      "My son died .\n",
      "prediction: [[0.01146895 0.00296643 0.13873987 0.00632519]]\n",
      "I am afraid because of this horror .\n",
      "prediction: [[1.4921252e-03 6.5133584e-01 1.0503677e-02 3.0726520e-04]]\n"
     ]
    }
   ],
   "source": [
    "## Example use of the model!\n",
    "test_corpora = ['I hate you bastard ! Go away !', 'This is a lovely film , but it makes me sad .','That is because - damn it !']\n",
    "test_corpora.append('Die bitch ! You make me angry .')\n",
    "test_corpora.append('My son died .')\n",
    "test_corpora.append('I am afraid because of this horror .')\n",
    "for text in test_corpora:\n",
    "    textarray = [text]\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(textarray)\n",
    "    sequences = tokenizer.texts_to_sequences(textarray)\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "    print(text)\n",
    "    pred = model.predict(data)\n",
    "    print(\"prediction:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
