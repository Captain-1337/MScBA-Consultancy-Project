{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, LSTM, GRU, AveragePooling1D, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "#import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from corpora_utils import CorporaHelper,CorporaDomains, CorporaProperties\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Activate GPU\n",
    "#WARNING GPU TAKES 5 TIMES LONGER THAN CPU! With Consul Project 1\n",
    "#Check for GPU\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n",
    "# GPU CONFIG\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\"\"\"\n",
    "\n",
    "#Deactivate GPU\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIGENRE = 'muligenre'\n",
    "TWITTER = 'twitter'\n",
    "MG_AND_TWITTER = 'mg_and_twitter'\n",
    "\n",
    "# set wich corpora to use Multigenre or twitter\n",
    "use_mg_train_corpora = TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_labels = []\n",
    "train_texts = []\n",
    "test_labels = []\n",
    "test_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpora(filepath, sep=';'):\n",
    "    print('Load: ', filepath)\n",
    "    corpora_helper = CorporaHelper(filepath, separator=sep)\n",
    "    count_joy = 0\n",
    "    count_sadness = 0\n",
    "    count_anger = 0\n",
    "    count_fear = 0\n",
    "    labels = []\n",
    "    texts = []\n",
    "    # preprocessing corpora\n",
    "    corpora_helper.translate_urls()\n",
    "    corpora_helper.translate_emoticons()\n",
    "    corpora_helper.translate_emojis()\n",
    "    corpora_helper.translate_email()\n",
    "    #corpora_helper.translate_mention()\n",
    "    corpora_helper.translate_html_tags()\n",
    "    #corpora_helper.translate_camel_case()\n",
    "    corpora_helper.translate_underscore()\n",
    "\n",
    "    corpora_helper.translate_string('-LRB-','(')\n",
    "    corpora_helper.translate_string('-RRB-',')')\n",
    "    corpora_helper.translate_string('`',\"'\") # ` to '\n",
    "    corpora_helper.translate_string(\"''\",'\"') # double '' to \"\n",
    "    corpora_helper.translate_contractions()\n",
    "    corpora_helper.translate_string(\"'\",\"\") # remove '\n",
    "    corpora_helper.translate_string(\"\\\\n\",\" \") # replace new lines with space\n",
    "\n",
    "    #corpora_helper.spell_correction()\n",
    "    corpora_helper.add_space_at_special_chars()\n",
    "    corpora_helper.add_space_at_special_chars(regexlist = r\"([#])\")\n",
    "    #corpora_helper.translate_to_lower()\n",
    "\n",
    "    # 0 anger\n",
    "    # 1 fear\n",
    "    # 2 joy\n",
    "    # 3 sadness\n",
    "    for index, corpus in corpora_helper.get_data().iterrows():\n",
    "        if corpus[CorporaProperties.EMOTION.value] == 'anger':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(0)\n",
    "            count_anger += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'fear':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(1)\n",
    "            count_fear += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'joy':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(2)\n",
    "            count_joy += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'sadness':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(3)\n",
    "            count_sadness += 1\n",
    "    print('number of anger labels: ',count_anger)\n",
    "    print('number of fear labels: ', count_fear)\n",
    "    print('number of joy labels: ', count_joy)\n",
    "    print('number of sadness labels: ', count_sadness)\n",
    "    print('----------------------------------------------------------------------')\n",
    "    return texts, labels\n",
    "    #max_data = count_anger + count_fear + count_joy + count_sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use TWITTER train corpora\n",
      "Load:  corpora/twitter_2000_train.csv\n",
      "number of anger labels:  1800\n",
      "number of fear labels:  1800\n",
      "number of joy labels:  1800\n",
      "number of sadness labels:  1800\n",
      "----------------------------------------------------------------------\n",
      "Load:  corpora/twitter_2000_test.csv\n",
      "number of anger labels:  200\n",
      "number of fear labels:  200\n",
      "number of joy labels:  200\n",
      "number of sadness labels:  200\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_file = \"\"\n",
    "test_file = \"\"\n",
    "sep = ';'\n",
    "word_embeddings_path = ''\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    train_file = \"corpora/multigenre_450_train.csv\"\n",
    "    test_file = \"corpora/multigenre_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = ';'\n",
    "    print(\"Use MULTIGENRE train corpora\")\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    train_file = \"corpora/twitter_2000_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER train corpora\")\n",
    "else:\n",
    "    train_file = \"corpora/twitter_2000_mg_450_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_mg_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER and MULTIGENRE train corpora\")\n",
    "    \n",
    "train_texts, train_labels = load_corpora(train_file, sep=sep)\n",
    "test_texts, test_labels = load_corpora(test_file, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared custom ensemble embedding\n",
    "with open(word_embeddings_path, 'rb') as word_embeddings_file:\n",
    "    embedding_info = pickle.load(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding helper functions\n",
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "    \n",
    "def get_unigram_embedding(word, word_embedding_dict, bin_string):\n",
    "    \n",
    "    if word in word_embedding_dict:\n",
    "        word_feature_embedding_dict = word_embedding_dict[word]\n",
    "        final_embedding = np.array([])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for i in range(16):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen:  100\n"
     ]
    }
   ],
   "source": [
    "pre_padding = 0\n",
    "embeddings_index = embedding_info[0]\n",
    "MAX_SEQUENCE_LENGTH = embedding_info[1]\n",
    "maxlen = MAX_SEQUENCE_LENGTH\n",
    "print(\"maxlen: \",maxlen)\n",
    "#MAX_NB_WORDS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Unigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting relevant embeddings for multigenre\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    # Multigenre\n",
    "    unigram_feature_string = \"1001111111111101\"\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    # Twitter\n",
    "    unigram_feature_string = \"0110001111111101\"\n",
    "    unigram_feature_string = \"1111111111111111\"\n",
    "else:\n",
    "    # Twitter and Multigenre\n",
    "    unigram_feature_string = \"1110010000000000\"\n",
    "# 1 Google news pretrained vectors : GoogleNews-vectors-negative300.bin.gz  \n",
    "# 2 Twitter pretrained vectors: word2vec_twitter_model.bin\n",
    "# 3 glove.twitter.27B.200d.txt\n",
    "# 4 glove.6B.300d.txt\n",
    "# 5 glove.42B.300d.txt\n",
    "# 6 glove.840B.300d.txt\n",
    "# 7 NRC Emotion Intensity Lexicon\n",
    "# 8 senti word net\n",
    "#9  NRC Sentiment lexicon: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#10 lexicons/Emoticon-unigrams.txt\n",
    "#11 lexicons/Emoticon-AFFLEX-NEGLEX-unigrams.txt\n",
    "#12 NRC Hashtag Lexica: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#13 HS-unigrams.txtNRC-Hashtag-Emotion-Lexicon-v0.2.txt\n",
    "#14 HS-AFFLEX-NEGLEX-unigrams.txt\n",
    "#15 Emoji Polarities\n",
    "#16 Depeche mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep learning with multigenre and twitter corpus and 4 emotions\n",
    "\"\"\"\n",
    "# K-Fold variables\n",
    "num_folds = 10 # 10\n",
    "fold_runs = 3 # 3\n",
    "fold_no = 1\n",
    "# train\n",
    "epochs = 4\n",
    "max_words = 10000\n",
    "# max. different words:\n",
    "# Multigerne: 5140  => 10000 or 3000 or 1000 ?\n",
    "# Twitter: 17580 => 20000 or 10000 ?\n",
    "# MG and Twitter: 20073 => evtl. 20000?\n",
    "#optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer = Adam(learning_rate=0.001) # default 0.001\n",
    "skfold = StratifiedKFold(n_splits = num_folds, random_state = 7, shuffle = True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_per_fold = []\n",
    "avg_acc_per_run = []\n",
    "avg_loss_per_run = []\n",
    "avg_precision_per_run = []\n",
    "avg_recall_per_run = []\n",
    "avg_f1_per_run = []\n",
    "create_final_model = True\n",
    "# run only final model without kfold\n",
    "run_final_train_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 2164\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = len(get_unigram_embedding(\"glad\", embedding_info[0], unigram_feature_string))\n",
    "print(\"Embedding dimension:\",EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, filters = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train an test data set\n",
    "def create_data(texts, labels, maxlen):\n",
    "    ## Create one hot encoding\n",
    "    #max_words = 10000\n",
    "    #maxlen = 100 # max. number of words in sequences\n",
    "    #tokenizer = Tokenizer(num_words=max_words, filters = '')\n",
    "    #tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    #word_i = tokenizer.word_index\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    labels_arr = np.asarray(labels)\n",
    "    print('Shape of data:', data.shape)\n",
    "    print('Shape of labels:', labels_arr.shape)\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "    # mix the data\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels_arr = labels_arr[indices]\n",
    "\n",
    "    # split in train and validate\n",
    "    x_data = data\n",
    "    y_data = labels_arr\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer\n",
    "all_texts = train_texts.copy()\n",
    "all_texts.append(test_texts.copy())\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (7200, 100)\n",
      "Shape of labels: (7200,)\n",
      "-------------------------------------------\n",
      "Shape of data: (800, 100)\n",
      "Shape of labels: (800,)\n",
      "-------------------------------------------\n",
      "16580 unique Tokens found.\n"
     ]
    }
   ],
   "source": [
    "# Train an word index for embedding enrichment\n",
    "x_train, y_train = create_data(train_texts, train_labels, maxlen)\n",
    "x_test, y_test = create_data(test_texts, test_labels, maxlen)\n",
    "word_index = tokenizer.word_index\n",
    "x_train_copy = x_train.copy()\n",
    "y_train_copy = y_train.copy()\n",
    "print ('%s unique Tokens found.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Matrix\n",
    "word_embedding_matrix = list()\n",
    "word_embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "#word_embedding_matrix.append(np.zeros(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items(): # sorted(word_indices, key=word_indices.get):\n",
    "    embedding_features = get_unigram_embedding(word, embedding_info[0], unigram_feature_string)\n",
    "    if i < max_words:\n",
    "        if embedding_features is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            word_embedding_matrix[i] = embedding_features\n",
    "\n",
    "word_embedding_matrix = np.asarray(word_embedding_matrix, dtype='f')\n",
    "word_embedding_matrix = scale(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM 2164\n",
      "input_length 100\n"
     ]
    }
   ],
   "source": [
    "#print('word_indices_len',word_indices_len)\n",
    "print('EMBEDDING_DIM',EMBEDDING_DIM)\n",
    "print('input_length', MAX_SEQUENCE_LENGTH + pre_padding)\n",
    "embedding = Embedding(max_words, EMBEDDING_DIM, input_length=maxlen, trainable=False)\n",
    "#embedding = Embedding(word_indices_len + 1, EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH + pre_padding, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running \"optimal\" BiLSTM Model with 3 Runs and 10 k-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "        \n",
    "    model.add(Bidirectional(LSTM(32, dropout=0.4, recurrent_dropout=0.4, return_sequences=True)))\n",
    "    \n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"Adam\", learningrate = 0.0001)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 41s 6ms/step - loss: 0.8271 - acc: 0.6715 - val_loss: 0.4251 - val_acc: 0.8403\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 37s 6ms/step - loss: 0.3384 - acc: 0.8880 - val_loss: 0.3277 - val_acc: 0.8722\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 36s 5ms/step - loss: 0.2527 - acc: 0.9105 - val_loss: 0.3252 - val_acc: 0.8708\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 27s 4ms/step - loss: 0.2067 - acc: 0.9265 - val_loss: 0.3172 - val_acc: 0.8667\n",
      "720/720 [==============================] - 1s 2ms/step\n",
      "Score for fold 1: loss of 0.3172317736678653; accuracy of 86.66666746139526%\n",
      "720/720 [==============================] - 1s 2ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 27s 4ms/step - loss: 0.6260 - acc: 0.7509 - val_loss: 0.3334 - val_acc: 0.8778\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 43s 7ms/step - loss: 0.2668 - acc: 0.9034 - val_loss: 0.3102 - val_acc: 0.8806\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 37s 6ms/step - loss: 0.2027 - acc: 0.9258 - val_loss: 0.3019 - val_acc: 0.8792\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 44s 7ms/step - loss: 0.1585 - acc: 0.9418 - val_loss: 0.3329 - val_acc: 0.8708\n",
      "720/720 [==============================] - 2s 3ms/step\n",
      "Score for fold 2: loss of 0.3329445719718933; accuracy of 87.08333373069763%\n",
      "720/720 [==============================] - 3s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 39s 6ms/step - loss: 0.5619 - acc: 0.7818 - val_loss: 0.2996 - val_acc: 0.8889\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 44s 7ms/step - loss: 0.2501 - acc: 0.9062 - val_loss: 0.2777 - val_acc: 0.8944\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 39s 6ms/step - loss: 0.1939 - acc: 0.9282 - val_loss: 0.2722 - val_acc: 0.8986\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 43s 7ms/step - loss: 0.1601 - acc: 0.9412 - val_loss: 0.2833 - val_acc: 0.8819\n",
      "720/720 [==============================] - 1s 2ms/step\n",
      "Score for fold 3: loss of 0.28329672945870293; accuracy of 88.19444179534912%\n",
      "720/720 [==============================] - 2s 2ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 51s 8ms/step - loss: 0.5776 - acc: 0.7674 - val_loss: 0.3375 - val_acc: 0.8778\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 40s 6ms/step - loss: 0.2690 - acc: 0.8992 - val_loss: 0.2884 - val_acc: 0.8750\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 43s 7ms/step - loss: 0.2001 - acc: 0.9301 - val_loss: 0.2959 - val_acc: 0.8778\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 44s 7ms/step - loss: 0.1598 - acc: 0.9420 - val_loss: 0.2995 - val_acc: 0.8833\n",
      "720/720 [==============================] - 2s 3ms/step\n",
      "Score for fold 4: loss of 0.29949496222866906; accuracy of 88.33333253860474%\n",
      "720/720 [==============================] - 4s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 42s 6ms/step - loss: 0.5974 - acc: 0.7662 - val_loss: 0.3516 - val_acc: 0.8847\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 43s 7ms/step - loss: 0.2657 - acc: 0.9028 - val_loss: 0.2936 - val_acc: 0.8861\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 47s 7ms/step - loss: 0.2007 - acc: 0.9252 - val_loss: 0.3002 - val_acc: 0.8944\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 41s 6ms/step - loss: 0.1667 - acc: 0.9378 - val_loss: 0.3011 - val_acc: 0.8986\n",
      "720/720 [==============================] - 2s 3ms/step\n",
      "Score for fold 5: loss of 0.30106100142002107; accuracy of 89.86111283302307%\n",
      "720/720 [==============================] - 4s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 48s 7ms/step - loss: 0.5733 - acc: 0.7730 - val_loss: 0.3195 - val_acc: 0.8750\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 49s 8ms/step - loss: 0.2673 - acc: 0.8995 - val_loss: 0.2907 - val_acc: 0.9000\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 43s 7ms/step - loss: 0.2039 - acc: 0.9250 - val_loss: 0.3077 - val_acc: 0.8944\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 50s 8ms/step - loss: 0.1680 - acc: 0.9377 - val_loss: 0.2836 - val_acc: 0.9056\n",
      "720/720 [==============================] - 2s 3ms/step\n",
      "Score for fold 6: loss of 0.28357724646727245; accuracy of 90.55555462837219%\n",
      "720/720 [==============================] - 3s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 59s 9ms/step - loss: 0.5495 - acc: 0.7873 - val_loss: 0.3160 - val_acc: 0.8833\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 58s 9ms/step - loss: 0.2687 - acc: 0.9009 - val_loss: 0.3020 - val_acc: 0.8792\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 54s 8ms/step - loss: 0.2018 - acc: 0.9267 - val_loss: 0.2899 - val_acc: 0.8847\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 55s 9ms/step - loss: 0.1709 - acc: 0.9386 - val_loss: 0.2896 - val_acc: 0.8917\n",
      "720/720 [==============================] - 3s 5ms/step\n",
      "Score for fold 7: loss of 0.28964903785122764; accuracy of 89.16666507720947%\n",
      "720/720 [==============================] - 6s 8ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 57s 9ms/step - loss: 0.6364 - acc: 0.7520 - val_loss: 0.3346 - val_acc: 0.8792\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 53s 8ms/step - loss: 0.2808 - acc: 0.8980 - val_loss: 0.2905 - val_acc: 0.8861\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 55s 9ms/step - loss: 0.2076 - acc: 0.9276 - val_loss: 0.2775 - val_acc: 0.8944\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 57s 9ms/step - loss: 0.1701 - acc: 0.9414 - val_loss: 0.2824 - val_acc: 0.8972\n",
      "720/720 [==============================] - 3s 4ms/step\n",
      "Score for fold 8: loss of 0.2824415922164917; accuracy of 89.72222208976746%\n",
      "720/720 [==============================] - 5s 7ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 57s 9ms/step - loss: 0.5799 - acc: 0.7710 - val_loss: 0.3699 - val_acc: 0.8514\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 57s 9ms/step - loss: 0.2639 - acc: 0.9043 - val_loss: 0.3428 - val_acc: 0.8653\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 59s 9ms/step - loss: 0.1970 - acc: 0.9299 - val_loss: 0.3340 - val_acc: 0.8694\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 59s 9ms/step - loss: 0.1647 - acc: 0.9414 - val_loss: 0.3293 - val_acc: 0.8722\n",
      "720/720 [==============================] - 4s 5ms/step\n",
      "Score for fold 9: loss of 0.3292810320854187; accuracy of 87.22222447395325%\n",
      "720/720 [==============================] - 6s 8ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ind run 1 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 54s 8ms/step - loss: 0.5890 - acc: 0.7668 - val_loss: 0.3473 - val_acc: 0.8736\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6480/6480 [==============================] - 58s 9ms/step - loss: 0.2742 - acc: 0.9023 - val_loss: 0.2973 - val_acc: 0.8931\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 58s 9ms/step - loss: 0.2113 - acc: 0.9236 - val_loss: 0.2839 - val_acc: 0.8903\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 59s 9ms/step - loss: 0.1718 - acc: 0.9378 - val_loss: 0.2917 - val_acc: 0.8889\n",
      "720/720 [==============================] - 4s 5ms/step\n",
      "Score for fold 10: loss of 0.29174252682262; accuracy of 88.88888955116272%\n",
      "720/720 [==============================] - 5s 7ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3172317736678653 - Accuracy: 86.66666746139526%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3329445719718933 - Accuracy: 87.08333373069763%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.28329672945870293 - Accuracy: 88.19444179534912%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.29949496222866906 - Accuracy: 88.33333253860474%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.30106100142002107 - Accuracy: 89.86111283302307%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.28357724646727245 - Accuracy: 90.55555462837219%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.28964903785122764 - Accuracy: 89.16666507720947%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.2824415922164917 - Accuracy: 89.72222208976746%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.3292810320854187 - Accuracy: 87.22222447395325%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.29174252682262 - Accuracy: 88.88888955116272%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 88.56944441795349 (+- 1.2345492139745045)\n",
      "> Loss: 0.3010720474190182\n",
      "> Precision: 0.8867810632347084\n",
      "> Recall: 0.8856944444444445\n",
      "> F1: 0.8857940061586198\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 55s 8ms/step - loss: 0.5725 - acc: 0.7818 - val_loss: 0.3542 - val_acc: 0.8653\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 52s 8ms/step - loss: 0.2701 - acc: 0.9003 - val_loss: 0.2991 - val_acc: 0.8861\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 60s 9ms/step - loss: 0.2123 - acc: 0.9210 - val_loss: 0.3071 - val_acc: 0.8778\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 57s 9ms/step - loss: 0.1655 - acc: 0.9383 - val_loss: 0.2986 - val_acc: 0.8903\n",
      "720/720 [==============================] - 4s 6ms/step\n",
      "Score for fold 1: loss of 0.2986337337228987; accuracy of 89.02778029441833%\n",
      "720/720 [==============================] - 6s 8ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 63s 10ms/step - loss: 0.5908 - acc: 0.7733 - val_loss: 0.3454 - val_acc: 0.8736\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 59s 9ms/step - loss: 0.2800 - acc: 0.8958 - val_loss: 0.3096 - val_acc: 0.8833\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 63s 10ms/step - loss: 0.2087 - acc: 0.9269 - val_loss: 0.3060 - val_acc: 0.8833\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 55s 8ms/step - loss: 0.1712 - acc: 0.9364 - val_loss: 0.3131 - val_acc: 0.8875\n",
      "720/720 [==============================] - 3s 4ms/step\n",
      "Score for fold 2: loss of 0.3130675087372462; accuracy of 88.7499988079071%\n",
      "720/720 [==============================] - 3s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 40s 6ms/step - loss: 0.6099 - acc: 0.7540 - val_loss: 0.3201 - val_acc: 0.8806\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 39s 6ms/step - loss: 0.2706 - acc: 0.9039 - val_loss: 0.2810 - val_acc: 0.8833\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 38s 6ms/step - loss: 0.2035 - acc: 0.9285 - val_loss: 0.2626 - val_acc: 0.8944\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 39s 6ms/step - loss: 0.1630 - acc: 0.9423 - val_loss: 0.2769 - val_acc: 0.9028\n",
      "720/720 [==============================] - 3s 4ms/step\n",
      "Score for fold 3: loss of 0.27691468257043095; accuracy of 90.27777910232544%\n",
      "720/720 [==============================] - 4s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 66s 10ms/step - loss: 0.5664 - acc: 0.7832 - val_loss: 0.3352 - val_acc: 0.8708\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 79s 12ms/step - loss: 0.2619 - acc: 0.9088 - val_loss: 0.2900 - val_acc: 0.8889\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 80s 12ms/step - loss: 0.1929 - acc: 0.9310 - val_loss: 0.2929 - val_acc: 0.8903\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 80s 12ms/step - loss: 0.1619 - acc: 0.9429 - val_loss: 0.3051 - val_acc: 0.8903\n",
      "720/720 [==============================] - 6s 8ms/step\n",
      "Score for fold 4: loss of 0.3050790203942193; accuracy of 89.02778029441833%\n",
      "720/720 [==============================] - 7s 10ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 81s 13ms/step - loss: 0.5707 - acc: 0.7849 - val_loss: 0.3353 - val_acc: 0.8694\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 75s 12ms/step - loss: 0.2739 - acc: 0.8963 - val_loss: 0.3058 - val_acc: 0.8847\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 76s 12ms/step - loss: 0.2076 - acc: 0.9245 - val_loss: 0.3016 - val_acc: 0.8833\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 83s 13ms/step - loss: 0.1695 - acc: 0.9366 - val_loss: 0.2974 - val_acc: 0.8847\n",
      "720/720 [==============================] - 6s 9ms/step\n",
      "Score for fold 5: loss of 0.2973797927300135; accuracy of 88.47222328186035%\n",
      "720/720 [==============================] - 8s 11ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 84s 13ms/step - loss: 0.5740 - acc: 0.7758 - val_loss: 0.3281 - val_acc: 0.8819\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 79s 12ms/step - loss: 0.2654 - acc: 0.9017 - val_loss: 0.3000 - val_acc: 0.8958\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 80s 12ms/step - loss: 0.2009 - acc: 0.9210 - val_loss: 0.2911 - val_acc: 0.8917\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 89s 14ms/step - loss: 0.1611 - acc: 0.9412 - val_loss: 0.3046 - val_acc: 0.8986\n",
      "720/720 [==============================] - 7s 9ms/step\n",
      "Score for fold 6: loss of 0.3045672317345937; accuracy of 89.86111283302307%\n",
      "720/720 [==============================] - 6s 8ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 81s 13ms/step - loss: 0.5768 - acc: 0.7856 - val_loss: 0.3309 - val_acc: 0.8681\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 70s 11ms/step - loss: 0.2709 - acc: 0.9002 - val_loss: 0.2837 - val_acc: 0.8889\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6480/6480 [==============================] - 79s 12ms/step - loss: 0.2037 - acc: 0.9259 - val_loss: 0.2861 - val_acc: 0.8861\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 75s 11ms/step - loss: 0.1700 - acc: 0.9367 - val_loss: 0.2895 - val_acc: 0.8931\n",
      "720/720 [==============================] - 5s 7ms/step\n",
      "Score for fold 7: loss of 0.2894795855714215; accuracy of 89.30555582046509%\n",
      "720/720 [==============================] - 7s 10ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 77s 12ms/step - loss: 0.5744 - acc: 0.7827 - val_loss: 0.3190 - val_acc: 0.8819\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 74s 11ms/step - loss: 0.2655 - acc: 0.9035 - val_loss: 0.2741 - val_acc: 0.9000\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 57s 9ms/step - loss: 0.2000 - acc: 0.9267 - val_loss: 0.2826 - val_acc: 0.8972\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 53s 8ms/step - loss: 0.1617 - acc: 0.9421 - val_loss: 0.2500 - val_acc: 0.9014\n",
      "720/720 [==============================] - 4s 6ms/step\n",
      "Score for fold 8: loss of 0.24996621674961514; accuracy of 90.13888835906982%\n",
      "720/720 [==============================] - 5s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 56s 9ms/step - loss: 0.6114 - acc: 0.7563 - val_loss: 0.3833 - val_acc: 0.8556\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 59s 9ms/step - loss: 0.2748 - acc: 0.8988 - val_loss: 0.3215 - val_acc: 0.8708\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 57s 9ms/step - loss: 0.2105 - acc: 0.9228 - val_loss: 0.3114 - val_acc: 0.8764\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 58s 9ms/step - loss: 0.1727 - acc: 0.9360 - val_loss: 0.3178 - val_acc: 0.8750\n",
      "720/720 [==============================] - 5s 7ms/step\n",
      "Score for fold 9: loss of 0.3178007576200697; accuracy of 87.5%\n",
      "720/720 [==============================] - 5s 7ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ind run 2 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 59s 9ms/step - loss: 0.5560 - acc: 0.7789 - val_loss: 0.3134 - val_acc: 0.8875\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 61s 9ms/step - loss: 0.2673 - acc: 0.9032 - val_loss: 0.2991 - val_acc: 0.8875\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 63s 10ms/step - loss: 0.2114 - acc: 0.9215 - val_loss: 0.2925 - val_acc: 0.9069\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 63s 10ms/step - loss: 0.1687 - acc: 0.9380 - val_loss: 0.2905 - val_acc: 0.8875\n",
      "720/720 [==============================] - 5s 7ms/step\n",
      "Score for fold 10: loss of 0.29054387774732376; accuracy of 88.7499988079071%\n",
      "720/720 [==============================] - 6s 8ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2986337337228987 - Accuracy: 89.02778029441833%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3130675087372462 - Accuracy: 88.7499988079071%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.27691468257043095 - Accuracy: 90.27777910232544%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3050790203942193 - Accuracy: 89.02778029441833%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2973797927300135 - Accuracy: 88.47222328186035%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.3045672317345937 - Accuracy: 89.86111283302307%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.2894795855714215 - Accuracy: 89.30555582046509%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.24996621674961514 - Accuracy: 90.13888835906982%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.3178007576200697 - Accuracy: 87.5%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.29054387774732376 - Accuracy: 88.7499988079071%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 89.11111176013947 (+- 0.7934923133081069)\n",
      "> Loss: 0.2943432407577833\n",
      "> Precision: 0.8920375003454837\n",
      "> Recall: 0.8911111111111112\n",
      "> F1: 0.8912208156578038\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 72s 11ms/step - loss: 0.6369 - acc: 0.7568 - val_loss: 0.3759 - val_acc: 0.8583\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 72s 11ms/step - loss: 0.2832 - acc: 0.8981 - val_loss: 0.3366 - val_acc: 0.8625\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 72s 11ms/step - loss: 0.2177 - acc: 0.9187 - val_loss: 0.3264 - val_acc: 0.8722\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 72s 11ms/step - loss: 0.1693 - acc: 0.9373 - val_loss: 0.3248 - val_acc: 0.8764\n",
      "720/720 [==============================] - 6s 9ms/step\n",
      "Score for fold 1: loss of 0.32476547559102376; accuracy of 87.63889074325562%\n",
      "720/720 [==============================] - 7s 9ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 71s 11ms/step - loss: 0.5974 - acc: 0.7602 - val_loss: 0.3486 - val_acc: 0.8708\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 72s 11ms/step - loss: 0.2718 - acc: 0.9003 - val_loss: 0.3110 - val_acc: 0.8833\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 71s 11ms/step - loss: 0.2030 - acc: 0.9248 - val_loss: 0.3241 - val_acc: 0.8875\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 71s 11ms/step - loss: 0.1706 - acc: 0.9375 - val_loss: 0.3223 - val_acc: 0.8861\n",
      "720/720 [==============================] - 6s 9ms/step\n",
      "Score for fold 2: loss of 0.32228915625148347; accuracy of 88.61111402511597%\n",
      "720/720 [==============================] - 6s 9ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 72s 11ms/step - loss: 0.6059 - acc: 0.7707 - val_loss: 0.3195 - val_acc: 0.8750\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 68s 11ms/step - loss: 0.2764 - acc: 0.8981 - val_loss: 0.2811 - val_acc: 0.8833\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 68s 10ms/step - loss: 0.2097 - acc: 0.9224 - val_loss: 0.2791 - val_acc: 0.8944\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 71s 11ms/step - loss: 0.1738 - acc: 0.9380 - val_loss: 0.2849 - val_acc: 0.8903\n",
      "720/720 [==============================] - 6s 8ms/step\n",
      "Score for fold 3: loss of 0.28487486177020604; accuracy of 89.02778029441833%\n",
      "720/720 [==============================] - 6s 9ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 72s 11ms/step - loss: 0.5523 - acc: 0.7872 - val_loss: 0.3031 - val_acc: 0.8931\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 71s 11ms/step - loss: 0.2651 - acc: 0.9043 - val_loss: 0.2561 - val_acc: 0.8986\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 70s 11ms/step - loss: 0.2029 - acc: 0.9261 - val_loss: 0.2680 - val_acc: 0.8944\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 69s 11ms/step - loss: 0.1696 - acc: 0.9381 - val_loss: 0.2565 - val_acc: 0.8972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 6s 8ms/step\n",
      "Score for fold 4: loss of 0.2564655757612652; accuracy of 89.72222208976746%\n",
      "720/720 [==============================] - 6s 9ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 70s 11ms/step - loss: 0.5893 - acc: 0.7679 - val_loss: 0.3390 - val_acc: 0.8750\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 71s 11ms/step - loss: 0.2749 - acc: 0.8991 - val_loss: 0.3057 - val_acc: 0.8722\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 73s 11ms/step - loss: 0.2103 - acc: 0.9225 - val_loss: 0.3076 - val_acc: 0.8889\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 74s 11ms/step - loss: 0.1683 - acc: 0.9358 - val_loss: 0.3180 - val_acc: 0.8778\n",
      "720/720 [==============================] - 6s 9ms/step\n",
      "Score for fold 5: loss of 0.3180227064424091; accuracy of 87.77777552604675%\n",
      "720/720 [==============================] - 7s 9ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 74s 11ms/step - loss: 0.5510 - acc: 0.7832 - val_loss: 0.3027 - val_acc: 0.8986\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 78s 12ms/step - loss: 0.2587 - acc: 0.9032 - val_loss: 0.2790 - val_acc: 0.8986\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 78s 12ms/step - loss: 0.2002 - acc: 0.9269 - val_loss: 0.3002 - val_acc: 0.8944\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 79s 12ms/step - loss: 0.1628 - acc: 0.9392 - val_loss: 0.2967 - val_acc: 0.8958\n",
      "720/720 [==============================] - 7s 10ms/step\n",
      "Score for fold 6: loss of 0.29672105610370636; accuracy of 89.58333134651184%\n",
      "720/720 [==============================] - 7s 10ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 86s 13ms/step - loss: 0.6351 - acc: 0.7417 - val_loss: 0.3345 - val_acc: 0.8681\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 84s 13ms/step - loss: 0.2815 - acc: 0.8983 - val_loss: 0.2970 - val_acc: 0.8833\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 83s 13ms/step - loss: 0.2155 - acc: 0.9193 - val_loss: 0.2862 - val_acc: 0.8903\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 84s 13ms/step - loss: 0.1707 - acc: 0.9403 - val_loss: 0.2937 - val_acc: 0.8931\n",
      "720/720 [==============================] - 8s 11ms/step\n",
      "Score for fold 7: loss of 0.29371324943171606; accuracy of 89.30555582046509%\n",
      "720/720 [==============================] - 8s 11ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 90s 14ms/step - loss: 0.6821 - acc: 0.7285 - val_loss: 0.3907 - val_acc: 0.8528\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 89s 14ms/step - loss: 0.3228 - acc: 0.8795 - val_loss: 0.3228 - val_acc: 0.8833\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 88s 14ms/step - loss: 0.2302 - acc: 0.9160 - val_loss: 0.3117 - val_acc: 0.8833\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 88s 14ms/step - loss: 0.1865 - acc: 0.9316 - val_loss: 0.2961 - val_acc: 0.8861\n",
      "720/720 [==============================] - 8s 11ms/step\n",
      "Score for fold 8: loss of 0.29606646961636013; accuracy of 88.61111402511597%\n",
      "720/720 [==============================] - 8s 12ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 88s 14ms/step - loss: 0.5506 - acc: 0.7852 - val_loss: 0.3771 - val_acc: 0.8542\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 89s 14ms/step - loss: 0.2700 - acc: 0.9006 - val_loss: 0.3513 - val_acc: 0.8583\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 88s 14ms/step - loss: 0.2042 - acc: 0.9276 - val_loss: 0.3117 - val_acc: 0.8694\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 89s 14ms/step - loss: 0.1666 - acc: 0.9429 - val_loss: 0.3337 - val_acc: 0.8764\n",
      "720/720 [==============================] - 8s 11ms/step\n",
      "Score for fold 9: loss of 0.33374518652757007; accuracy of 87.63889074325562%\n",
      "720/720 [==============================] - 8s 11ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ind run 3 ...\n",
      "Train on 6480 samples, validate on 720 samples\n",
      "Epoch 1/4\n",
      "6480/6480 [==============================] - 90s 14ms/step - loss: 0.6454 - acc: 0.7432 - val_loss: 0.3442 - val_acc: 0.8819\n",
      "Epoch 2/4\n",
      "6480/6480 [==============================] - 90s 14ms/step - loss: 0.2822 - acc: 0.8966 - val_loss: 0.2787 - val_acc: 0.9056\n",
      "Epoch 3/4\n",
      "6480/6480 [==============================] - 92s 14ms/step - loss: 0.2115 - acc: 0.9216 - val_loss: 0.2734 - val_acc: 0.8903\n",
      "Epoch 4/4\n",
      "6480/6480 [==============================] - 91s 14ms/step - loss: 0.1811 - acc: 0.9343 - val_loss: 0.2817 - val_acc: 0.8958\n",
      "720/720 [==============================] - 8s 11ms/step\n",
      "Score for fold 10: loss of 0.2816717541880078; accuracy of 89.58333134651184%\n",
      "720/720 [==============================] - 8s 11ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.32476547559102376 - Accuracy: 87.63889074325562%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.32228915625148347 - Accuracy: 88.61111402511597%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.28487486177020604 - Accuracy: 89.02778029441833%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2564655757612652 - Accuracy: 89.72222208976746%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3180227064424091 - Accuracy: 87.77777552604675%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.29672105610370636 - Accuracy: 89.58333134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.29371324943171606 - Accuracy: 89.30555582046509%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.29606646961636013 - Accuracy: 88.61111402511597%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.33374518652757007 - Accuracy: 87.63889074325562%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.2816717541880078 - Accuracy: 89.58333134651184%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 88.75000059604645 (+- 0.7856735222880594)\n",
      "> Loss: 0.3008335491683748\n",
      "> Precision: 0.8884872898208924\n",
      "> Recall: 0.8875\n",
      "> F1: 0.8875338856530535\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Score for k-fold runs\n",
      "------------------------------------------------------------------------\n",
      "> Run 1 Fold averages - Loss: 0.3010720474190182 - Accuracy: 88.56944441795349% \n",
      "> Run 1 Fold averages - Precision: 0.8867810632347084 - Recall: 0.8856944444444445 F1: 0.8857940061586198\n",
      "------------------------------------------------------------------------\n",
      "> Run 2 Fold averages - Loss: 0.2943432407577833 - Accuracy: 89.11111176013947% \n",
      "> Run 2 Fold averages - Precision: 0.8920375003454837 - Recall: 0.8911111111111112 F1: 0.8912208156578038\n",
      "------------------------------------------------------------------------\n",
      "> Run 3 Fold averages - Loss: 0.3008335491683748 - Accuracy: 88.75000059604645% \n",
      "> Run 3 Fold averages - Precision: 0.8884872898208924 - Recall: 0.8875 F1: 0.8875338856530535\n",
      "------------------------------------------------------------------------\n",
      "Overall average scores for all 3 runs:\n",
      "> Accuracy: 88.8101855913798 (+- 0.22519258816275628)\n",
      "> Loss: 0.2987496124483921\n",
      "> Precision: 0.889101951133695\n",
      "> Recall: 0.888101851851852\n",
      "> F1: 0.8881829024898257\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 2164)         21640000  \n",
      "_________________________________________________________________\n",
      "bidirectional_31 (Bidirectio (None, 100, 64)           562432    \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100, 16)           1040      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_31 (Glo (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 22,203,540\n",
      "Trainable params: 563,540\n",
      "Non-trainable params: 21,640,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for final model ...\n",
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/4\n",
      "7200/7200 [==============================] - 100s 14ms/step - loss: 0.7762 - acc: 0.6875 - val_loss: 0.3916 - val_acc: 0.8562\n",
      "Epoch 2/4\n",
      "7200/7200 [==============================] - 102s 14ms/step - loss: 0.3258 - acc: 0.8835 - val_loss: 0.3188 - val_acc: 0.8875\n",
      "Epoch 3/4\n",
      "7200/7200 [==============================] - 103s 14ms/step - loss: 0.2442 - acc: 0.9104 - val_loss: 0.3005 - val_acc: 0.9087\n",
      "Epoch 4/4\n",
      "7200/7200 [==============================] - 104s 14ms/step - loss: 0.1997 - acc: 0.9296 - val_loss: 0.3087 - val_acc: 0.8988\n",
      "Evaluate final model on test data\n",
      "800/800 [==============================] - 7s 9ms/step\n",
      "test loss, test acc: [0.308726555109024, 0.8987500071525574]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run x Times the folds\n",
    "if not run_final_train_only:\n",
    "    for run_num in range(1,fold_runs+1):\n",
    "        # k-fold\n",
    "        for train_ind, val_ind in skfold.split(x_train,y_train):\n",
    "\n",
    "            # Create model\n",
    "            model = create_model()\n",
    "\n",
    "            # Load GloVe embedding\n",
    "            model.layers[0].set_weights([word_embedding_matrix])\n",
    "            model.layers[0].trainable = False\n",
    "\n",
    "            # Train and Evaluate\n",
    "            model.compile(optimizer=optimizer,\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['acc'])\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} ind run {run_num} ...')\n",
    "\n",
    "            history = model.fit(x_train[train_ind], y_train[train_ind],\n",
    "                                epochs=epochs,\n",
    "                                batch_size=64,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_train[val_ind], y_train[val_ind]))\n",
    "\n",
    "            # metrics\n",
    "            scores = model.evaluate(x_train[val_ind], y_train[val_ind], batch_size=32)\n",
    "            #print(f'Score for fold {fold_no}: {model.metrics_name[0]} of {scores[0]}; {model.metrics_name[1]} of {scores[1]*100}%')\n",
    "            print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
    "            acc_per_fold.append(scores[1]*100)\n",
    "            loss_per_fold.append(scores[0])\n",
    "\n",
    "            # Evaluation metrics precison recall f1\n",
    "            y_pred = model.predict(x_train[val_ind], batch_size=64, verbose=1)\n",
    "            y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(y_train[val_ind], y_pred_bool)\n",
    "            mean_precision = np.mean(precision)\n",
    "            mean_recall = np.mean(recall)\n",
    "            mean_f1 = np.mean(f1)\n",
    "            precision_per_fold.append(mean_precision)\n",
    "            recall_per_fold.append(mean_recall)\n",
    "            f1_per_fold.append(mean_f1)\n",
    "\n",
    "            fold_no += 1\n",
    "\n",
    "        # == Provide average scores ==\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Score per fold')\n",
    "        for i in range(0, len(acc_per_fold)):\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Average scores for all folds:')\n",
    "        avg_acc_per_run.append(np.mean(acc_per_fold))\n",
    "        avg_loss_per_run.append(np.mean(loss_per_fold))\n",
    "        avg_precision_per_run.append(np.mean(precision_per_fold))\n",
    "        avg_recall_per_run.append(np.mean(recall_per_fold))\n",
    "        avg_f1_per_run.append(np.mean(f1_per_fold))\n",
    "\n",
    "        print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "        print(f'> Precision: {np.mean(precision_per_fold)}')\n",
    "        print(f'> Recall: {np.mean(recall_per_fold)}')\n",
    "        print(f'> F1: {np.mean(f1_per_fold)}')\n",
    "        print('------------------------------------------------------------------------')\n",
    "\n",
    "        # reset fold vars\n",
    "        acc_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        precision_per_fold = []\n",
    "        recall_per_fold = []\n",
    "        f1_per_fold = []\n",
    "        fold_no = 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score for k-fold runs')\n",
    "    for i in range(0, len(avg_acc_per_run)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Run {i+1} Fold averages - Loss: {avg_loss_per_run[i]} - Accuracy: {avg_acc_per_run[i]}% ')\n",
    "        print(f'> Run {i+1} Fold averages - Precision: {avg_precision_per_run[i]} - Recall: {avg_recall_per_run[i]} F1: {avg_f1_per_run[i]}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Overall average scores for all {fold_runs} runs:')\n",
    "    print(f'> Accuracy: {np.mean(avg_acc_per_run)} (+- {np.std(avg_acc_per_run)})')\n",
    "    print(f'> Loss: {np.mean(avg_loss_per_run)}')\n",
    "    print(f'> Precision: {np.mean(avg_precision_per_run)}')\n",
    "    print(f'> Recall: {np.mean(avg_recall_per_run)}')\n",
    "    print(f'> F1: {np.mean(avg_f1_per_run)}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "# create final model #Todo sync with fold rund\n",
    "if create_final_model:\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "\n",
    "    # Load GloVe embedding\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "\n",
    "    # Train and Evaluate\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Training for final model ...')\n",
    "\n",
    "    history = model.fit(x_train_copy, y_train_copy,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    # Save Model\n",
    "    if use_mg_train_corpora == MULTIGENRE:\n",
    "        model.save('models/model_emotion_detection_multigenre.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre.pkl\", \"wb\"))\n",
    "    elif use_mg_train_corpora == TWITTER:\n",
    "        model.save('models/model_emotion_detection_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_twitter.pkl\", \"wb\"))\n",
    "    else:\n",
    "        model.save('models/model_emotion_detection_multigenre_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre_twitter.pkl\", \"wb\"))\n",
    "\n",
    "    # Test final model\n",
    "    print(\"Evaluate final model on test data\")\n",
    "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    # For Model evaluation metrics run evalModel\n",
    "\n",
    "    # Plot performance\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
