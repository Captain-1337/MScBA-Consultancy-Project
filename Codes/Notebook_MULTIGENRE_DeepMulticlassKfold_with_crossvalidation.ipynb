{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, LSTM, GRU, AveragePooling1D, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "#import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from corpora_utils import CorporaHelper,CorporaDomains, CorporaProperties\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Activate GPU\n",
    "#WARNING GPU TAKES 5 TIMES LONGER THAN CPU! With Consul Project 1\n",
    "#Check for GPU\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n",
    "# GPU CONFIG\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\"\"\"\n",
    "\n",
    "#Deactivate GPU\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIGENRE = 'muligenre'\n",
    "TWITTER = 'twitter'\n",
    "MG_AND_TWITTER = 'mg_and_twitter'\n",
    "\n",
    "# set wich corpora to use Multigenre or twitter\n",
    "use_mg_train_corpora = MULTIGENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_labels = []\n",
    "train_texts = []\n",
    "test_labels = []\n",
    "test_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpora(filepath, sep=';'):\n",
    "    print('Load: ', filepath)\n",
    "    corpora_helper = CorporaHelper(filepath, separator=sep)\n",
    "    count_joy = 0\n",
    "    count_sadness = 0\n",
    "    count_anger = 0\n",
    "    count_fear = 0\n",
    "    labels = []\n",
    "    texts = []\n",
    "    # preprocessing corpora\n",
    "    corpora_helper.translate_urls()\n",
    "    corpora_helper.translate_emoticons()\n",
    "    corpora_helper.translate_emojis()\n",
    "    corpora_helper.translate_email()\n",
    "    #corpora_helper.translate_mention()\n",
    "    corpora_helper.translate_html_tags()\n",
    "    #corpora_helper.translate_camel_case()\n",
    "    corpora_helper.translate_underscore()\n",
    "\n",
    "    corpora_helper.translate_string('-LRB-','(')\n",
    "    corpora_helper.translate_string('-RRB-',')')\n",
    "    corpora_helper.translate_string('`',\"'\") # ` to '\n",
    "    corpora_helper.translate_string(\"''\",'\"') # double '' to \"\n",
    "    corpora_helper.translate_contractions()\n",
    "    corpora_helper.translate_string(\"'\",\"\") # remove '\n",
    "    corpora_helper.translate_string(\"\\\\n\",\" \") # replace new lines with space\n",
    "\n",
    "    #corpora_helper.spell_correction()\n",
    "    corpora_helper.add_space_at_special_chars()\n",
    "    corpora_helper.add_space_at_special_chars(regexlist = r\"([#])\")\n",
    "    #corpora_helper.translate_to_lower()\n",
    "\n",
    "    # 0 anger\n",
    "    # 1 fear\n",
    "    # 2 joy\n",
    "    # 3 sadness\n",
    "    for index, corpus in corpora_helper.get_data().iterrows():\n",
    "        if corpus[CorporaProperties.EMOTION.value] == 'anger':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(0)\n",
    "            count_anger += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'fear':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(1)\n",
    "            count_fear += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'joy':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(2)\n",
    "            count_joy += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'sadness':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(3)\n",
    "            count_sadness += 1\n",
    "    print('number of anger labels: ',count_anger)\n",
    "    print('number of fear labels: ', count_fear)\n",
    "    print('number of joy labels: ', count_joy)\n",
    "    print('number of sadness labels: ', count_sadness)\n",
    "    print('----------------------------------------------------------------------')\n",
    "    return texts, labels\n",
    "    #max_data = count_anger + count_fear + count_joy + count_sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use MULTIGENRE train corpora\n",
      "Load:  corpora/multigenre_450_train.csv\n",
      "number of anger labels:  405\n",
      "number of fear labels:  405\n",
      "number of joy labels:  405\n",
      "number of sadness labels:  405\n",
      "----------------------------------------------------------------------\n",
      "Load:  corpora/multigenre_450_test.csv\n",
      "number of anger labels:  45\n",
      "number of fear labels:  45\n",
      "number of joy labels:  45\n",
      "number of sadness labels:  45\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_file = \"\"\n",
    "test_file = \"\"\n",
    "sep = ';'\n",
    "word_embeddings_path = ''\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    train_file = \"corpora/multigenre_450_train.csv\"\n",
    "    test_file = \"corpora/multigenre_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = ';'\n",
    "    print(\"Use MULTIGENRE train corpora\")\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    train_file = \"corpora/twitter_2000_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER train corpora\")\n",
    "else:\n",
    "    train_file = \"corpora/twitter_2000_mg_450_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_mg_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER and MULTIGENRE train corpora\")\n",
    "    \n",
    "train_texts, train_labels = load_corpora(train_file, sep=sep)\n",
    "test_texts, test_labels = load_corpora(test_file, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared custom ensemble embedding\n",
    "with open(word_embeddings_path, 'rb') as word_embeddings_file:\n",
    "    embedding_info = pickle.load(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding helper functions\n",
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "    \n",
    "def get_unigram_embedding(word, word_embedding_dict, bin_string):\n",
    "    \n",
    "    if word in word_embedding_dict:\n",
    "        word_feature_embedding_dict = word_embedding_dict[word]\n",
    "        final_embedding = np.array([])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for i in range(16):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen:  100\n"
     ]
    }
   ],
   "source": [
    "pre_padding = 0\n",
    "embeddings_index = embedding_info[0]\n",
    "MAX_SEQUENCE_LENGTH = embedding_info[1]\n",
    "maxlen = MAX_SEQUENCE_LENGTH\n",
    "print(\"maxlen: \",maxlen)\n",
    "#MAX_NB_WORDS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Unigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting relevant embeddings for multigenre\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    # Multigenre\n",
    "    unigram_feature_string = \"1001111111111101\"\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    # Twitter\n",
    "    unigram_feature_string = \"0110001111111101\"\n",
    "    unigram_feature_string = \"1111111111111111\"\n",
    "else:\n",
    "    # Twitter and Multigenre\n",
    "    unigram_feature_string = \"1110010000000000\"\n",
    "# 1 Google news pretrained vectors : GoogleNews-vectors-negative300.bin.gz  \n",
    "# 2 Twitter pretrained vectors: word2vec_twitter_model.bin\n",
    "# 3 glove.twitter.27B.200d.txt\n",
    "# 4 glove.6B.300d.txt\n",
    "# 5 glove.42B.300d.txt\n",
    "# 6 glove.840B.300d.txt\n",
    "# 7 NRC Emotion Intensity Lexicon\n",
    "# 8 senti word net\n",
    "#9  NRC Sentiment lexicon: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#10 lexicons/Emoticon-unigrams.txt\n",
    "#11 lexicons/Emoticon-AFFLEX-NEGLEX-unigrams.txt\n",
    "#12 NRC Hashtag Lexica: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#13 HS-unigrams.txtNRC-Hashtag-Emotion-Lexicon-v0.2.txt\n",
    "#14 HS-AFFLEX-NEGLEX-unigrams.txt\n",
    "#15 Emoji Polarities\n",
    "#16 Depeche mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep learning with multigenre and twitter corpus and 4 emotions\n",
    "\"\"\"\n",
    "# K-Fold variables\n",
    "num_folds = 10 # 10\n",
    "fold_runs = 3 # 3\n",
    "fold_no = 1\n",
    "# train\n",
    "epochs = 5\n",
    "max_words = 10000\n",
    "# max. different words:\n",
    "# Multigerne: 5140  => 10000 or 3000 or 1000 ?\n",
    "# Twitter: 17580 => 20000 or 10000 ?\n",
    "# MG and Twitter: 20073 => evtl. 20000?\n",
    "#optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer = Adam(learning_rate=0.001) # default 0.001\n",
    "skfold = StratifiedKFold(n_splits = num_folds, random_state = 7, shuffle = True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_per_fold = []\n",
    "avg_acc_per_run = []\n",
    "avg_loss_per_run = []\n",
    "avg_precision_per_run = []\n",
    "avg_recall_per_run = []\n",
    "avg_f1_per_run = []\n",
    "create_final_model = True\n",
    "# run only final model without kfold\n",
    "run_final_train_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1560\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = len(get_unigram_embedding(\"glad\", embedding_info[0], unigram_feature_string))\n",
    "print(\"Embedding dimension:\",EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, filters = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train an test data set\n",
    "def create_data(texts, labels, maxlen):\n",
    "    ## Create one hot encoding\n",
    "    #max_words = 10000\n",
    "    #maxlen = 100 # max. number of words in sequences\n",
    "    #tokenizer = Tokenizer(num_words=max_words, filters = '')\n",
    "    #tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    #word_i = tokenizer.word_index\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    labels_arr = np.asarray(labels)\n",
    "    print('Shape of data:', data.shape)\n",
    "    print('Shape of labels:', labels_arr.shape)\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "    # mix the data\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels_arr = labels_arr[indices]\n",
    "\n",
    "    # split in train and validate\n",
    "    x_data = data\n",
    "    y_data = labels_arr\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer\n",
    "all_texts = train_texts.copy()\n",
    "all_texts.append(test_texts.copy())\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (1620, 100)\n",
      "Shape of labels: (1620,)\n",
      "-------------------------------------------\n",
      "Shape of data: (180, 100)\n",
      "Shape of labels: (180,)\n",
      "-------------------------------------------\n",
      "5140 unique Tokens found.\n"
     ]
    }
   ],
   "source": [
    "# Train an word index for embedding enrichment\n",
    "x_train, y_train = create_data(train_texts, train_labels, maxlen)\n",
    "x_test, y_test = create_data(test_texts, test_labels, maxlen)\n",
    "word_index = tokenizer.word_index\n",
    "x_train_copy = x_train.copy()\n",
    "y_train_copy = y_train.copy()\n",
    "print ('%s unique Tokens found.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Matrix\n",
    "word_embedding_matrix = list()\n",
    "word_embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "#word_embedding_matrix.append(np.zeros(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items(): # sorted(word_indices, key=word_indices.get):\n",
    "    embedding_features = get_unigram_embedding(word, embedding_info[0], unigram_feature_string)\n",
    "    if i < max_words:\n",
    "        if embedding_features is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            word_embedding_matrix[i] = embedding_features\n",
    "\n",
    "word_embedding_matrix = np.asarray(word_embedding_matrix, dtype='f')\n",
    "word_embedding_matrix = scale(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM 1560\n",
      "input_length 100\n"
     ]
    }
   ],
   "source": [
    "#print('word_indices_len',word_indices_len)\n",
    "print('EMBEDDING_DIM',EMBEDDING_DIM)\n",
    "print('input_length', MAX_SEQUENCE_LENGTH + pre_padding)\n",
    "embedding = Embedding(max_words, EMBEDDING_DIM, input_length=maxlen, trainable=False)\n",
    "#embedding = Embedding(word_indices_len + 1, EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH + pre_padding, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running \"optimal\" BiLSTM Model with 3 Runs and 10 k-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "        \n",
    "    model.add(Bidirectional(LSTM(32, dropout=0.4, recurrent_dropout=0.4, return_sequences=True)))\n",
    "    \n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"Adam\", learningrate = 0.0001)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 1.3363 - acc: 0.3532 - val_loss: 1.1395 - val_acc: 0.5247\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 1.0293 - acc: 0.5953 - val_loss: 0.9343 - val_acc: 0.6111\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.7684 - acc: 0.7298 - val_loss: 0.8561 - val_acc: 0.6852\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.5976 - acc: 0.7915 - val_loss: 0.8179 - val_acc: 0.7099\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.4811 - acc: 0.8443 - val_loss: 0.7848 - val_acc: 0.7099\n",
      "162/162 [==============================] - 0s 2ms/step\n",
      "Score for fold 1: loss of 0.7848310514732644; accuracy of 70.9876537322998%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 1.3659 - acc: 0.3779 - val_loss: 1.0081 - val_acc: 0.6296\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.9082 - acc: 0.6440 - val_loss: 0.7647 - val_acc: 0.7160\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.6516 - acc: 0.7778 - val_loss: 0.6875 - val_acc: 0.7654\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.5086 - acc: 0.8299 - val_loss: 0.6997 - val_acc: 0.7654\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.3965 - acc: 0.8827 - val_loss: 0.6548 - val_acc: 0.7840\n",
      "162/162 [==============================] - 0s 2ms/step\n",
      "Score for fold 2: loss of 0.6547968937199057; accuracy of 78.39506268501282%\n",
      "162/162 [==============================] - 1s 3ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 1.3868 - acc: 0.3663 - val_loss: 1.0874 - val_acc: 0.5988\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.9043 - acc: 0.7037 - val_loss: 0.7710 - val_acc: 0.7037\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.6313 - acc: 0.7963 - val_loss: 0.6535 - val_acc: 0.7531\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.4572 - acc: 0.8532 - val_loss: 0.6168 - val_acc: 0.7593\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.3459 - acc: 0.9047 - val_loss: 0.5870 - val_acc: 0.7901\n",
      "162/162 [==============================] - 0s 2ms/step\n",
      "Score for fold 3: loss of 0.5870085962024736; accuracy of 79.0123462677002%\n",
      "162/162 [==============================] - 0s 3ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 1.2469 - acc: 0.4225 - val_loss: 1.1429 - val_acc: 0.5123\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.8070 - acc: 0.6948 - val_loss: 0.8959 - val_acc: 0.6358\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.5413 - acc: 0.8134 - val_loss: 0.8505 - val_acc: 0.6852\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.3997 - acc: 0.8827 - val_loss: 0.7923 - val_acc: 0.7407\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.2869 - acc: 0.9287 - val_loss: 0.7628 - val_acc: 0.7346\n",
      "162/162 [==============================] - 0s 2ms/step\n",
      "Score for fold 4: loss of 0.7628160104339505; accuracy of 73.45678806304932%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 1.2309 - acc: 0.4465 - val_loss: 0.9381 - val_acc: 0.5988\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.7355 - acc: 0.7421 - val_loss: 0.7036 - val_acc: 0.7346\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.4920 - acc: 0.8313 - val_loss: 0.6224 - val_acc: 0.7716\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.3420 - acc: 0.9060 - val_loss: 0.6280 - val_acc: 0.7654\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.2601 - acc: 0.9335 - val_loss: 0.5963 - val_acc: 0.7963\n",
      "162/162 [==============================] - 0s 2ms/step\n",
      "Score for fold 5: loss of 0.5963208793122091; accuracy of 79.62962985038757%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 1.1922 - acc: 0.4712 - val_loss: 0.8112 - val_acc: 0.6975\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.7370 - acc: 0.7257 - val_loss: 0.6286 - val_acc: 0.7469\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.5111 - acc: 0.8278 - val_loss: 0.5327 - val_acc: 0.7901\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.3740 - acc: 0.8759 - val_loss: 0.4643 - val_acc: 0.8272\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.2718 - acc: 0.9335 - val_loss: 0.4192 - val_acc: 0.8395\n",
      "162/162 [==============================] - 1s 3ms/step\n",
      "Score for fold 6: loss of 0.4191614643291191; accuracy of 83.95061492919922%\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 1.1459 - acc: 0.4829 - val_loss: 0.8883 - val_acc: 0.6543\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.6951 - acc: 0.7291 - val_loss: 0.6897 - val_acc: 0.7531\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.4835 - acc: 0.8313 - val_loss: 0.6062 - val_acc: 0.7963\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.3684 - acc: 0.8909 - val_loss: 0.5918 - val_acc: 0.8272\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.2667 - acc: 0.9376 - val_loss: 0.5391 - val_acc: 0.8272\n",
      "162/162 [==============================] - 1s 3ms/step\n",
      "Score for fold 7: loss of 0.5391405463954548; accuracy of 82.71604776382446%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 1.1831 - acc: 0.4739 - val_loss: 0.8709 - val_acc: 0.6420\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.7488 - acc: 0.7030 - val_loss: 0.5943 - val_acc: 0.7593\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 5s 3ms/step - loss: 0.5063 - acc: 0.8395 - val_loss: 0.4998 - val_acc: 0.8148\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 4s 3ms/step - loss: 0.3643 - acc: 0.8868 - val_loss: 0.4412 - val_acc: 0.8395\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 5s 3ms/step - loss: 0.2708 - acc: 0.9280 - val_loss: 0.3968 - val_acc: 0.8704\n",
      "162/162 [==============================] - 0s 2ms/step\n",
      "Score for fold 8: loss of 0.39678349833429594; accuracy of 87.03703880310059%\n",
      "162/162 [==============================] - 0s 2ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1458/1458 [==============================] - 11s 8ms/step - loss: 1.1809 - acc: 0.4671 - val_loss: 0.8975 - val_acc: 0.6111\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.6489 - acc: 0.7689 - val_loss: 0.7144 - val_acc: 0.7407\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.4273 - acc: 0.8621 - val_loss: 0.7132 - val_acc: 0.7531\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.2916 - acc: 0.9019 - val_loss: 0.7295 - val_acc: 0.7469\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.2039 - acc: 0.9472 - val_loss: 0.7512 - val_acc: 0.7284\n",
      "162/162 [==============================] - 0s 3ms/step\n",
      "Score for fold 9: loss of 0.7511844134625093; accuracy of 72.83950448036194%\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ind run 1 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1347 - acc: 0.5000 - val_loss: 0.8863 - val_acc: 0.6049\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.6847 - acc: 0.7380 - val_loss: 0.6937 - val_acc: 0.7099\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.4698 - acc: 0.8347 - val_loss: 0.6669 - val_acc: 0.7160\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.3352 - acc: 0.9012 - val_loss: 0.5827 - val_acc: 0.7716\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 7s 4ms/step - loss: 0.2340 - acc: 0.9348 - val_loss: 0.5453 - val_acc: 0.7963\n",
      "162/162 [==============================] - 0s 3ms/step\n",
      "Score for fold 10: loss of 0.5453270699966837; accuracy of 79.62962985038757%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7848310514732644 - Accuracy: 70.9876537322998%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6547968937199057 - Accuracy: 78.39506268501282%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5870085962024736 - Accuracy: 79.0123462677002%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7628160104339505 - Accuracy: 73.45678806304932%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5963208793122091 - Accuracy: 79.62962985038757%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.4191614643291191 - Accuracy: 83.95061492919922%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.5391405463954548 - Accuracy: 82.71604776382446%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.39678349833429594 - Accuracy: 87.03703880310059%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.7511844134625093 - Accuracy: 72.83950448036194%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.5453270699966837 - Accuracy: 79.62962985038757%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 78.76543164253235 (+- 4.862066784412565)\n",
      "> Loss: 0.6037370423659866\n",
      "> Precision: 0.7868679909048522\n",
      "> Recall: 0.7877743902439024\n",
      "> F1: 0.7849906021576307\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 1.1305 - acc: 0.5014 - val_loss: 0.9212 - val_acc: 0.6543\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.6957 - acc: 0.7250 - val_loss: 0.8118 - val_acc: 0.6975\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.4836 - acc: 0.8381 - val_loss: 0.7486 - val_acc: 0.7284\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.3675 - acc: 0.8841 - val_loss: 0.7279 - val_acc: 0.7469\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.2654 - acc: 0.9348 - val_loss: 0.7021 - val_acc: 0.7654\n",
      "162/162 [==============================] - 1s 3ms/step\n",
      "Score for fold 1: loss of 0.7020951520513605; accuracy of 76.54321193695068%\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 1.1948 - acc: 0.4678 - val_loss: 0.8527 - val_acc: 0.6790\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 4ms/step - loss: 0.7299 - acc: 0.7202 - val_loss: 0.6428 - val_acc: 0.7654\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.5060 - acc: 0.8306 - val_loss: 0.6531 - val_acc: 0.7778\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.3735 - acc: 0.8834 - val_loss: 0.6625 - val_acc: 0.7840\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.2641 - acc: 0.9335 - val_loss: 0.6705 - val_acc: 0.7716\n",
      "162/162 [==============================] - 1s 3ms/step\n",
      "Score for fold 2: loss of 0.6704756108882987; accuracy of 77.16049551963806%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1072 - acc: 0.5034 - val_loss: 0.8367 - val_acc: 0.6481\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.6549 - acc: 0.7579 - val_loss: 0.6311 - val_acc: 0.7407\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.4304 - acc: 0.8532 - val_loss: 0.5548 - val_acc: 0.7963\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.2913 - acc: 0.9259 - val_loss: 0.5262 - val_acc: 0.8025\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.2147 - acc: 0.9472 - val_loss: 0.5285 - val_acc: 0.8333\n",
      "162/162 [==============================] - 1s 3ms/step\n",
      "Score for fold 3: loss of 0.5285177344893232; accuracy of 83.33333134651184%\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1549 - acc: 0.4849 - val_loss: 0.8824 - val_acc: 0.6358\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.6813 - acc: 0.7435 - val_loss: 0.7818 - val_acc: 0.7346\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.4700 - acc: 0.8484 - val_loss: 0.6672 - val_acc: 0.7840\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.3259 - acc: 0.9102 - val_loss: 0.6963 - val_acc: 0.7593\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.2321 - acc: 0.9410 - val_loss: 0.7157 - val_acc: 0.7469\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "Score for fold 4: loss of 0.7157331528487029; accuracy of 74.69135522842407%\n",
      "162/162 [==============================] - 1s 8ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 14s 9ms/step - loss: 1.1072 - acc: 0.5130 - val_loss: 0.8565 - val_acc: 0.6358\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.6221 - acc: 0.7641 - val_loss: 0.6985 - val_acc: 0.7346\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.4465 - acc: 0.8368 - val_loss: 0.6234 - val_acc: 0.7469\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.3091 - acc: 0.8999 - val_loss: 0.5239 - val_acc: 0.8457\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.2114 - acc: 0.9396 - val_loss: 0.5396 - val_acc: 0.8457\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "Score for fold 5: loss of 0.5395917191549584; accuracy of 84.5678985118866%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1759 - acc: 0.4671 - val_loss: 0.7968 - val_acc: 0.6728\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.7354 - acc: 0.7366 - val_loss: 0.6196 - val_acc: 0.7346\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.5238 - acc: 0.8272 - val_loss: 0.5174 - val_acc: 0.7901\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.3824 - acc: 0.8813 - val_loss: 0.4555 - val_acc: 0.8210\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.2880 - acc: 0.9204 - val_loss: 0.4078 - val_acc: 0.8272\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "Score for fold 6: loss of 0.4078115831922602; accuracy of 82.71604776382446%\n",
      "162/162 [==============================] - 1s 7ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1341 - acc: 0.5007 - val_loss: 0.8347 - val_acc: 0.7160\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.6975 - acc: 0.7442 - val_loss: 0.6874 - val_acc: 0.7469\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.4887 - acc: 0.8491 - val_loss: 0.6287 - val_acc: 0.7654\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.3602 - acc: 0.8868 - val_loss: 0.5901 - val_acc: 0.7778\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.2611 - acc: 0.9307 - val_loss: 0.5501 - val_acc: 0.7901\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "Score for fold 7: loss of 0.5500620934698317; accuracy of 79.0123462677002%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1393 - acc: 0.4952 - val_loss: 0.7733 - val_acc: 0.7160\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.6290 - acc: 0.7695 - val_loss: 0.5947 - val_acc: 0.7407\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.4481 - acc: 0.8422 - val_loss: 0.5095 - val_acc: 0.7901\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.3087 - acc: 0.9081 - val_loss: 0.4829 - val_acc: 0.7901\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.2263 - acc: 0.9348 - val_loss: 0.4507 - val_acc: 0.8210\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "Score for fold 8: loss of 0.4507318455495952; accuracy of 82.09876418113708%\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1240 - acc: 0.5000 - val_loss: 0.8968 - val_acc: 0.6358\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.6283 - acc: 0.7593 - val_loss: 0.7840 - val_acc: 0.7037\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.4374 - acc: 0.8532 - val_loss: 0.7922 - val_acc: 0.7037\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.2984 - acc: 0.9095 - val_loss: 0.7722 - val_acc: 0.7037\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.2193 - acc: 0.9451 - val_loss: 0.8051 - val_acc: 0.7346\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "Score for fold 9: loss of 0.8051216043071983; accuracy of 73.45678806304932%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ind run 2 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1012 - acc: 0.5165 - val_loss: 0.8288 - val_acc: 0.6667\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.6201 - acc: 0.7888 - val_loss: 0.6583 - val_acc: 0.7778\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.4371 - acc: 0.8587 - val_loss: 0.6088 - val_acc: 0.7531\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.3101 - acc: 0.9150 - val_loss: 0.5586 - val_acc: 0.8025\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.2339 - acc: 0.9362 - val_loss: 0.5368 - val_acc: 0.8025\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 10: loss of 0.53682830106513; accuracy of 80.24691343307495%\n",
      "162/162 [==============================] - 1s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7020951520513605 - Accuracy: 76.54321193695068%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6704756108882987 - Accuracy: 77.16049551963806%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5285177344893232 - Accuracy: 83.33333134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7157331528487029 - Accuracy: 74.69135522842407%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5395917191549584 - Accuracy: 84.5678985118866%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.4078115831922602 - Accuracy: 82.71604776382446%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.5500620934698317 - Accuracy: 79.0123462677002%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.4507318455495952 - Accuracy: 82.09876418113708%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.8051216043071983 - Accuracy: 73.45678806304932%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.53682830106513 - Accuracy: 80.24691343307495%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 79.38271522521973 (+- 3.633070926915789)\n",
      "> Loss: 0.5906968797016658\n",
      "> Precision: 0.796260329349357\n",
      "> Recall: 0.7937804878048781\n",
      "> F1: 0.7934116729486764\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1594 - acc: 0.4897 - val_loss: 0.9501 - val_acc: 0.6420\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.6953 - acc: 0.7359 - val_loss: 0.8644 - val_acc: 0.6728\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.4792 - acc: 0.8230 - val_loss: 0.7977 - val_acc: 0.6975\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.3388 - acc: 0.8985 - val_loss: 0.7584 - val_acc: 0.6790\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.2560 - acc: 0.9266 - val_loss: 0.7490 - val_acc: 0.7099\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 1: loss of 0.7490419535725205; accuracy of 70.9876537322998%\n",
      "162/162 [==============================] - 1s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 12s 8ms/step - loss: 1.1445 - acc: 0.4897 - val_loss: 0.8398 - val_acc: 0.6852\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.7091 - acc: 0.7325 - val_loss: 0.6890 - val_acc: 0.7346\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.4873 - acc: 0.8354 - val_loss: 0.6415 - val_acc: 0.7840\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.3342 - acc: 0.8985 - val_loss: 0.6349 - val_acc: 0.8086\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.2321 - acc: 0.9396 - val_loss: 0.6489 - val_acc: 0.7901\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "Score for fold 2: loss of 0.6489427406110881; accuracy of 79.0123462677002%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 1.1566 - acc: 0.4650 - val_loss: 0.8690 - val_acc: 0.6358\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 6s 4ms/step - loss: 0.7524 - acc: 0.7215 - val_loss: 0.7543 - val_acc: 0.7160\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.5238 - acc: 0.8258 - val_loss: 0.6708 - val_acc: 0.7222\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.3786 - acc: 0.8765 - val_loss: 0.6856 - val_acc: 0.6728\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.2711 - acc: 0.9232 - val_loss: 0.6899 - val_acc: 0.7407\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 3: loss of 0.6899258045502651; accuracy of 74.0740716457367%\n",
      "162/162 [==============================] - 1s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 12s 8ms/step - loss: 1.1031 - acc: 0.5014 - val_loss: 0.8877 - val_acc: 0.6296\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.6336 - acc: 0.7675 - val_loss: 0.7411 - val_acc: 0.6975\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.4355 - acc: 0.8553 - val_loss: 0.7233 - val_acc: 0.7284\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.2956 - acc: 0.9115 - val_loss: 0.6919 - val_acc: 0.7469\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.2137 - acc: 0.9438 - val_loss: 0.7079 - val_acc: 0.7531\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 4: loss of 0.7078607030856756; accuracy of 75.30864477157593%\n",
      "162/162 [==============================] - 1s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 1.1388 - acc: 0.4835 - val_loss: 0.7986 - val_acc: 0.7037\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.6691 - acc: 0.7476 - val_loss: 0.6112 - val_acc: 0.7654\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.4422 - acc: 0.8608 - val_loss: 0.5658 - val_acc: 0.7963\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.3201 - acc: 0.9115 - val_loss: 0.5214 - val_acc: 0.8642\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.2315 - acc: 0.9410 - val_loss: 0.5209 - val_acc: 0.8210\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 5: loss of 0.5209469252300484; accuracy of 82.09876418113708%\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 11s 7ms/step - loss: 1.1546 - acc: 0.4753 - val_loss: 0.8035 - val_acc: 0.7160\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 8s 5ms/step - loss: 0.6962 - acc: 0.7359 - val_loss: 0.5695 - val_acc: 0.7593\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.4689 - acc: 0.8361 - val_loss: 0.5439 - val_acc: 0.7531\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.3334 - acc: 0.8964 - val_loss: 0.5341 - val_acc: 0.7840\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.2414 - acc: 0.9328 - val_loss: 0.4896 - val_acc: 0.8333\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 6: loss of 0.489631806442767; accuracy of 83.33333134651184%\n",
      "162/162 [==============================] - 1s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 14s 9ms/step - loss: 1.0784 - acc: 0.5316 - val_loss: 0.7599 - val_acc: 0.6914\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.6196 - acc: 0.7524 - val_loss: 0.5916 - val_acc: 0.7901\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.4296 - acc: 0.8402 - val_loss: 0.4750 - val_acc: 0.8395\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 8s 6ms/step - loss: 0.2974 - acc: 0.9177 - val_loss: 0.4609 - val_acc: 0.8333\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.2236 - acc: 0.9438 - val_loss: 0.4639 - val_acc: 0.8210\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 7: loss of 0.4639151729183433; accuracy of 82.09876418113708%\n",
      "162/162 [==============================] - 1s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 12s 9ms/step - loss: 1.1654 - acc: 0.4801 - val_loss: 0.7037 - val_acc: 0.7593\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.6724 - acc: 0.7490 - val_loss: 0.5471 - val_acc: 0.8148\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 11s 7ms/step - loss: 0.4412 - acc: 0.8519 - val_loss: 0.4647 - val_acc: 0.8333\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.3097 - acc: 0.9150 - val_loss: 0.4277 - val_acc: 0.8333\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.2170 - acc: 0.9424 - val_loss: 0.4112 - val_acc: 0.8765\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "Score for fold 8: loss of 0.4111714245360575; accuracy of 87.65432238578796%\n",
      "162/162 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 1.0675 - acc: 0.5261 - val_loss: 0.8081 - val_acc: 0.6790\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.6304 - acc: 0.7798 - val_loss: 0.7537 - val_acc: 0.7284\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 7s 5ms/step - loss: 0.4180 - acc: 0.8587 - val_loss: 0.6961 - val_acc: 0.7531\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.3051 - acc: 0.9033 - val_loss: 0.6671 - val_acc: 0.7716\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.2264 - acc: 0.9369 - val_loss: 0.6811 - val_acc: 0.7778\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 9: loss of 0.6811100667641486; accuracy of 77.77777910232544%\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ind run 3 ...\n",
      "Train on 1458 samples, validate on 162 samples\n",
      "Epoch 1/5\n",
      "1458/1458 [==============================] - 12s 8ms/step - loss: 1.1816 - acc: 0.5082 - val_loss: 0.8493 - val_acc: 0.6790\n",
      "Epoch 2/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.6823 - acc: 0.7476 - val_loss: 0.6962 - val_acc: 0.7593\n",
      "Epoch 3/5\n",
      "1458/1458 [==============================] - 10s 7ms/step - loss: 0.4788 - acc: 0.8374 - val_loss: 0.5960 - val_acc: 0.7840\n",
      "Epoch 4/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.3484 - acc: 0.8903 - val_loss: 0.5485 - val_acc: 0.7716\n",
      "Epoch 5/5\n",
      "1458/1458 [==============================] - 9s 6ms/step - loss: 0.2565 - acc: 0.9280 - val_loss: 0.5228 - val_acc: 0.8025\n",
      "162/162 [==============================] - 1s 5ms/step\n",
      "Score for fold 10: loss of 0.5228007607345964; accuracy of 80.24691343307495%\n",
      "162/162 [==============================] - 1s 7ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7490419535725205 - Accuracy: 70.9876537322998%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6489427406110881 - Accuracy: 79.0123462677002%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6899258045502651 - Accuracy: 74.0740716457367%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7078607030856756 - Accuracy: 75.30864477157593%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5209469252300484 - Accuracy: 82.09876418113708%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.489631806442767 - Accuracy: 83.33333134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.4639151729183433 - Accuracy: 82.09876418113708%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.4111714245360575 - Accuracy: 87.65432238578796%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.6811100667641486 - Accuracy: 77.77777910232544%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.5228007607345964 - Accuracy: 80.24691343307495%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 79.2592591047287 (+- 4.662026580066456)\n",
      "> Loss: 0.588534735844551\n",
      "> Precision: 0.7909994733871197\n",
      "> Recall: 0.7926829268292683\n",
      "> F1: 0.7881804491222683\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Score for k-fold runs\n",
      "------------------------------------------------------------------------\n",
      "> Run 1 Fold averages - Loss: 0.6037370423659866 - Accuracy: 78.76543164253235% \n",
      "> Run 1 Fold averages - Precision: 0.7868679909048522 - Recall: 0.7877743902439024 F1: 0.7849906021576307\n",
      "------------------------------------------------------------------------\n",
      "> Run 2 Fold averages - Loss: 0.5906968797016658 - Accuracy: 79.38271522521973% \n",
      "> Run 2 Fold averages - Precision: 0.796260329349357 - Recall: 0.7937804878048781 F1: 0.7934116729486764\n",
      "------------------------------------------------------------------------\n",
      "> Run 3 Fold averages - Loss: 0.588534735844551 - Accuracy: 79.2592591047287% \n",
      "> Run 3 Fold averages - Precision: 0.7909994733871197 - Recall: 0.7926829268292683 F1: 0.7881804491222683\n",
      "------------------------------------------------------------------------\n",
      "Overall average scores for all 3 runs:\n",
      "> Accuracy: 79.13580199082692 (+- 0.266697081090836)\n",
      "> Loss: 0.5943228859707345\n",
      "> Precision: 0.7913759312137764\n",
      "> Recall: 0.7914126016260162\n",
      "> F1: 0.7888609080761918\n",
      "------------------------------------------------------------------------\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 1560)         15600000  \n",
      "_________________________________________________________________\n",
      "bidirectional_31 (Bidirectio (None, 100, 64)           407808    \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100, 16)           1040      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_31 (Glo (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 16,008,916\n",
      "Trainable params: 408,916\n",
      "Non-trainable params: 15,600,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for final model ...\n",
      "Train on 1620 samples, validate on 180 samples\n",
      "Epoch 1/5\n",
      "1620/1620 [==============================] - 16s 10ms/step - loss: 1.2994 - acc: 0.3846 - val_loss: 1.0666 - val_acc: 0.5278\n",
      "Epoch 2/5\n",
      "1620/1620 [==============================] - 13s 8ms/step - loss: 0.9440 - acc: 0.6123 - val_loss: 0.7547 - val_acc: 0.7000\n",
      "Epoch 3/5\n",
      "1620/1620 [==============================] - 13s 8ms/step - loss: 0.7219 - acc: 0.7309 - val_loss: 0.6902 - val_acc: 0.7000\n",
      "Epoch 4/5\n",
      "1620/1620 [==============================] - 13s 8ms/step - loss: 0.5638 - acc: 0.7981 - val_loss: 0.6616 - val_acc: 0.7500\n",
      "Epoch 5/5\n",
      "1620/1620 [==============================] - 13s 8ms/step - loss: 0.4660 - acc: 0.8444 - val_loss: 0.6164 - val_acc: 0.7389\n",
      "Evaluate final model on test data\n",
      "180/180 [==============================] - 1s 5ms/step\n",
      "test loss, test acc: [0.6164436843660143, 0.7388888597488403]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run x Times the folds\n",
    "if not run_final_train_only:\n",
    "    for run_num in range(1,fold_runs+1):\n",
    "        # k-fold\n",
    "        for train_ind, val_ind in skfold.split(x_train,y_train):\n",
    "\n",
    "            # Create model\n",
    "            model = create_model()\n",
    "\n",
    "            # Load GloVe embedding\n",
    "            model.layers[0].set_weights([word_embedding_matrix])\n",
    "            model.layers[0].trainable = False\n",
    "\n",
    "            # Train and Evaluate\n",
    "            model.compile(optimizer=optimizer,\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['acc'])\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} ind run {run_num} ...')\n",
    "\n",
    "            history = model.fit(x_train[train_ind], y_train[train_ind],\n",
    "                                epochs=epochs,\n",
    "                                batch_size=64,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_train[val_ind], y_train[val_ind]))\n",
    "\n",
    "            # metrics\n",
    "            scores = model.evaluate(x_train[val_ind], y_train[val_ind], batch_size=32)\n",
    "            #print(f'Score for fold {fold_no}: {model.metrics_name[0]} of {scores[0]}; {model.metrics_name[1]} of {scores[1]*100}%')\n",
    "            print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
    "            acc_per_fold.append(scores[1]*100)\n",
    "            loss_per_fold.append(scores[0])\n",
    "\n",
    "            # Evaluation metrics precison recall f1\n",
    "            y_pred = model.predict(x_train[val_ind], batch_size=64, verbose=1)\n",
    "            y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(y_train[val_ind], y_pred_bool)\n",
    "            mean_precision = np.mean(precision)\n",
    "            mean_recall = np.mean(recall)\n",
    "            mean_f1 = np.mean(f1)\n",
    "            precision_per_fold.append(mean_precision)\n",
    "            recall_per_fold.append(mean_recall)\n",
    "            f1_per_fold.append(mean_f1)\n",
    "\n",
    "            fold_no += 1\n",
    "\n",
    "        # == Provide average scores ==\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Score per fold')\n",
    "        for i in range(0, len(acc_per_fold)):\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Average scores for all folds:')\n",
    "        avg_acc_per_run.append(np.mean(acc_per_fold))\n",
    "        avg_loss_per_run.append(np.mean(loss_per_fold))\n",
    "        avg_precision_per_run.append(np.mean(precision_per_fold))\n",
    "        avg_recall_per_run.append(np.mean(recall_per_fold))\n",
    "        avg_f1_per_run.append(np.mean(f1_per_fold))\n",
    "\n",
    "        print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "        print(f'> Precision: {np.mean(precision_per_fold)}')\n",
    "        print(f'> Recall: {np.mean(recall_per_fold)}')\n",
    "        print(f'> F1: {np.mean(f1_per_fold)}')\n",
    "        print('------------------------------------------------------------------------')\n",
    "\n",
    "        # reset fold vars\n",
    "        acc_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        precision_per_fold = []\n",
    "        recall_per_fold = []\n",
    "        f1_per_fold = []\n",
    "        fold_no = 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score for k-fold runs')\n",
    "    for i in range(0, len(avg_acc_per_run)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Run {i+1} Fold averages - Loss: {avg_loss_per_run[i]} - Accuracy: {avg_acc_per_run[i]}% ')\n",
    "        print(f'> Run {i+1} Fold averages - Precision: {avg_precision_per_run[i]} - Recall: {avg_recall_per_run[i]} F1: {avg_f1_per_run[i]}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Overall average scores for all {fold_runs} runs:')\n",
    "    print(f'> Accuracy: {np.mean(avg_acc_per_run)} (+- {np.std(avg_acc_per_run)})')\n",
    "    print(f'> Loss: {np.mean(avg_loss_per_run)}')\n",
    "    print(f'> Precision: {np.mean(avg_precision_per_run)}')\n",
    "    print(f'> Recall: {np.mean(avg_recall_per_run)}')\n",
    "    print(f'> F1: {np.mean(avg_f1_per_run)}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "# create final model #Todo sync with fold rund\n",
    "if create_final_model:\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "\n",
    "    # Load GloVe embedding\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "\n",
    "    # Train and Evaluate\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Training for final model ...')\n",
    "\n",
    "    history = model.fit(x_train_copy, y_train_copy,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    # Save Model\n",
    "    if use_mg_train_corpora == MULTIGENRE:\n",
    "        model.save('models/model_emotion_detection_multigenre.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre.pkl\", \"wb\"))\n",
    "    elif use_mg_train_corpora == TWITTER:\n",
    "        model.save('models/model_emotion_detection_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_twitter.pkl\", \"wb\"))\n",
    "    else:\n",
    "        model.save('models/model_emotion_detection_multigenre_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre_twitter.pkl\", \"wb\"))\n",
    "\n",
    "    # Test final model\n",
    "    print(\"Evaluate final model on test data\")\n",
    "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    # For Model evaluation metrics run evalModel\n",
    "\n",
    "    # Plot performance\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
