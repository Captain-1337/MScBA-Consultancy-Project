{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from corpora_utils import CorporaHelper,CorporaDomains, CorporaProperties\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep learning with multigenre corpus LSTM two layers and 4 emotions\n",
    "\"\"\"\n",
    "# K-Fold variables\n",
    "num_folds = 3\n",
    "fold_runs = 2\n",
    "fold_no = 1\n",
    "\n",
    "MULTIGENRE = True\n",
    "TWITTER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wich corpora to use Multigenre or twitter\n",
    "use_mg_train_corpora = MULTIGENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "epochs = 3\n",
    "skfold = StratifiedKFold(n_splits = num_folds, random_state = 7, shuffle = True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "avg_acc_per_run = []\n",
    "avg_loss_per_run = []\n",
    "create_final_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_labels = []\n",
    "train_texts = []\n",
    "test_labels = []\n",
    "test_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpora(filepath, sep=';'):\n",
    "    print('Load: ', filepath)\n",
    "    corpora_helper = CorporaHelper(filepath, separator=sep)\n",
    "    count_joy = 0\n",
    "    count_sadness = 0\n",
    "    count_anger = 0\n",
    "    count_fear = 0\n",
    "    labels = []\n",
    "    texts = []\n",
    "    # preprocessing corpora\n",
    "    corpora_helper.translate_urls()\n",
    "    corpora_helper.translate_emoticons()\n",
    "    corpora_helper.translate_emojis()\n",
    "    corpora_helper.translate_email()\n",
    "    #corpora_helper.translate_mention()\n",
    "    corpora_helper.translate_html_tags()\n",
    "    #corpora_helper.translate_camel_case()\n",
    "    corpora_helper.translate_underscore()\n",
    "\n",
    "    corpora_helper.translate_string('-LRB-','(')\n",
    "    corpora_helper.translate_string('-RRB-',')')\n",
    "    corpora_helper.translate_string('`',\"'\") # ` to '\n",
    "    corpora_helper.translate_string(\"''\",'\"') # double '' to \"\n",
    "    #corpora_helper.translate_contractions()\n",
    "    corpora_helper.translate_string(\"'\",\"\") # remove '\n",
    "    corpora_helper.translate_string(\"\\\\n\",\" \") # replace new lines with space\n",
    "\n",
    "    #corpora_helper.spell_correction()\n",
    "    corpora_helper.add_space_at_special_chars()\n",
    "    corpora_helper.translate_to_lower()\n",
    "\n",
    "    # 0 anger\n",
    "    # 1 fear\n",
    "    # 2 joy\n",
    "    # 3 sadness\n",
    "    for index, corpus in corpora_helper.get_data().iterrows():\n",
    "        if corpus[CorporaProperties.EMOTION.value] == 'anger':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(0)\n",
    "            count_anger += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'fear':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(1)\n",
    "            count_fear += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'joy':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(2)\n",
    "            count_joy += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'sadness':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(3)\n",
    "            count_sadness += 1\n",
    "    print('number of anger labels: ',count_anger)\n",
    "    print('number of fear labels: ', count_fear)\n",
    "    print('number of joy labels: ',count_joy)\n",
    "    print('number of sadness labels: ', count_sadness)\n",
    "    print('----------------------------------------------------------------------')\n",
    "    return texts, labels\n",
    "    #max_data = count_anger + count_fear + count_joy + count_sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load:  corpora/multigenre_450_train.csv\n",
      "number of anger labels:  405\n",
      "number of fear labels:  405\n",
      "number of joy labels:  405\n",
      "number of sadness labels:  405\n",
      "----------------------------------------------------------------------\n",
      "Load:  corpora/multigenre_450_test.csv\n",
      "number of anger labels:  45\n",
      "number of fear labels:  45\n",
      "number of joy labels:  45\n",
      "number of sadness labels:  45\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_file = \"\"\n",
    "test_file = \"\"\n",
    "sep = ';'\n",
    "word_embeddings_path = ''\n",
    "if use_mg_train_corpora:\n",
    "    train_file = \"corpora/multigenre_450_train.csv\"\n",
    "    test_file = \"corpora/multigenre_450_test.csv\"\n",
    "    word_embeddings_path = 'costum_embeddings/multigenre_embedding.pkl'\n",
    "    sep = ';'\n",
    "else:\n",
    "    train_file = \"corpora/twitter_2000_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_test.csv\"\n",
    "    word_embeddings_path = 'costum_embeddings/twitter_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "\n",
    "train_texts, train_labels = load_corpora(train_file, sep=sep)\n",
    "test_texts, test_labels = load_corpora(test_file, sep=sep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared Multigenre ensemble embedding\n",
    "\n",
    "with open(word_embeddings_path, 'rb') as word_embeddings_file:\n",
    "    embedding_info = pickle.load(word_embeddings_file)\n",
    "max_words = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding helper functions\n",
    "def is_active_vector_method(string):\n",
    "    return int(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_embedding(word, word_embedding_dict, bin_string):\n",
    "    \n",
    "    if word in word_embedding_dict:\n",
    "        word_feature_embedding_dict = word_embedding_dict[word]\n",
    "        final_embedding = np.array([])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for i in range(16):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_padding = 0\n",
    "embeddings_index = embedding_info[0]\n",
    "MAX_SEQUENCE_LENGTH = embedding_info[1]\n",
    "maxlen = MAX_SEQUENCE_LENGTH\n",
    "#MAX_NB_WORDS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feature_string = \"1001111111100001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1485\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = len(get_unigram_embedding(\"glad\", embedding_info[0], unigram_feature_string))\n",
    "print(\"Embedding dimension:\",EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train an test data set\n",
    "def create_data(texts, labels, maxlen, max_words = 10000):\n",
    "    ## Create one hot encoding\n",
    "    #max_words = 10000\n",
    "    #maxlen = 100 # max. number of words in sequences\n",
    "    tokenizer = Tokenizer(num_words=max_words, filters = '')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_i = tokenizer.word_index\n",
    "    print ('%s eindeutige Tokens gefunden.' % len(word_i))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    labels = np.asarray(labels)\n",
    "    print('Shape of data:', data.shape)\n",
    "    print('Shape of labels:', labels.shape)\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "    # mix the data\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # split in train and validate\n",
    "    x_data = data\n",
    "    y_data = labels\n",
    "    return x_data, y_data, word_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950 eindeutige Tokens gefunden.\n",
      "Shape of data: (1620, 95)\n",
      "Shape of labels: (1620,)\n",
      "-------------------------------------------\n",
      "1145 eindeutige Tokens gefunden.\n",
      "Shape of data: (180, 95)\n",
      "Shape of labels: (180,)\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train an word index for embedding enrichment\n",
    "x_train, y_train, word_index = create_data(train_texts, train_labels, maxlen)\n",
    "x_test, y_test, text_word_index = create_data(test_texts, test_labels, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Matrix\n",
    "word_embedding_matrix = list()\n",
    "word_embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "#word_embedding_matrix.append(np.zeros(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items(): # sorted(word_indices, key=word_indices.get):\n",
    "    embedding_features = get_unigram_embedding(word, embedding_info[0], unigram_feature_string)\n",
    "    if i < max_words:\n",
    "        if embedding_features is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            word_embedding_matrix[i] = embedding_features\n",
    "\n",
    "word_embedding_matrix = np.asarray(word_embedding_matrix, dtype='f')\n",
    "word_embedding_matrix = scale(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM 1485\n",
      "input_length 95\n"
     ]
    }
   ],
   "source": [
    "#print('word_indices_len',word_indices_len)\n",
    "print('EMBEDDING_DIM',EMBEDDING_DIM)\n",
    "print('input_length', MAX_SEQUENCE_LENGTH + pre_padding)\n",
    "embedding = Embedding(max_words, EMBEDDING_DIM, input_length=maxlen, trainable=False)\n",
    "#embedding = Embedding(word_indices_len + 1, EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH + pre_padding, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Create model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(32,5, activation='relu'))\n",
    "    model.add(Flatten()) #3D to 2D\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    #model.summary()\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Conv1D(32,5, activation='relu'))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(32,dropout=0.4, recurrent_dropout=0.4,)))\n",
    "    #model.add(Dense(8, activation='relu'))\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 1 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 1.4229 - acc: 0.2759 - val_loss: 1.3131 - val_acc: 0.3889\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2868 - acc: 0.3954 - val_loss: 1.1899 - val_acc: 0.4611\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2208 - acc: 0.4509 - val_loss: 1.1193 - val_acc: 0.4889\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.1399 - acc: 0.5046 - val_loss: 1.0920 - val_acc: 0.5296\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.0576 - acc: 0.5454 - val_loss: 1.0363 - val_acc: 0.5648\n",
      "540/540 [==============================] - 0s 457us/step\n",
      "Score for fold 1: ... of 1.0363049568953338; ... of 56.48148059844971%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 1 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 1.3351 - acc: 0.3380 - val_loss: 1.2352 - val_acc: 0.4333\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2343 - acc: 0.4241 - val_loss: 1.1340 - val_acc: 0.4778\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.1691 - acc: 0.4648 - val_loss: 1.0927 - val_acc: 0.5241\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.0852 - acc: 0.5231 - val_loss: 1.0592 - val_acc: 0.5370\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.0153 - acc: 0.5639 - val_loss: 0.9999 - val_acc: 0.5593\n",
      "540/540 [==============================] - 0s 489us/step\n",
      "Score for fold 2: ... of 0.9999263772258052; ... of 55.925923585891724%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 1 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 1.4037 - acc: 0.3074 - val_loss: 1.2716 - val_acc: 0.4111\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.3044 - acc: 0.3963 - val_loss: 1.2133 - val_acc: 0.4333\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2137 - acc: 0.4574 - val_loss: 1.1416 - val_acc: 0.5056\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.1535 - acc: 0.4880 - val_loss: 1.1143 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.0825 - acc: 0.5361 - val_loss: 1.0416 - val_acc: 0.5278\n",
      "540/540 [==============================] - 0s 467us/step\n",
      "Score for fold 3: ... of 1.0416074655674121; ... of 52.77777910232544%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ind run 2 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 1.4184 - acc: 0.2824 - val_loss: 1.3038 - val_acc: 0.3519\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.3354 - acc: 0.3639 - val_loss: 1.2262 - val_acc: 0.4907\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2641 - acc: 0.4444 - val_loss: 1.2097 - val_acc: 0.4407\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2143 - acc: 0.4528 - val_loss: 1.1422 - val_acc: 0.5185\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.1509 - acc: 0.4981 - val_loss: 1.0771 - val_acc: 0.5333\n",
      "540/540 [==============================] - 0s 440us/step\n",
      "Score for fold 4: ... of 1.0771256817711725; ... of 53.33333611488342%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ind run 2 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 4s 4ms/step - loss: 1.3870 - acc: 0.3000 - val_loss: 1.2538 - val_acc: 0.4241\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2738 - acc: 0.4130 - val_loss: 1.1936 - val_acc: 0.4593\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.1788 - acc: 0.4704 - val_loss: 1.1085 - val_acc: 0.5037\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.1214 - acc: 0.5167 - val_loss: 1.1153 - val_acc: 0.5056\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.0528 - acc: 0.5463 - val_loss: 1.0149 - val_acc: 0.5593\n",
      "540/540 [==============================] - 0s 526us/step\n",
      "Score for fold 5: ... of 1.0149269783938373; ... of 55.925923585891724%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ind run 2 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/5\n",
      "1080/1080 [==============================] - 5s 4ms/step - loss: 1.3339 - acc: 0.3750 - val_loss: 1.2183 - val_acc: 0.4593\n",
      "Epoch 2/5\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 1.2026 - acc: 0.4685 - val_loss: 1.1377 - val_acc: 0.4981\n",
      "Epoch 3/5\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 1.1182 - acc: 0.5231 - val_loss: 1.0589 - val_acc: 0.5519\n",
      "Epoch 4/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.0004 - acc: 0.5815 - val_loss: 0.9988 - val_acc: 0.5796\n",
      "Epoch 5/5\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 0.9199 - acc: 0.6296 - val_loss: 0.9746 - val_acc: 0.5870\n",
      "540/540 [==============================] - 0s 483us/step\n",
      "Score for fold 6: ... of 0.9745594633950128; ... of 58.70370268821716%\n"
     ]
    }
   ],
   "source": [
    "# run x Times the folds\n",
    "for run_num in range(1,fold_runs+1):\n",
    "    # k-fold\n",
    "    for train_ind, val_ind in skfold.split(x_train,y_train):\n",
    "\n",
    "        # Create model\n",
    "        model = create_model()\n",
    "\n",
    "        # Load GloVe embedding\n",
    "        model.layers[0].set_weights([word_embedding_matrix])\n",
    "        model.layers[0].trainable = False\n",
    "\n",
    "        # Train and Evaluate\n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ind run {run_num} ...')\n",
    "\n",
    "        history = model.fit(x_train[train_ind], y_train[train_ind],\n",
    "                            #epochs=epochs,\n",
    "                            epochs=5,\n",
    "                            batch_size=32,\n",
    "                            verbose=1,\n",
    "                            validation_data=(x_train[val_ind], y_train[val_ind]))\n",
    "\n",
    "        # metrics\n",
    "        scores = model.evaluate(x_train[val_ind], y_train[val_ind], batch_size=128)\n",
    "        #print(f'Score for fold {fold_no}: {model.metrics_name[0]} of {scores[0]}; {model.metrics_name[1]} of {scores[1]*100}%')\n",
    "        print(f'Score for fold {fold_no}: ... of {scores[0]}; ... of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1]*100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "\n",
    "        fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.178701270951165 - Accuracy: 48.33333194255829%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.1146059901626022 - Accuracy: 53.14815044403076%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.0720727028670134 - Accuracy: 53.33333611488342%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.104002708858914 - Accuracy: 54.07407283782959%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.1047057814068264 - Accuracy: 50.18518567085266%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.159952214912132 - Accuracy: 48.148149251937866%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 51.203704377015434 (+- 2.421020870658538)\n",
      "> Loss: 1.1223401115264422\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, len(acc_per_fold)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Average scores for all folds:')\n",
    "    avg_acc_per_run.append(np.mean(acc_per_fold))\n",
    "    avg_loss_per_run.append(np.mean(loss_per_fold))\n",
    "    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "    # reset fold vars\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    fold_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Run 1 Fold averages - Loss: 1.1223401115264422 - Accuracy: 51.203704377015434%\n",
      "------------------------------------------------------------------------\n",
      "Overall average scores for all 2 runs:\n",
      "> Accuracy: 51.203704377015434 (+- 0.0)\n",
      "> Loss: 1.1223401115264422\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(avg_acc_per_run)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Run {i+1} Fold averages - Loss: {avg_loss_per_run[i]} - Accuracy: {avg_acc_per_run[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Overall average scores for all {fold_runs} runs:')\n",
    "\n",
    "print(f'> Accuracy: {np.mean(avg_acc_per_run)} (+- {np.std(avg_acc_per_run)})')\n",
    "print(f'> Loss: {np.mean(avg_loss_per_run)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 95, 1485)          14850000  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 91, 32)            237632    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 15,104,532\n",
      "Trainable params: 254,532\n",
      "Non-trainable params: 14,850,000\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for final model ...\n",
      "Epoch 1/3\n",
      "1080/1080 [==============================] - 4s 3ms/step - loss: 1.3855 - acc: 0.2889\n",
      "Epoch 2/3\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2682 - acc: 0.3907\n",
      "Epoch 3/3\n",
      "1080/1080 [==============================] - 3s 3ms/step - loss: 1.2057 - acc: 0.4602\n",
      "Evaluate Findal Model on test data\n",
      "180/180 [==============================] - 0s 1ms/step\n",
      "test loss, test acc: [1.4015240563286675, 0.31111112236976624]\n"
     ]
    }
   ],
   "source": [
    "# create final model #Todo sync with fold rund\n",
    "if create_final_model:\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "\n",
    "    # Load GloVe embedding\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "\n",
    "    # Train and Evaluate\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Training for final model ...')\n",
    "\n",
    "    history = model.fit(x_train[train_ind], y_train[train_ind],\n",
    "                        epochs=epochs,\n",
    "                        batch_size=32,\n",
    "                        verbose=1)\n",
    "    model.save('model_emotion_detection.h5')   \n",
    "\n",
    "    # Test final model\n",
    "    print(\"Evaluate Findal Model on test data\")\n",
    "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
