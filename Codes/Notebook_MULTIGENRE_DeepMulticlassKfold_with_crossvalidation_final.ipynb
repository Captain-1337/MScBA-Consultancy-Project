{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, LSTM, GRU, AveragePooling1D, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from corpora_utils import CorporaHelper, CorporaDomains, CorporaProperties\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Activate GPU\n",
    "#WARNING GPU TAKES 5 TIMES LONGER THAN CPU! With Consul Project 1\n",
    "#Check for GPU\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n",
    "# GPU CONFIG\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\"\"\"\n",
    "\n",
    "#Deactivate GPU\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIGENRE = 'muligenre'\n",
    "TWITTER = 'twitter'\n",
    "MG_AND_TWITTER = 'mg_and_twitter'\n",
    "\n",
    "# set wich corpora to use Multigenre or twitter\n",
    "use_mg_train_corpora = MULTIGENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_labels = []\n",
    "train_texts = []\n",
    "test_labels = []\n",
    "test_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpora(filepath, sep=';'):\n",
    "    print('Load: ', filepath)\n",
    "    corpora_helper = CorporaHelper(filepath, separator=sep)\n",
    "    count_joy = 0\n",
    "    count_sadness = 0\n",
    "    count_anger = 0\n",
    "    count_fear = 0\n",
    "    labels = []\n",
    "    texts = []\n",
    "    # preprocessing corpora\n",
    "    corpora_helper.translate_urls()\n",
    "    corpora_helper.translate_emoticons()\n",
    "    corpora_helper.translate_emojis()\n",
    "    corpora_helper.translate_email()\n",
    "    #corpora_helper.translate_mention()\n",
    "    corpora_helper.translate_html_tags()\n",
    "    #corpora_helper.translate_camel_case()\n",
    "    corpora_helper.translate_underscore()\n",
    "\n",
    "    corpora_helper.translate_string('-LRB-','(')\n",
    "    corpora_helper.translate_string('-RRB-',')')\n",
    "    corpora_helper.translate_string('`',\"'\") # ` to '\n",
    "    corpora_helper.translate_string(\"''\",'\"') # double '' to \"\n",
    "    corpora_helper.translate_contractions()\n",
    "    corpora_helper.translate_string(\"'\",\"\") # remove '\n",
    "    corpora_helper.translate_string(\"\\\\n\",\" \") # replace new lines with space\n",
    "\n",
    "    #corpora_helper.spell_correction()\n",
    "    corpora_helper.add_space_at_special_chars()\n",
    "    corpora_helper.add_space_at_special_chars(regexlist = r\"([#])\")\n",
    "    #corpora_helper.translate_to_lower()\n",
    "\n",
    "    # 0 anger\n",
    "    # 1 fear\n",
    "    # 2 joy\n",
    "    # 3 sadness\n",
    "    for index, corpus in corpora_helper.get_data().iterrows():\n",
    "        if corpus[CorporaProperties.EMOTION.value] == 'anger':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(0)\n",
    "            count_anger += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'fear':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(1)\n",
    "            count_fear += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'joy':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(2)\n",
    "            count_joy += 1\n",
    "        elif corpus[CorporaProperties.EMOTION.value] == 'sadness':\n",
    "            texts.append(corpus[CorporaProperties.CLEANED_CORPUS.value])\n",
    "            labels.append(3)\n",
    "            count_sadness += 1\n",
    "    print('number of anger labels: ',count_anger)\n",
    "    print('number of fear labels: ', count_fear)\n",
    "    print('number of joy labels: ', count_joy)\n",
    "    print('number of sadness labels: ', count_sadness)\n",
    "    print('----------------------------------------------------------------------')\n",
    "    return texts, labels\n",
    "    #max_data = count_anger + count_fear + count_joy + count_sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use MULTIGENRE train corpora\n",
      "Load:  corpora/multigenre_450_train.csv\n",
      "number of anger labels:  405\n",
      "number of fear labels:  405\n",
      "number of joy labels:  405\n",
      "number of sadness labels:  405\n",
      "----------------------------------------------------------------------\n",
      "Load:  corpora/multigenre_450_test.csv\n",
      "number of anger labels:  45\n",
      "number of fear labels:  45\n",
      "number of joy labels:  45\n",
      "number of sadness labels:  45\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_file = \"\"\n",
    "test_file = \"\"\n",
    "sep = ';'\n",
    "word_embeddings_path = ''\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    train_file = \"corpora/multigenre_450_train.csv\"\n",
    "    test_file = \"corpora/multigenre_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = ';'\n",
    "    print(\"Use MULTIGENRE train corpora\")\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    train_file = \"corpora/twitter_2000_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER train corpora\")\n",
    "else:\n",
    "    train_file = \"corpora/twitter_2000_mg_450_train.csv\"\n",
    "    test_file = \"corpora/twitter_2000_mg_450_test.csv\"\n",
    "    word_embeddings_path = 'custom_embedding/multi_embedding.pkl'\n",
    "    sep = '\\t'\n",
    "    print(\"Use TWITTER and MULTIGENRE train corpora\")\n",
    "    \n",
    "train_texts, train_labels = load_corpora(train_file, sep=sep)\n",
    "test_texts, test_labels = load_corpora(test_file, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared custom ensemble embedding\n",
    "with open(word_embeddings_path, 'rb') as word_embeddings_file:\n",
    "    embedding_info = pickle.load(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding helper functions\n",
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "    \n",
    "def get_unigram_embedding(word, word_embedding_dict, bin_string):\n",
    "    \n",
    "    if word in word_embedding_dict:\n",
    "        word_feature_embedding_dict = word_embedding_dict[word]\n",
    "        final_embedding = np.array([])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for i in range(16):\n",
    "        if is_active_vector_method(bin_string[i]):\n",
    "            final_embedding = np.append(final_embedding, word_feature_embedding_dict[i])\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen:  100\n"
     ]
    }
   ],
   "source": [
    "pre_padding = 0\n",
    "embeddings_index = embedding_info[0]\n",
    "MAX_SEQUENCE_LENGTH = embedding_info[1]\n",
    "maxlen = MAX_SEQUENCE_LENGTH\n",
    "print(\"maxlen: \",maxlen)\n",
    "#MAX_NB_WORDS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Unigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting relevant embeddings for multigenre\n",
    "if use_mg_train_corpora == MULTIGENRE:\n",
    "    # Multigenre\n",
    "    unigram_feature_string = \"1001111111111101\"\n",
    "elif use_mg_train_corpora == TWITTER:\n",
    "    # Twitter\n",
    "    unigram_feature_string = \"0110001111111101\"\n",
    "    unigram_feature_string = \"1111111111111111\"\n",
    "else:\n",
    "    # Twitter and Multigenre\n",
    "    unigram_feature_string = \"1110010000000000\"\n",
    "# 1 Google news pretrained vectors : GoogleNews-vectors-negative300.bin.gz  \n",
    "# 2 Twitter pretrained vectors: word2vec_twitter_model.bin\n",
    "# 3 glove.twitter.27B.200d.txt\n",
    "# 4 glove.6B.300d.txt\n",
    "# 5 glove.42B.300d.txt\n",
    "# 6 glove.840B.300d.txt\n",
    "# 7 NRC Emotion Intensity Lexicon\n",
    "# 8 senti word net\n",
    "#9  NRC Sentiment lexicon: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#10 lexicons/Emoticon-unigrams.txt\n",
    "#11 lexicons/Emoticon-AFFLEX-NEGLEX-unigrams.txt\n",
    "#12 NRC Hashtag Lexica: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "#13 HS-unigrams.txtNRC-Hashtag-Emotion-Lexicon-v0.2.txt\n",
    "#14 HS-AFFLEX-NEGLEX-unigrams.txt\n",
    "#15 Emoji Polarities\n",
    "#16 Depeche mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep learning with multigenre and twitter corpus and 4 emotions\n",
    "\"\"\"\n",
    "# K-Fold variables\n",
    "num_folds = 3 # 10\n",
    "fold_runs = 2 # 3\n",
    "fold_no = 1\n",
    "# train\n",
    "epochs = 4\n",
    "max_words = 20000\n",
    "# max. different words:\n",
    "# Multigerne: 5140  => 10000 or 3000 or 1000 ?\n",
    "# Twitter: 17580 => 20000 or 10000 ?\n",
    "# MG and Twitter: 20073 => evtl. 20000?\n",
    "#optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer = Adam(learning_rate=0.001) # default 0.001\n",
    "skfold = StratifiedKFold(n_splits = num_folds, random_state = 7, shuffle = True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_per_fold = []\n",
    "avg_acc_per_run = []\n",
    "avg_loss_per_run = []\n",
    "avg_precision_per_run = []\n",
    "avg_recall_per_run = []\n",
    "avg_f1_per_run = []\n",
    "create_final_model = True\n",
    "# run only final model without kfold\n",
    "run_final_train_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1560\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = len(get_unigram_embedding(\"glad\", embedding_info[0], unigram_feature_string))\n",
    "print(\"Embedding dimension:\",EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, filters = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train an test data set\n",
    "def create_data(texts, labels, maxlen):\n",
    "    ## Create one hot encoding\n",
    "    #max_words = 10000\n",
    "    #maxlen = 100 # max. number of words in sequences\n",
    "    #tokenizer = Tokenizer(num_words=max_words, filters = '')\n",
    "    #tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    #word_i = tokenizer.word_index\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    labels_arr = np.asarray(labels)\n",
    "    print('Shape of data:', data.shape)\n",
    "    print('Shape of labels:', labels_arr.shape)\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "    # mix the data\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels_arr = labels_arr[indices]\n",
    "\n",
    "    # split in train and validate\n",
    "    x_data = data\n",
    "    y_data = labels_arr\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer\n",
    "all_texts = train_texts.copy()\n",
    "all_texts.append(test_texts.copy())\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (1620, 100)\n",
      "Shape of labels: (1620,)\n",
      "-------------------------------------------\n",
      "Shape of data: (180, 100)\n",
      "Shape of labels: (180,)\n",
      "-------------------------------------------\n",
      "5140 unique Tokens found.\n"
     ]
    }
   ],
   "source": [
    "# Train an word index for embedding enrichment\n",
    "x_train, y_train = create_data(train_texts, train_labels, maxlen)\n",
    "x_test, y_test = create_data(test_texts, test_labels, maxlen)\n",
    "word_index = tokenizer.word_index\n",
    "x_train_copy = x_train.copy()\n",
    "y_train_copy = y_train.copy()\n",
    "print ('%s unique Tokens found.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Matrix\n",
    "word_embedding_matrix = list()\n",
    "word_embedding_matrix = np.zeros((max_words, EMBEDDING_DIM))\n",
    "#word_embedding_matrix.append(np.zeros(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\COMMANDER\\.conda\\envs\\gputest\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items(): # sorted(word_indices, key=word_indices.get):\n",
    "    embedding_features = get_unigram_embedding(word, embedding_info[0], unigram_feature_string)\n",
    "    if i < max_words:\n",
    "        if embedding_features is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            word_embedding_matrix[i] = embedding_features\n",
    "\n",
    "word_embedding_matrix = np.asarray(word_embedding_matrix, dtype='f')\n",
    "word_embedding_matrix = scale(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM 1560\n",
      "input_length 100\n"
     ]
    }
   ],
   "source": [
    "#print('word_indices_len',word_indices_len)\n",
    "print('EMBEDDING_DIM',EMBEDDING_DIM)\n",
    "print('input_length', MAX_SEQUENCE_LENGTH + pre_padding)\n",
    "embedding = Embedding(max_words, EMBEDDING_DIM, input_length=maxlen, trainable=False)\n",
    "#embedding = Embedding(word_indices_len + 1, EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH + pre_padding, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running \"optimal\" BiLSTM Model with 3 Runs and 10 k-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(Bidirectional(LSTM(32, dropout=0.4, recurrent_dropout=0.4, return_sequences=True)))   \n",
    "    model.add(Dense(16, activation='relu'))   \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 1 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 1.5008 - acc: 0.2593 - val_loss: 1.3444 - val_acc: 0.3296\n",
      "Epoch 2/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 1.2751 - acc: 0.4361 - val_loss: 1.1573 - val_acc: 0.5630\n",
      "Epoch 3/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 1.0895 - acc: 0.6065 - val_loss: 0.9662 - val_acc: 0.6444\n",
      "Epoch 4/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.9040 - acc: 0.6815 - val_loss: 0.8445 - val_acc: 0.6870\n",
      "540/540 [==============================] - 2s 3ms/step\n",
      "Score for fold 1: loss of 0.8445408949145564; accuracy of 68.70370507240295%\n",
      "540/540 [==============================] - 2s 3ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 1 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/4\n",
      "1080/1080 [==============================] - 9s 8ms/step - loss: 1.5265 - acc: 0.2972 - val_loss: 1.2702 - val_acc: 0.4074\n",
      "Epoch 2/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 1.2146 - acc: 0.4639 - val_loss: 1.1368 - val_acc: 0.4944\n",
      "Epoch 3/4\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 1.0659 - acc: 0.5750 - val_loss: 1.0263 - val_acc: 0.6185\n",
      "Epoch 4/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.9129 - acc: 0.6852 - val_loss: 0.9050 - val_acc: 0.6796\n",
      "540/540 [==============================] - 2s 4ms/step\n",
      "Score for fold 2: loss of 0.9050263404846192; accuracy of 67.96296238899231%\n",
      "540/540 [==============================] - 2s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 1 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/4\n",
      "1080/1080 [==============================] - 9s 8ms/step - loss: 1.3169 - acc: 0.3685 - val_loss: 1.1073 - val_acc: 0.5574\n",
      "Epoch 2/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.9636 - acc: 0.6278 - val_loss: 0.7999 - val_acc: 0.6926\n",
      "Epoch 3/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.6905 - acc: 0.7574 - val_loss: 0.7018 - val_acc: 0.7463\n",
      "Epoch 4/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.5373 - acc: 0.8194 - val_loss: 0.6753 - val_acc: 0.7537\n",
      "540/540 [==============================] - 2s 4ms/step\n",
      "Score for fold 3: loss of 0.6752861928056788; accuracy of 75.37037134170532%\n",
      "540/540 [==============================] - 2s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.8445408949145564 - Accuracy: 68.70370507240295%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.9050263404846192 - Accuracy: 67.96296238899231%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6752861928056788 - Accuracy: 75.37037134170532%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 70.67901293436687 (+- 3.331046653364429)\n",
      "> Loss: 0.8082844760682848\n",
      "> Precision: 0.7084835292603583\n",
      "> Recall: 0.70679012345679\n",
      "> F1: 0.7032528452088677\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ind run 2 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/4\n",
      "1080/1080 [==============================] - 9s 8ms/step - loss: 1.3941 - acc: 0.3639 - val_loss: 1.2586 - val_acc: 0.4167\n",
      "Epoch 2/4\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 1.1079 - acc: 0.5454 - val_loss: 1.0398 - val_acc: 0.5833\n",
      "Epoch 3/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.8590 - acc: 0.6731 - val_loss: 0.9216 - val_acc: 0.6315\n",
      "Epoch 4/4\n",
      "1080/1080 [==============================] - 8s 7ms/step - loss: 0.6864 - acc: 0.7602 - val_loss: 0.8179 - val_acc: 0.6833\n",
      "540/540 [==============================] - 3s 5ms/step\n",
      "Score for fold 1: loss of 0.8179150466565732; accuracy of 68.33333373069763%\n",
      "540/540 [==============================] - 3s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ind run 2 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/4\n",
      "1080/1080 [==============================] - 10s 9ms/step - loss: 1.3153 - acc: 0.4009 - val_loss: 1.0861 - val_acc: 0.5463\n",
      "Epoch 2/4\n",
      "1080/1080 [==============================] - 8s 7ms/step - loss: 0.9243 - acc: 0.6333 - val_loss: 0.8406 - val_acc: 0.6722\n",
      "Epoch 3/4\n",
      "1080/1080 [==============================] - 8s 7ms/step - loss: 0.6653 - acc: 0.7574 - val_loss: 0.7948 - val_acc: 0.6833\n",
      "Epoch 4/4\n",
      "1080/1080 [==============================] - 8s 8ms/step - loss: 0.5235 - acc: 0.8194 - val_loss: 0.7532 - val_acc: 0.6907\n",
      "540/540 [==============================] - 2s 4ms/step\n",
      "Score for fold 2: loss of 0.7531882206598918; accuracy of 69.07407641410828%\n",
      "540/540 [==============================] - 2s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ind run 2 ...\n",
      "Train on 1080 samples, validate on 540 samples\n",
      "Epoch 1/4\n",
      "1080/1080 [==============================] - 9s 9ms/step - loss: 1.3458 - acc: 0.3685 - val_loss: 1.0742 - val_acc: 0.5481\n",
      "Epoch 2/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.9319 - acc: 0.6361 - val_loss: 0.8374 - val_acc: 0.6648\n",
      "Epoch 3/4\n",
      "1080/1080 [==============================] - 7s 7ms/step - loss: 0.6905 - acc: 0.7556 - val_loss: 0.7245 - val_acc: 0.7130\n",
      "Epoch 4/4\n",
      "1080/1080 [==============================] - 7s 6ms/step - loss: 0.5177 - acc: 0.8343 - val_loss: 0.6819 - val_acc: 0.7500\n",
      "540/540 [==============================] - 2s 4ms/step\n",
      "Score for fold 3: loss of 0.6819484538502163; accuracy of 75.0%\n",
      "540/540 [==============================] - 2s 3ms/step\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.8179150466565732 - Accuracy: 68.33333373069763%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.7531882206598918 - Accuracy: 69.07407641410828%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6819484538502163 - Accuracy: 75.0%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 70.80247004826863 (+- 2.9834675801286474)\n",
      "> Loss: 0.7510172403888937\n",
      "> Precision: 0.7102374520007885\n",
      "> Recall: 0.7080246913580247\n",
      "> F1: 0.7050255024341712\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Score for k-fold runs\n",
      "------------------------------------------------------------------------\n",
      "> Run 1 Fold averages - Loss: 0.8082844760682848 - Accuracy: 70.67901293436687% \n",
      "> Run 1 Fold averages - Precision: 0.7084835292603583 - Recall: 0.70679012345679 F1: 0.7032528452088677\n",
      "------------------------------------------------------------------------\n",
      "> Run 2 Fold averages - Loss: 0.7510172403888937 - Accuracy: 70.80247004826863% \n",
      "> Run 2 Fold averages - Precision: 0.7102374520007885 - Recall: 0.7080246913580247 F1: 0.7050255024341712\n",
      "------------------------------------------------------------------------\n",
      "Overall average scores for all 2 runs:\n",
      "> Accuracy: 70.74074149131775 (+- 0.06172855695088231)\n",
      "> Loss: 0.7796508582285893\n",
      "> Precision: 0.7093604906305734\n",
      "> Recall: 0.7074074074074074\n",
      "> F1: 0.7041391738215195\n",
      "------------------------------------------------------------------------\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 1560)         31200000  \n",
      "_________________________________________________________________\n",
      "bidirectional_35 (Bidirectio (None, 100, 64)           407808    \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100, 16)           1040      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_35 (Glo (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 31,608,916\n",
      "Trainable params: 408,916\n",
      "Non-trainable params: 31,200,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for final model ...\n",
      "Train on 1620 samples, validate on 180 samples\n",
      "Epoch 1/4\n",
      "1620/1620 [==============================] - 10s 6ms/step - loss: 1.4088 - acc: 0.3296 - val_loss: 1.0924 - val_acc: 0.5667\n",
      "Epoch 2/4\n",
      "1620/1620 [==============================] - 8s 5ms/step - loss: 1.0958 - acc: 0.5327 - val_loss: 0.8967 - val_acc: 0.6722\n",
      "Epoch 3/4\n",
      "1620/1620 [==============================] - 8s 5ms/step - loss: 0.8877 - acc: 0.6432 - val_loss: 0.7894 - val_acc: 0.7167\n",
      "Epoch 4/4\n",
      "1620/1620 [==============================] - 8s 5ms/step - loss: 0.7298 - acc: 0.7210 - val_loss: 0.7109 - val_acc: 0.7389\n",
      "Evaluate final model on test data\n",
      "180/180 [==============================] - 1s 3ms/step\n",
      "test loss, test acc: [0.7108617610401577, 0.7388888597488403]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run x Times the folds\n",
    "if not run_final_train_only:\n",
    "    for run_num in range(1,fold_runs+1):\n",
    "        # k-fold\n",
    "        for train_ind, val_ind in skfold.split(x_train,y_train):\n",
    "\n",
    "            # Create model\n",
    "            model = create_model()\n",
    "\n",
    "            # Load GloVe embedding\n",
    "            model.layers[0].set_weights([word_embedding_matrix])\n",
    "            model.layers[0].trainable = False\n",
    "\n",
    "            # Train and Evaluate\n",
    "            model.compile(optimizer=optimizer,\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['acc'])\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} ind run {run_num} ...')\n",
    "\n",
    "            history = model.fit(x_train[train_ind], y_train[train_ind],\n",
    "                                epochs=epochs,\n",
    "                                batch_size=64,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_train[val_ind], y_train[val_ind]))\n",
    "\n",
    "            # metrics\n",
    "            scores = model.evaluate(x_train[val_ind], y_train[val_ind], batch_size=32)\n",
    "            #print(f'Score for fold {fold_no}: {model.metrics_name[0]} of {scores[0]}; {model.metrics_name[1]} of {scores[1]*100}%')\n",
    "            print(f'Score for fold {fold_no}: loss of {scores[0]}; accuracy of {scores[1]*100}%')\n",
    "            acc_per_fold.append(scores[1]*100)\n",
    "            loss_per_fold.append(scores[0])\n",
    "\n",
    "            # Evaluation metrics precison recall f1\n",
    "            y_pred = model.predict(x_train[val_ind], batch_size=64, verbose=1)\n",
    "            y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(y_train[val_ind], y_pred_bool)\n",
    "            mean_precision = np.mean(precision)\n",
    "            mean_recall = np.mean(recall)\n",
    "            mean_f1 = np.mean(f1)\n",
    "            precision_per_fold.append(mean_precision)\n",
    "            recall_per_fold.append(mean_recall)\n",
    "            f1_per_fold.append(mean_f1)\n",
    "\n",
    "            fold_no += 1\n",
    "\n",
    "        # == Provide average scores ==\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Score per fold')\n",
    "        for i in range(0, len(acc_per_fold)):\n",
    "            print('------------------------------------------------------------------------')\n",
    "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print('Average scores for all folds:')\n",
    "        avg_acc_per_run.append(np.mean(acc_per_fold))\n",
    "        avg_loss_per_run.append(np.mean(loss_per_fold))\n",
    "        avg_precision_per_run.append(np.mean(precision_per_fold))\n",
    "        avg_recall_per_run.append(np.mean(recall_per_fold))\n",
    "        avg_f1_per_run.append(np.mean(f1_per_fold))\n",
    "\n",
    "        print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "        print(f'> Precision: {np.mean(precision_per_fold)}')\n",
    "        print(f'> Recall: {np.mean(recall_per_fold)}')\n",
    "        print(f'> F1: {np.mean(f1_per_fold)}')\n",
    "        print('------------------------------------------------------------------------')\n",
    "\n",
    "        # reset fold vars\n",
    "        acc_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        precision_per_fold = []\n",
    "        recall_per_fold = []\n",
    "        f1_per_fold = []\n",
    "        fold_no = 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score for k-fold runs')\n",
    "    for i in range(0, len(avg_acc_per_run)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Run {i+1} Fold averages - Loss: {avg_loss_per_run[i]} - Accuracy: {avg_acc_per_run[i]}% ')\n",
    "        print(f'> Run {i+1} Fold averages - Precision: {avg_precision_per_run[i]} - Recall: {avg_recall_per_run[i]} F1: {avg_f1_per_run[i]}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Overall average scores for all {fold_runs} runs:')\n",
    "    print(f'> Accuracy: {np.mean(avg_acc_per_run)} (+- {np.std(avg_acc_per_run)})')\n",
    "    print(f'> Loss: {np.mean(avg_loss_per_run)}')\n",
    "    print(f'> Precision: {np.mean(avg_precision_per_run)}')\n",
    "    print(f'> Recall: {np.mean(avg_recall_per_run)}')\n",
    "    print(f'> F1: {np.mean(avg_f1_per_run)}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "# create final model #Todo sync with fold rund\n",
    "if create_final_model:\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "\n",
    "    # Load GloVe embedding\n",
    "    model.layers[0].set_weights([word_embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "\n",
    "    # Train and Evaluate\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Training for final model ...')\n",
    "\n",
    "    history = model.fit(x_train_copy, y_train_copy,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=64,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    # Save Model\n",
    "    if use_mg_train_corpora == MULTIGENRE:\n",
    "        model.save('models/model_emotion_detection_multigenre.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre.pkl\", \"wb\"))\n",
    "    elif use_mg_train_corpora == TWITTER:\n",
    "        model.save('models/model_emotion_detection_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_twitter.pkl\", \"wb\"))\n",
    "    else:\n",
    "        model.save('models/model_emotion_detection_multigenre_twitter.h5')\n",
    "        pickle.dump(tokenizer, open(\"models/tokenizer_multigenre_twitter.pkl\", \"wb\"))\n",
    "\n",
    "    # Test final model\n",
    "    print(\"Evaluate final model on test data\")\n",
    "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "    # For Model evaluation metrics run evalModel\n",
    "\n",
    "    # Plot performance\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
