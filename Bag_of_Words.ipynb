{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: Sentdex\n",
    "### Author: Himmet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model focuses completely on the words, or sometimes a string of words, \n",
    "# but usually pays no attention to the \"context\" so-to-speak. The bag of words model usually has a large list, \n",
    "# probably better thought of as a sort of \"dictionary,\" which are considered to be words that carry sentiment. \n",
    "# These words each have their own \"value\" when found in text. The values are typically all added up and the result is a \n",
    "# sentiment valuation. The equation to add and derive a number can vary, but this model mainly focuses on the words, \n",
    "# and makes no attempt to actually understand language fundamentals.\n",
    "\n",
    "# source: http://sentdex.com/sentiment-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize = seperate text (ex: words or sentences)\n",
    "# Corpera = body of text (ex: twtter, news, etc.)\n",
    "# Lexicon = words and their means (ex: emotion, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source crowdflower_tweets.xlxs\n",
    "ex_text = \"@BrodyJenner if u watch the hills in london u will realise what tourture it is because were weeks and weeks late i just watch it online lol. @GABBYiSACTiVE Aw you would not unfollow me would you? Then I would cry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@BrodyJenner if u watch the hills in london u will realise what tourture it is because were weeks and weeks late i just watch it online lol. @GABBYiSACTiVE Aw you would not unfollow me would you? Then I would cry'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@BrodyJenner if u watch the hills in london u will realise what tourture it is because were weeks and weeks late i just watch it online lol.', '@GABBYiSACTiVE Aw you would not unfollow me would you?', 'Then I would cry']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(ex_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'BrodyJenner', 'if', 'u', 'watch', 'the', 'hills', 'in', 'london', 'u', 'will', 'realise', 'what', 'tourture', 'it', 'is', 'because', 'were', 'weeks', 'and', 'weeks', 'late', 'i', 'just', 'watch', 'it', 'online', 'lol', '.', '@', 'GABBYiSACTiVE', 'Aw', 'you', 'would', 'not', 'unfollow', 'me', 'would', 'you', '?', 'Then', 'I', 'would', 'cry']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(ex_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BrodyJenner if u watch the hills in london u will realise what tourture it is because were weeks and weeks late i just watch it online lol.\n",
      "@GABBYiSACTiVE Aw you would not unfollow me would you?\n",
      "Then I would cry\n"
     ]
    }
   ],
   "source": [
    "for i in sent_tokenize(ex_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the first steps to pre-processing is to utilize stop-words. \n",
    "# Stop words are words that you want to filter out of any analysis. \n",
    "# These are words that carry no meaning, or carry conflicting meanings that you simply do not want to deal with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or in one line:\n",
    "filtered_sentence = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'BrodyJenner', 'watch', 'hills', 'london', 'realise', 'tourture', 'weeks', 'weeks', 'late', 'watch', 'online', 'lol', '.', '@', 'GABBYiSACTiVE', 'Aw', 'would', 'unfollow', 'would', '?', 'Then', 'I', 'would', 'cry']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\n",
      "BrodyJenner\n",
      "watch\n",
      "hills\n",
      "london\n",
      "realise\n",
      "tourture\n",
      "weeks\n",
      "weeks\n",
      "late\n",
      "watch\n",
      "online\n",
      "lol\n",
      ".\n",
      "@\n",
      "GABBYiSACTiVE\n",
      "Aw\n",
      "would\n",
      "unfollow\n",
      "would\n",
      "?\n",
      "Then\n",
      "I\n",
      "would\n",
      "cry\n"
     ]
    }
   ],
   "source": [
    "for i in filtered_sentence:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the process where we remove word affixes from the end of words. \n",
    "# The reason we would do this is so that we do not need to store the meaning of every single tense of a word. \n",
    "# They all have the same meaning for their \"root\" stem (read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for i in ex_words:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@brodyjenner if u watch the hills in london u will realise what tourture it is because were weeks and weeks late i just watch it online lol. @gabbyisactive aw you would not unfollow me would you? then i would cri\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem(ex_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART OF SPEECH TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of Speech (Pos) tagging does exactly what it sounds like, \n",
    "# it tags each word in a sentence with the part of speech for that word. \n",
    "# This means it labels words as noun, adjective, verb, etc. \n",
    "# PoS tagging also covers tenses of the parts of speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sent_tokenizer = PunktSentenceTokenizer(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = c_sent_tokenizer.tokenize(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@', 'NN'), ('BrodyJenner', 'NNP'), ('if', 'IN'), ('u', 'JJ'), ('watch', 'VBP'), ('the', 'DT'), ('hills', 'NNS'), ('in', 'IN'), ('london', 'JJ'), ('u', 'NN'), ('will', 'MD'), ('realise', 'VB'), ('what', 'WP'), ('tourture', 'NN'), ('it', 'PRP'), ('is', 'VBZ'), ('because', 'IN'), ('were', 'VBD'), ('weeks', 'NNS'), ('and', 'CC'), ('weeks', 'NNS'), ('late', 'JJ'), ('i', 'NN'), ('just', 'RB'), ('watch', 'VB'), ('it', 'PRP'), ('online', 'JJ'), ('lol', 'NN'), ('.', '.'), ('@', 'JJ'), ('GABBYiSACTiVE', 'NNP'), ('Aw', 'NNP'), ('you', 'PRP'), ('would', 'MD'), ('not', 'RB'), ('unfollow', 'VB'), ('me', 'PRP'), ('would', 'MD'), ('you', 'PRP'), ('?', '.')]\n",
      "[('Then', 'RB'), ('I', 'PRP'), ('would', 'MD'), ('cry', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking in Natural Language Processing (NLP) is the process by which we group various words together by their PoS tags. \n",
    "# One of the most popular uses of this is to group things by what are called \"noun phrases.\" \n",
    "# We do this to find the main subjects and descriptive words around them, \n",
    "# but chunking can be used for any combination of parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
