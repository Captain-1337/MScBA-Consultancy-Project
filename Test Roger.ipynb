{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\COMMANDER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import senticnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sentence = \"Could you please not take this? John does not likes to watch movies. Mary does not likes movies from Austria. :) (see under http://rogers.li) #lovemovie N\"\n",
    "sentence2 = \"John like watch movies . Mary likes movies . happy\"\n",
    "\n",
    "pos_map = {} # not used!\n",
    "pos_map[\"NN\"] = \"n\"\n",
    "pos_map[\"NNP\"] = \"n\"\n",
    "pos_map[\"NNS\"] = \"n\"\n",
    "pos_map[\"NNPS\"] = \"n\"\n",
    "pos_map[\"NNP\"] = \"n\"\n",
    "pos_map[\"VB\"] = \"v\"\n",
    "pos_map[\"VBD\"] = \"v\"\n",
    "pos_map[\"VBG\"] = \"v\"\n",
    "pos_map[\"VBN\"] = \"v\"\n",
    "pos_map[\"VBP\"] = \"v\"\n",
    "pos_map[\"VBZ\"] = \"v\"\n",
    "pos_map[\"JJ\"] = \"a\"\n",
    "pos_map[\"JJR\"] = \"a\"\n",
    "pos_map[\"JJS\"] = \"a\"\n",
    "pos_map[\"RB\"] = \"r\"\n",
    "pos_map[\"RBR\"] = \"r\"\n",
    "pos_map[\"RBS\"] = \"r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    from nltk.corpus import wordnet\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_tokens):\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow(sentence: str):\n",
    "    \"\"\"\n",
    "    Build a Bag of words from a sentence\n",
    "    \"\"\"\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence)\n",
    "    sequences = tokenizer.texts_to_sequences(sentence)\n",
    "    word_index = tokenizer.word_index\n",
    "    bow = {}\n",
    "    for key in word_index:\n",
    "        bow[key] = sequences[0].count(word_index[key])\n",
    "    return bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bow(bow: {}):\n",
    "    \"\"\"\n",
    "    Print a bag of words\n",
    "    :param bow: A bag od words to print\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(f\"Bag of word sentence 1 :\\n{bow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    result_tokens = word_tokenize(sentence)\n",
    "    return result_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_tokens(word_tokens):\n",
    "    from nltk import pos_tag\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    pos_tokens = pos_tag(word_tokens)\n",
    "    print(pos_tokens)\n",
    "    result_tokens = []\n",
    "    # Map to\n",
    "    # first element is the word, second element is the pos tag\n",
    "    for token in pos_tokens :\n",
    "\n",
    "        # skip not mapped speech\n",
    "        if token[1].upper() != token[0].upper():\n",
    "            # Get Part of Speech from pos_tag to relevant tag of the lemmatizer\n",
    "            # example 'NNP' => 'n'\n",
    "            pos = nltk_tag_to_wordnet_tag(token[1])\n",
    "        else:\n",
    "            pos = \"\"\n",
    "        if pos != \"\" and pos is not None:\n",
    "            result_tokens.append(lemmatizer.lemmatize(token[0], pos=pos))\n",
    "        else:\n",
    "             result_tokens.append(token[0])\n",
    "    return result_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotions(tokens):\n",
    "    from senticnet.senticnet import SenticNet\n",
    "    result = {}\n",
    "    sn = SenticNet()\n",
    "    for token in tokens:\n",
    "        moodtags = \"\"\n",
    "\n",
    "        if token in sn.data:\n",
    "            moodtags = sn.moodtags(token)\n",
    "            print(token, moodtags)\n",
    "    #TODO\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please not take this? John does not likes to watch movies. Mary does not likes movies from Austria. :) (see under http://rogers.li) #lovemovie N\n",
      "['Could', 'you', 'please', 'not', 'take', 'this', '?', 'John', 'does', 'not', 'likes', 'to', 'watch', 'movies', '.', 'Mary', 'does', 'not', 'likes', 'movies', 'from', 'Austria', '.', ':', ')', '(', 'see', 'under', 'http', ':', '//rogers.li', ')', '#', 'lovemovie', 'N']\n",
      "[('Could', 'NNP'), ('you', 'PRP'), ('please', 'VBP'), ('not', 'RB'), ('take', 'VB'), ('this', 'DT'), ('?', '.'), ('John', 'NNP'), ('does', 'VBZ'), ('not', 'RB'), ('likes', 'VB'), ('to', 'TO'), ('watch', 'VB'), ('movies', 'NNS'), ('.', '.'), ('Mary', 'NNP'), ('does', 'VBZ'), ('not', 'RB'), ('likes', 'VB'), ('movies', 'NNS'), ('from', 'IN'), ('Austria', 'NNP'), ('.', '.'), (':', ':'), (')', ')'), ('(', '('), ('see', 'VB'), ('under', 'IN'), ('http', 'NN'), (':', ':'), ('//rogers.li', 'NN'), (')', ')'), ('#', '#'), ('lovemovie', 'JJ'), ('N', 'NNP')]\n",
      "['Could', 'you', 'please', 'not', 'take', 'this', '?', 'John', 'do', 'not', 'like', 'to', 'watch', 'movie', '.', 'Mary', 'do', 'not', 'like', 'movie', 'from', 'Austria', '.', ':', ')', '(', 'see', 'under', 'http', ':', '//rogers.li', ')', '#', 'lovemovie', 'N']\n",
      "Could please take ? John like watch movie . Mary like movie Austria . : ) ( see http : //rogers.li ) # lovemovie N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of word sentence 1 :\n",
      "{'not': 3, 'does': 2, 'likes': 2, 'movies': 2, 'could': 1, 'you': 1, 'please': 1, 'take': 1, 'this': 1, 'john': 1, 'to': 1, 'watch': 1, 'mary': 1, 'from': 1, 'austria': 1, 'see': 1, 'under': 1, 'http': 1, 'rogers': 1, 'li': 1, 'lovemovie': 1, 'n': 1}\n",
      "=======================================\n",
      "please ['#interest', '#admiration']\n",
      "take ['#interest', '#admiration']\n",
      "watch ['#joy', '#interest']\n",
      "movie ['#joy', '#joy']\n",
      "movie ['#joy', '#joy']\n",
      "see ['#interest', '#admiration']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Script\n",
    "print(sentence)\n",
    "# get Bow\n",
    "# remove stopwords -> remove special string/words -> Get POS -> Lemm -> lookup emotion for each word -> w emotions\n",
    "tokens1 = tokenize(sentence)\n",
    "print(tokens1)\n",
    "\n",
    "tokens2 = lemm_tokens(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "tokens3 = remove_stopwords(tokens2)\n",
    "#sentence2 = sentence\n",
    "print(' '.join(tokens3))\n",
    "print_bow(build_bow([sentence]))\n",
    "print(\"=======================================\")\n",
    "get_emotions(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could please take ? John like watch movie . Mary like movie Austria . : ) ( see http : //rogers.li ) # lovemovie N\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ab hier kommt senticnet zum einsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
